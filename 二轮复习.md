##                                                                                                                                                                    JDK集合

#### 基础数据结构

1. 数组：数组大小固定，查找方便，增删困难，需要完整的内存块，容易产生碎片。
2. 链表：大小可以动态调整，增删迅速，查找较慢，不需要整块内存块。
3. 哈希表：遇到hash冲突的时候常用的解决办法：
   + 链地址法：HashMap使用链表法，链表是单向链表。
   + 开放地址法：**线性探查法、二次探查法**

#### 红黑树/B树/B+树

##### **红黑树**

+ 根节点是黑的，叶子结点是黑色；
+ 如果一个节点是红的，他的字节点必须是黑的
+ 从一个节点到该节点的子孙节点的所有路径上包含相同数量的黑色节点【平衡】

> 红黑树的平衡并不是严格的平衡，而是一种黑色平衡，即从一个节点到每个叶子结点的路径上包含的黑色结点个数相同。

**不用二叉查找树/AVL/2-3树的原因：** 

虽然二叉查找树的平均复杂度也是logn，但是二叉查找树的平衡会被打破，退化成一个线性链表，导致复杂度变成n。

而AVL树是严格平衡的二叉树，能够保证查找的效率，但是在增加删除为了维持严格的平衡付出更大的代价。

2-3树不是二叉树，当我们要插入新的节点时，如果查找结束于一个2-节点，`可以将一个2-节点转换为3-节点`，从而避免了平衡操作。它只是红黑树的一种实现方式。红黑树本质上使用标准的二叉查找树形式和一些额外的信息（颜色）来代替3节点来表示2-3树。把红链接画平，就是一个2-3树。

红黑树则是这两者的一种中庸选择，能一定程度上保证平衡，也不至于为了维持平衡付出很大的代价。

**不用B树/B+树的原因：**

hashmap的操作是在内存中，并且可能有频繁的插入操作。而B树和B+树更适合用于读取磁盘上的数据，一次读取一批数据可以减少IO的次数。但是hashmap中用B树并不能发挥它的优势，反倒因为要修改树的结构引起更多的麻烦。



##### **B树**

每个节点最多m个孩子，称为m阶B树。根节点至少包括2个孩子，所有叶子结点都在同一层。

数据库的查询是位于磁盘，读取到数据之后存储到索引结构中。磁盘的读取是按照扇区来读取的。每个磁盘页能对应一个节点，那B树就能很好的将高度降低，减少IO次数。

##### B+树

树中节点指针与关键字数目一样，且数据均在叶子节点中。所有叶子结点有一个链指针，所有关键字都在叶子结点出现。

**B+树的优势在于：**

1. B+树的内部节点中的关键字代表子树的最大值，key和子树的数量一样，比B树能存储的key更多，能存的指针也更多，这样树的高度下降，IO次数下降了。
2. B+树的范围查找更加方便，可以先找到范围的下限，然后通过叶子结点的链表顺序遍历，直到找到上限。而B树只能通过遍历节点，找到上限。而数据库查找中，范围查找是很常用的。
3. B+树的查询效率更加稳定，每次都是查询到叶子结点。而B树查找的性能等价于在关键字全集做一次二分查找。

##### **总结**

为了快速查找，有了 binary search tree；为了避免 BST 数据严重倾斜时导致的最坏查找时间，有了 red-black tree；但是这还都是内存数据结构；为了应用在较慢存储介质上，有了 B tree；为了最后能快速遍历所有 node，于是在叶子节点加了个 linked list，这就有了 B+ tree。



##### 生产环境中的B+树

InnoDB 一棵 B+ 树可以存放多少行数据？**2千万**

> 最小存储单元

在计算机中磁盘存储数据最小单元是扇区，一个扇区的大小是 512 字节；

而文件系统（例如XFS/EXT4）他的最小单元是块，一个块的大小是 4k；**（macbook某文件的大小：1,944 字节（磁盘上的4 KB））**

而对于我们的 InnoDB 存储引擎也有自己的最小储存单元——页（Page），一个页的大小是 16K。

> B+树中的page

1. InnoDB中的最小存储单元是page，可以存放数据也可以存放键值和指向下一page的指针；
2. 主键索引根页的 page number 为 3；其他的二级索引 page number 为 4。
3. 组织索引表通过非叶子节点的人查找，以及指针确定数据在哪一个page中，进而去page中查找需要的数据；
4. 叶子节点（page）中的记录数量：假设一条记录1k，那就是16条；
5. 非叶子节点中的指针数量：假设主键id为bigint，8字节，而指针在innobd中是6字节，一共14字节。那么一个page中16k = 16384B ，16384/14 = 1170；
6. 高度为2的B+树：能存放1170*16条数据；
7. 高度为3的B+树：能存放1170\*1170\*16 条数据；
8. 所以在innodb中1-3层的B+树就能基本满足千万级别的数据量

还有一些细节问题没有算进去：比如**一个页中不可能所有空间都用于存放数据**，它还会存放一些少量的其他字段比如 page level，index number 等等，另外还有页的填充因子也导致一个页不可能全部用于保存数据。

#### ArrayList

Arraylist继承自Abstractlist，实现了collection、list、randomAccess等接口。

动态数组，允许元素为null，会自动扩容1.5倍

使用Collections.synchronizedList方法同步化；本质上是增加了synchronized关键字来保证线程安全。

```java
private static final int DEFAULT_CAPACITY = 10; //默认初始化容量
private static final Object[] EMPTY_ELEMENTDATA = {}; // 空实例的共享空数组
private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {};
transient Object[] elementData; // 元素对象数据，不是private以便子类访问
private int size; // 当前集合的大小
// ArrayList 的内部存储结构就是一个 Object 类型的数组，因此它可以存放任意类型的元素。在构造ArrayList 的时候，如果传入初始大小那么它将新建一个指定容量的 Object 数组，如果不设置初始大小那么它将不会分配内存空间而是使用空的对象数组EMPTY_ELEMENTDATA，在实际要放入元素时再进行内存分配。
//** initialCapacity > 0  new Object[initialCapacity]
//** initialCapacity == 0 EMPTY_ELEMENTDATA
//** 未指定初始化容量 DEFAULTCAPACITY_EMPTY_ELEMENTDATA
```



##### **add**

1. 每次添加数据的时候都要检查当前数组容量是否需要扩容，调用ensureCapacityInternal（size+1）；
2. 然后直接赋值elementData[size++] = e;
3. return true；
4. 如果是指定index的add，需要检查index是否出界，然后检查是否需要扩容，然后将index之后的数据用system.arraycopy向后移动一位，最后插入；

##### **remove**

1. 检查index是否越界；
2. modcount++；暂存oldvalue；
3. 将index之后的元素向前移动1位；置空最后一个元素为null；
4. 返回被删除元素



1. 如果是删除指定的Object；根据object是不是null，遍历元素，如果相等就删除。return true
2. return false



##### **扩容**

1. 传入参数minCapacity，是当前操作期望的最小容量；
2. 如果当前elementdata为空，表明还没有初始化，minCapacity和默认容量10相比，去较大的那个；
3. 调用ensureExplicitCapacity进行初始化或者扩容；在这个方法内，检查minCapacity是不是比当前的数组长度要大？如果是的，调用grow进行扩容/初始化。
4. grow方法中，通过原数组的长度oldCapacity计算1.5倍大的newCapacity，如果新容量还是小于minCapacity，表明1.5倍还不够，新容量=minCapacity。
5. 检查新容量有没有大于容量上限，最大不超过Integer.MAXVALUE;
6. 最后调用Arrays.copyOf方法将数组扩容到新容量



##### Vector

Vector调用无参构造时直接初始话一个长度为10的数组，而ArrayList调用无参构造函数时初始化一个空数组，直到首次调用add()方法才创建一个长度为10的数组

Vector可以指定每次扩容增加多少长度，而ArrayList无法指定

Vector的add（）方法使用关键字synchronized 修饰，所以是线程安全的，而ArrayList不是线程安全的，其实除了add方法，几乎Vector的所有方法都使用synchronized修饰，Vector是线程同步的，但是效率很低，一般不赞成使用。

Vector扩容多少根据capacityIncrement而定，capacityIncrement为0时则数组长度变为原来的两倍，不为0则在原来的数组长度上增加capacityIncrement，而ArrayList每次扩容长度变为原来的1.5倍



#### LinkedList

继承自AbstrackSequentialList，同时实现了List和Deque接口。AbstractSequentialList 提供了一套基于顺序访问的接口，LinkedList的方法都是基于迭代器实现的，所以它使用迭代器遍历更快。而继承了Deque，就又具备了队列/栈的功能，允许节点为null的双向链表。

```java
transient int size = 0;
transient Node<E  first;
transient Node<E> last;
private static class Node<E>{
  E item;
  Node<E> next;
  Node<E> prev;
  
  Node(Node<E> prev, E element, Node<E> next){
      this.item = element;  //当前节点值
      this.next = next; //后继节点
      this.prev = prev;//前驱节点
  }
}
```

**node(index)**

很多地方调用node(index)获取index处的节点。

1. 首先判断index是在链表偏左侧还是偏右侧
2. 从左或者从右遍历；找到目标值

##### **addFirst/addLast**

内部实现是调用linkFirst和linkLast方法；

1. 先把原first暂存；
2. 新增节点，并设为first；
3. 如果原来的first是空的那么last也是新增节点；否则原first的prev指向新增节点；
4. size++；modCount++；

##### **add(index, e)**

1. 先检查index越界情况
2. 如果index是尾巴，调用linkLast
3. 否则调用linkBefore(element, node(index))；

一般的add方法默认添加在尾部；
**上面叙述的各种操作中还是会记录modCount,所以LinkedList也是会产生快速失败事件的。**



#### PriorityQueue

优先级队列，是0个或多个元素的集合，集合中的每个元素都有一个权重值，每次出队都弹出优先级最大或者最小的元素。**一种无界的，线程不安全的队列**

```java
private static final int DEFAULT_INITIAL_CAPACITY = 11; // 默认容量
transient Object[] queue; // 数组存储元素
private int size = 0; // 元素个数
private final Comparator<? super E> comparator; // 比较器
transient int modCount = 0; // 修改次数
```

##### add

1. 入队不允许null元素；
2. 如果元素的个数达到最大容量，调用grow方法扩容；
3. 如果堆中还没有元素，就插入下标0的位置；
4. 如果有元素了，就从最后一个元素往后一个位置，自下而上堆化siftUp，一直往上和父节点继续比较；
5. 如果比父节点要小，就和父节点换位置；从而成为一个小顶堆；

##### grow

1. 如果旧容量小于64，容量翻倍；
2. 如果旧容量大于等于64，只增加一半的元素；

##### poll

1. 将队列首元素弹出；
2. 取队列尾部的元素，从堆顶开始自上而下的堆化，siftDown，一直往下和最小的子节点比；
3. 如果比子节点大，交换位置，直到堆化结束。



#### HashMap 1.8

对象需要重写hashcode和equasl：hashCode()用来定位要存放的位置，equal()用来判断是否*相等*。

```java
// 基本属性
static final int default_initial_capacity = 1 << 4; //初始容量 16
static final int MAXINUM_CAPACITY = 1 << 30; // 最大容量2^30 
static final float DEFAULT_LOAD_FACTOR = 0.75f; // 默认负载因子0.75
static final int TREEIFY_THRESHOLD = 8; // 链表节点转换为红黑树
static final int UNTREEIFY_THRESHOLD = 6; // 红黑树转换为链表
static fianl int MIN_TREEIFY_CAPACITY = 64; // 转红黑树的时候table的最小长度
// 节点
static class Node<K,V> implements Map.Entry<K,V>{
  final int hash;
  final K key;
  V value;
  Node<K,V> next;
}
static class TreeNode<K,V> extends LinkedHashMap.Entry<K,V>{
  TreeNode<K,V> parent;
  TreeNode<K,V> left;
  TreeNode<K,V> right;
  TreeNode<K,V> prev;
  boolean red;
}
```

##### **定位到哈希桶数组的位置**

+ 拿到key的hashcode值；

+ 将hashcode的高16位与hashcode进行按位异或运算（(h = key.hashCode()) ^ (h >>> 16)）；【为了降低hash冲突的概率】

+ 将计算出来的 hash 值与 (table.length - 1) 进行 按位与& 运算【实际上就是对长度取模】；

  > x mod 2^n = x & (2^n - 1)。HashMap 底层数组的长度总是 2 的 n 次方，并且取模运算为 “h mod table.length”，对应上面的公式，可以得到该运算等同于“h & (table.length - 1)”。这是 HashMap 在速度上的优化，因为 & 比 % 具有更高的效率。

##### **get**

1. 对table进行校验：table不为空 && table的长度大于0 && first节点（定位到的索引节点）不为空；

2. 满足以上条件，检查first节点的hash值和key是否和入参的一样，如果一样，first就是目标节点，直接返回first；

3. 如果first不是目标节点，且next不为空，则继续遍历；

4. 其中如果first节点是TreeNode，则调用红黑树的查找目标节点方法getTreeNode；首先找到红黑树的根节点 ((parent != null) ? root() : this)，然后根节点调用find方法：

   > 如果传入的hash值小于p节点的hash值，则往p节点的左边遍历；如果传入的hash值大于p节点的hash值，则往p节点的右边遍历；如果传入的hash值和key值等于p节点的hash值和key值,则p节点为目标节点,返回p节点

##### **put**

1. 校验table是否为空或者length等于0，如果是，则调用resize方法进行初始化；

2. 通过hash值定位索引位置，如果该位置为空，则直接在该位置新增一个节点，调用newNode；

3. 若不为空，称首节点为p，先判断p节点的key和hash值是否和入参相等，如果相等，则p节点是目标节点，将p赋值给e节点；

4. 判断p节点是不是TreeNode节点，如果是就调用红黑树的putTreeVal方法查找目标节点；

   > putTreeVal方法：红黑树插入同时会维护原来的链表next属性：
   >
   > 1. 查找根节点，索引位置的头节点并不一定是根节点；将根节点赋值给p节点，开始进行查找
   > 2. 如果传入的hash值小于p，dir = -1，向左查找；大于，dir = 1，向右查找；hash和key相等的时候返回p（即目标节点已存在）
   > 3. 如果入参k所属的类没有实现Comparable接口，需要另外处理，使用定义的一套规则来比较。
   > 4. dir<=0则向p左边查找,否则向p右边查找,如果为null,则代表该位置即为x的目标位置
   > 5. 新建节点，建立新节点和它的父节点的关系，
   > 6. balanceInsert、 moveRootToFront进行红黑树的插入平衡调整。

5. 如果都不满足以上2个情况，这时候p节点是链表节点，查找这个链表（binCount统计链表节点个数），令e=p.next，如果e是null则找不到目标节点，使用newNode**新增一个节点在链表的尾部**。新增之后检查是否binCount>=7，这时候链表中已经有9个节点（加上新增的那个），如果超过则调用treeifyBin方法把链表转为红黑树节点。如果e节点存在hash和key与入参相同，则此时的e节点是目标节点，跳出遍历的循环。

6. 通过以上3种情况的判断，这时候e如果不为空，则代表目标节点已经存在了，更新value并返回oldvalue；

7. ++modcount

8. 如果插入节点之后节点数量超过阈值，调用resize方法进行**扩容**



##### **treeifyBin**

1. 如果table长度小于64，不转化为红黑树，而是调用resize方法来**扩容**；
2. 根据hash值定位索引的位置，将索引处的节点赋值给e，从e开始遍历这个位置的链表；
3. replacementTreeNode方法将链表节点转化为红黑树节点；
4. 如果是第一次遍历，将头节点赋值给hd；如果不是，处理当前节点和上一个节点的prev和next关系；
5. tab[index] = hd 将前面得到的TreeNode hd赋给索引位置，如果这个节点不是null，那么以该节点为根，构建红黑树 treeify(tab)

> treeify中，如果root节点不是table的索引位置指向的头节点，调用moveRootToFront将其调整为头节点。原头节点放在root的next上



##### **resize**

1. 先初始化oldCap，oldThr变量，oldCap变量代表当前table的长度，如果老表为空，这个值是0；oldThr的值是当天的threshold值；
2. 分为3种情况来重新规划长度：
   + 老表非空，容量大于0，这时候是真的扩容，需要进一步判断。【转到步骤3】
   + 老表为空，阈值大于0，这种情况出现在第一次带参数初始化的时候，初始容量被放进了阈值当中：这时把新容量设置为老表的阈值。
   + 老表为空，阈值为0，这种情况是没有传初始容量的new方法创建的空表：将阈值和容量设置为默认值就可以，16，16*0.75。
3. **判断老表的容量是否超过最大容量限制，超过就不能再变大，只能修改阈值为Integer.MAX_VALUE; 直接return停止扩容；**
4. 没有超过，就把newCap设为原容量的2倍，如果newCap小于最大容量并且oldCap>=16，新的阈值也扩大为2倍。
5. 这时候如果新表阈值还没确定，那就用新的容量*负载因子计算得到。
6. 确定了新表的容量和阈值，就可以new 一个新的Node数组来作为新表。老表不为空的情况下需要把节点数据赋值给新表。有些数据这时候需要重新排列。
7. 将索引值为j的老表头节点赋值给e，此时如果e.next为null，表示这个位置只有1个节点，计算新表的索引位置，直接将e放到那个位置；
8. 如果这个位置有多个节点，红黑树节点使用split方法rehash，链表进行rehash：
   + 如果e的hash值与老表的容量进行与运算为0,则扩容后的索引位置跟老表的索引位置一样，插入到老位置的链表尾部；【为什么是(e.hash & oldCap) == 0，因为假设原先是使用最低的4位进行hash，那么扩容之后第5位也要开始起作用，用oldcap正好是第5位为1】
   + 如果e的hash值与老表的容量进行与运算为1,则扩容后的索引位置为:老表的索引位置＋oldCap，插入到新位置的链表**尾部**；
   + 最后都需要把尾部节点的next置为null；新表上的索引位置节点设置为对应的头节点；

9. 红黑树的rehash方法split的逻辑和链表的类似，也是通过hash的值&老容量来判断新位置。只不过在最后还需要判断节点数量是否小于6，如果小于，需要调用untreeify来把红黑树变回链表；否则继续判断原红黑树有没有拆分成两棵树，如果拆了，就需要重新构建红黑树treeify。



##### **remove**

1. 如果table不为空，并且通过hash值定位的位置不为空，将该位置的节点赋值给p；
2. 如果p的hash值和key都与入参相等，那就是目标节点，将它赋值给node；
3. 否则向下遍历节点，如果是红黑树就使用红黑树的查找方法getTreeNode；否则进行普通链表节点的查找；将找到的节点赋值给node；
4. 如果node不为空，表明目标节点存在，进行移除操作：如果是红黑树调用红黑树的removeTreeNode方法；如果是链表就按照链表的方式删除节点。
5. ++modCount；--size；afterNodeRemoval



##### **死循环问题的解决**

1.8之前之所以有死循环问题：扩容rehash之后，节点的顺序可能会反掉，因为用的是头插法。

1.8之后用的是尾插法，所以扩容之后，节点的顺序是一致的不会产生死循环的情况。但这时候仍然是线程不安全的，并发下仍可能丢失数据【已遍历的table区间新增了元素】。



##### HashTable

1. Hashmap允许key、value是null；hashtable不可以；
2. hashmap默认初始容量16；hashtable11质数；
3. 扩容2倍；扩容2倍+1；
4. 非线程安全的；线程安全的synchronized；
5. hash值是根据hashcode重新计算的；直接使用hashcode
6. hashmap去处了contains方法
7. hashmap继承自abstractMap，hashtable继承自dictionary



##### **总结**

1. hashmap的底层是一个Node数组，在数组的具体索引位置上，如果存在多个节点，可能是链表或者红黑树；
2. 增删查的时候，定位到哈系桶的位置很关键，首先要先得到hashcode值，然后将hashcode的高16位与hashcode按位异或得到hash值，最后将hash值与table.length-1按位与【其实就是按长度取模】。
3. hashmap的初始容量是16，因为计算hash值对capacity有要求，必须是2的幂次方；默认的负载因子是0.75，实际能存放的节点个数，即触发扩容的阈值
4. 扩容的时候，节点的hash值是Node中的域，直接取出来hash&（n-1）



拉链法导致的链表过深问题为什么不用二叉查找树代替，而选择红黑树？为什么不一直使用红黑树？

之所以选择红黑树是为了解决二叉查找树的缺陷，二叉查找树在特殊情况下会变成一条线性结构（这就跟原来使用链表结构一样了，造成很深的问题），遍历查找会非常慢。而红黑树在插入新数据后可能需要通过左旋，右旋、变色这些操作来保持平衡，引入红黑树就是为了查找数据快，解决链表查询深度的问题，我们知道红黑树属于平衡二叉树，但是为了保持“平衡”是需要付出代价的，但是该代价所损耗的资源要比遍历线性链表要少，所以当长度大于8的时候，会使用红黑树，链表长度低于6，就把红黑树转回链表，因为根本不需要引入红黑树，引入反而会慢。



#### ConcurrentHashMap

```java
// 基本属性 
// 这几个和hashmap基本一致
private static final int MAXIMUM_CAPACITY = 1 << 30;
private static final int DEFAULT_CAPACITY = 16;
static final int TREEIFY_THRESHOLD = 8;
static final int UNTREEIFY_THRESHOLD = 6;
static final int MIN_TREEIFY_CAPACITY = 64;
// 多出来的
static final int MOVED  = -1; // 表示正在转移
static final int TREEBIN = -2; //表示已经转换成红黑树
static final int RESERVED = -3; // hash for transient reservations
static final int HASH_BITS = 0x7fffffff; // &这个值可以保证第一位是0，即是个正数
// table
                      
// 用来控制表初始化和扩容，默认0
// 初始化的时候指定来大小，sizeCtl就保存初试容量的0.75
// 负数的时候：-1表示正在初始化；-（1+n）表示正在扩张，n是正在扩张的线程数量
private transient volatile int sizeCtl;

// 基本结构
// Node、TreeNode和hashmap里的一样，只不过value、next用了volatile来修饰
// TreeBin 作为树的头节点，只保存root和first节点，
static final class TreeBin<K,V> extends Node<K,V> {
    TreeNode<K,V> root;
    volatile TreeNode<K,V> first;
    volatile Thread waiter;
    volatile int lockState;
    // values for lockState
    static final int WRITER = 1; // set while holding write lock
    static final int WAITER = 2; // set when waiting for write lock
    static final int READER = 4; // increment value for setting read lock
}
// ForWardingNode 转移时放在头部的即诶单，是一个空节点；
static final class ForwardingNode<K,V> extends Node<K,V> {
    final Node<K,V>[] nextTable;
    ForwardingNode(Node<K,V>[] tab) {
        super(MOVED, null, null, null);
        this.nextTable = tab;
}}
```



##### **重要方法**

使用了unsafe.class下的方法，直接操作内存来保证并发处理的安全性，使用的是硬件的安全机制

```java
// 返回table中指定位置的节点，原子操作
@SuppressWarnings("unchecked")
static final <K,V> Node<K,V> tabAt(Node<K,V>[] tab, int i) {
    return (Node<K,V>)U.getObjectVolatile(tab, ((long)i << ASHIFT) + ABASE);
}

// 利用CAS，在指定位置设定值
static final <K,V> boolean casTabAt(Node<K,V>[] tab, int i,
                                    Node<K,V> c, Node<K,V> v) {
    return U.compareAndSwapObject(tab, ((long)i << ASHIFT) + ABASE, c, v);
}

//在指定位置设定值，原子操作
static final <K,V> void setTabAt(Node<K,V>[] tab, int i, Node<K,V> v) {
    U.putObjectVolatile(tab, ((long)i << ASHIFT) + ABASE, v);
}
```



##### **initTable**

1. while（table为空 ｜｜ length == 0）；会进入初始化的循环；
2. 如果sizeCtl的值小于0，那么表示别的线程正在初始化或者扩容，Thread.yield()线程让步；
3. 否则就使用CAS的将sizeCtl设为-1，表示我这个线程要开始初始化了；
4. 如果指定了大小，就用指定的值创建，否则使用16初始化容量，sizeCtl中保存数组长度*负载因子；



##### **put**

1. key，value不能为空，否则抛出异常；
2. 获取key的hash值，spread（key.hashcode()）;使用binCount来统计这个节点一共有多少元素；
3. 如果table为空，先调用initTable进行初始化；
4. 通过hash值定位到table中的位置，如果这时候位置为空，表示还没有元素存在，直接通过CAS的方式添加【没有hash冲突，没有加锁】；
5. 如果这个位置的头节点hash值是MOVED，表示正在进行数组扩张的数据复制阶段，调用helpTransfer来允许当前线程参与复制；
6. 不满足以上情况，表示这个位置有元素，**采用synchronized方式加锁【加锁了】**；加锁之后遍历元素，如果是红黑树调用putTreeVal方法；找到的话替换值，如果没找到的话，**则添加在链表的最后面【尾插】**。
7. 如果节点数量达到8个，调用treeifyBin方法，扩张数组或者转化为红黑树；
8. 调用addCount，判断当前数组中的元素是否达到了sizeCtl的量，如果达到了，进入transfer方法扩容；
9. 返回旧的值或者null；



##### **treeifyBin**

1. 数组长度不到64，先调用tryPresize进行数组扩容；

2. 否则，**synchronized锁住当前位置的首元素【加锁】**，将该位置的链表转化为红黑树；转换完之后用CAS来替换。

3. tryPresize中

   + 传入的参数size是原table长度的2倍；判断这个size是不是大于等于容量上限的一半，如果超过了直接用容量上限，否则通过tableSizeFor算出来；得到目标size大小

   + 判断table有没有初始化，初始化的大小选择sizeCtl和刚刚算出来的大小c中比较大的一个；之后的操作同初始化initTable；

     > 在tryPresize中初始化是因为如果第一次put不是单个元素的put，而是putall，这时候没有调用initTable而是tryPresize

   + **目标扩容size小于扩容阈值，或者容量超过最大限制时，不需要扩容**，直接return；

   + 否则开始扩容，如果有线程已经在扩容了，就帮助扩容。扩容的主要方法是transfer



##### **transfer**

1. 先计算一个stride步长，表示一个线程处理的数组长度，用来控制对CPU的使用，防止扩容占用了太多线程；每个线程最少处理16个长度的数组元素；
2. 如果复制的目标nextTable是null，新初始化一个长度2倍的table；第一个开始扩容的线程需要干这个事情；
3. 创建一个ForWardingNode，用来控制并发，这是一个空的标志节点，当这个位置为空或者已经被转移了，就设置为forwardingnode，hash值为-1MOVED
4. 否则，【通过synchronized加锁】，lastrun之后的部分直接接进新链表，然后逐个复制节点，插入在【**链表头】**，【由于lastrun机制，新链表不是严格倒序的】



##### **get**

很普通，不加锁，不同步。



##### **总结**

1. 读操作，不同步；
2. 只用tryPresize、helpTransfer、addCount方法会调用transfer进行扩容；【3种触发扩容的情况】
   + tryPresize：在put新元素的时候如果超过8，调用treeifyBin，数组长度小于64，调用tryPresize；putAll方法中用它初始化；
   + helpTransfer：当一个线程要操作table中的元素，如果这时候hash值是MOVED，表明正在扩容，调用helpTransfer来帮助扩容；【如果所有node都被分配或者扩容线程达到上限就放弃】
   + addCount：添加新元素，总量超过了threshold也解释sizeCtl的时候进行扩容；
3. 如何同步？主要通过synchronized和unsafe的方法（包括CAS）来完成，包括
   + 在获取sizeCtl或者某个位置的Node，使用的都是unsafe方法，设置sizeCtl的时候用CAS方法来设定；
   + 在需要在某个位置设置节点的时候【put】，或者把某个位置的节点复制到扩容后的table中的时候【transfer】，转化红黑树的时候【treeifyBin】，通过synchronized来同步；
4. 并发的重点：将table拆分，每个线程transfer的时候处理属于自己的部分；每个线程在处理自己桶中的数据时transfer。
5. 此时线程2访问到了ForwardingNode节点，如果线程2执行的put或remove等写操作，那么就会先帮其扩容。如果线程2执行的是get等读方法，则会调用ForwardingNode的find方法，去nextTable里面查找相关元素。
6. **不在递归中使用computeIfAbsent方法，就不会产生奇怪的bug**

#### HashSet

hashset的底层是hashmap实现，继承AbstactSet实现了set等接口。

```java
private transient HashMap<E,Object> map;
private static final Object PRESENT = new Object(); // 帮助map实现set

```



##### **add**

```java
public boolean add(E e) {
    return map.put(e, PRESENT)==null; // set只需要key所以value里存的是一样的present
  // map的put方法如果key原来不存在，返回的是null，否则返回的是oldvalue
  // 所以set只需要判断返回值是不是null就能知道原来map里有没有这个key
}
```

```java
// 一个绕口的例子
import java.util.HashSet;
public class JavaTest
{
    public static void main(String[] args){
        HashSet<Person> hs = new HashSet<Person>();
        Person p = new Person("张三", 21); // 对象p1， hash假设是a，直接被加入set
        hs.add(p);
        p.setName("李四");
        p.setAge(22);
        hs.add(p); // 这时候对象p1的hash变b，所以不会发生hash冲突，可加入，但是原来的张三变成了李四
        hs.add(new Person("李四", 22)); // 对象p2，hash为b，发生冲突，加入失败
        hs.add(new Person("张三", 21)); // 对象p3，hash为a，发生冲突，但是和在a位置的p1此时equal结果是false，判断不等，可以加入
        System.out.println(hs); // 所以结果是【李四p1，李四p1，张三p3】
    }
}

class Person
{
    private String name;
    private int age;
// 其他的get set略去
    public int hashCode(){
        return name.hashCode() + age * 21; //name和age一样hashcode就会一样
    }

    public boolean equals(Object obj){ // 如果是同一个对象 或者 name/age一样，true
        //增加了判断是否为同一个对象
        if (this == obj){
            return true;
        }
        if (obj instanceof Person){
            Person p = (Person) obj;
            return name.equals(p.getName()) && age == p.getAge();
        } else {
            return false;
        }
    }

    public String toString(){
        return name + "--" + age;
    }
}                                   
```



#### TreeMap

TreeMap是基于红黑树的NavigableMap、SortedMap实现的，该集合最重要的特点就是可以排序。

+ TreeMap中的类型实现Comparable接口，并实现compareTo的方法；
+ 写一个比较类，实现Comparator接口，并实现compare方法，然后作为TreeMap构造方法的参数

```java
private final Comparator<? super K> comparator;
private transient Entry<K,V> root = null; // 根节点
private transient int size = 0; // 
private transient modCount = 0; // fail-fast
// 用于导航的Set和Map
private transient EntrySet entrySet;
private transient KeySet<K> navigableKeySet;
private transient NavigableMap<K,V> descendingMap;
// 红黑树颜色枚举
private static final boolean RED = false;
private static final boolean BLACK = true;

static final class Entry<K,V> implements Map.Entry<K,V> {
    K key;
    V value;
    Entry<K,V> left; // 左子树
    Entry<K,V> right; // 右子树
    Entry<K,V> parent; // 父节点
    boolean color = BLACK;
    //用key，value和父节点构造一个Entry，默认为黑色
    Entry(K key, V value, Entry<K,V> parent) {
        this.key = key;
        this.value = value;
        this.parent = parent;
    }
}
```



##### **put**

1. 如果root为空，则直接新建为根节点；
2. cmp记录比较结果，负数插入左子树；parent记录要插入的父节点；
3. 如果有指定的比较器，优先使用比较器来比较，新插入的key小，向左子树查找，否则往右查找；如果找到了相同的key，覆盖这个值。直到当前节点为空，则为插入的位置。
4. 没有指定的comparator，从key中获取comparable，同理寻找。
5. 插入新节点之后，调用fixAfterInsertion，对红黑树进行调整，以保持平衡。
6. size++；modCount++；



##### 一致性hash

**一致哈希** 是一种特殊的哈希算法。在使用一致哈希算法后，哈希表槽位数（大小）的改变平均只需要对K/n个关键字重新映射，其中K是关键字的数量，n是槽位数量。然而在传统的哈希表中，添加或删除一个槽位的几乎需要对所有关键字进行重新映射。

**分布式缓存问题**

如果我们有三台Memcached缓存服务器，如何路由数据到不同的缓存服务器呢？

我们很容易想到通过hash的方法来路由请求，通过target=hash(key) %n，其中n是服务器数量。
但是以上方法会有一个问题，就是当某台服务器宕机或者新增服务器时，大量的缓存内容会变更目标地址，所以会导致大量的缓存失效和重设。

一致哈希算法的**主要思想是将每个缓存服务器与一个或多个哈希值域区间关联起来**，其中**区间边界**通过计算缓存服务器对应的哈希值来决定。如果一个缓存服务器被移除，则它所对应的**区间**会被并入到邻近的区间，其他的缓存服务器不需要任何改变。

<img src="/Users/chengleiyi/Library/Application Support/typora-user-images/image-20200722092806885.png" alt="image-20200722092806885" style="zoom:50%;" />

1. 首先求出memcached服务器（节点）的哈希值，并将其配置到0～2^32的圆环上
2. 然后采用同样的方法求出存储数据的键的哈希值，并映射到相同的圆上
3. 然后从数据映射到的位置开始顺时针查找，将数据保存到找到的第一个服务器上
4. 如果超过2^32仍然找不到服务器，就会保存到第一台memcached服务器上



**带虚拟节点的一致性哈希算法**

以上算法存在一个问题，就是在节点数目很少，且节点的hash值较为集中的时候，会造成很明显的数据分配不均的问题。

当只存在两个节点时，大量数据会分配在nodeA上，只有少量数据会分配在nodeB上。

为了解决上述问题，**引入了虚拟节点的概念**。即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器ip或主机名的后面增加编号来实现。例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “NodeA#1”、“NodeA#2”、“Node A#3”、“NodeB#1”、“NodeB#2”、“NodeB#3”的哈希值，于是形成六个虚拟节点。最后再根据虚拟节点-真实节点的映射规则，求出真实节点。

```java
public class ConsistentHashing{
  // 服务器信息
  private static final String[] SERVERS = new String[]{"node1","node2","node3"};
  // 一致性哈希环
  private static SortedMap<Long, String> hashRing = new TreeMap<>();
  // 虚拟节点数目
  private static final int VIRTUAL_NODE_COUNT = 32;
  // hash环最大的位置2^32
  private static final long MAX_HASH_LOCATION = 1 << 32;
  
  public ConsistantHashing(){
    initHashRingWithVirualNode();
  }
  public static void initHahsRingWithVirtualNode(){
    for(String server : SERVERS){
      for(int i = 0; i < VIRTUAL_NODE_COUNTl i++){
        String virtualserver = server + "#virtual" + i;
        hashRing.put(hash(virtualserver), virtualserver);
      }
    }
  }
  
  // 为key分配server
  public static String allocateServer(String key){
    long hashValue = hash(key);// hash算法，可以更改的
    // TreeMap保证所有的key有序排列，可以当作一个首尾相连的环。
    // 所以为key分配server的时候，查找第一个比该key的hashvalue大的serverkey
    // 这个就是其对应的server
    SortedMap<Long, String> tail = hashRing.taiMap(hashValue);
    String virtualNode;
    if(tail.isEmpty()){
      // 如果没有比他大的server，则使用第一个server（环状）
      virtualNode = hashRing.get(hashRing.firstKey());
    } else {
      // 比他大的第一个server
      virtualNode = hashRing.get(tail.firstKey());
    }
    return getRealNode(virtualNode); // 把虚拟节点转回机器节点
  }
}
```



#### LinkedHashMap

LinkedHashMap继承自HashMap，通过维护一条双向链表，解决了hashmap中无序的问题。

HashMap是无序的，而LinkedHashMap就弥补了该缺点，**默认为插入顺序**，即最后插入的key-value会加到双向链表的尾部，**若定义accessOrder为true的话，则为访问顺序**，即put key-value后，调用get，replace等方法，都会将节点放到链表尾部，即符合LRU算法，经常使用的数据放在链表尾部。

这里可以通过**重写removeEldestEntry方法来自定义自己的LRU算法**，即put一个key-value后，根据自己业务的LRU需求，将最旧的数据节点(即双向链表节点的头节点)删除。

```java
public class LinkedHashMap<K,V> extends HashMap<K,V> implements Map<K,V>{
  static class LinkedHashMapEntry<K,V> extends HashMap.Node<K,V>{
    LinkedHashMapEntry<K,V> before, after; // 比一般的Node多了before 和 after
    //分别指向当前节点的前后节点
    LinkedHashMapEntry(int hash, K key, V value, Node<K,V> next){
      super(hash, key, value, next);
    }
  }
  
  // 双向链表的头节点和尾节点
  transient LinkedHashMapEntry<K,V> head,tail;
  final boolean accessOrder; //迭代顺序，true的时候为访问顺序，默认false是插入顺序
}
```

LinkedHashMap就是复写了HashMap提供的几个抽象方法，在**每次插入数据，或者访问、修改数据时，会增加节点、或调整链表的节点顺序以改变它迭代遍历时的顺序**。

##### **put**

并没有重写put和putVal方法，而是重写了putVal中调用的以下三个方法

1. 构建新节点时调用的newNode方法，去构建LinkedHashMapEntry节点，而不是Node节点；逻辑上时没有改变的，创建新的Node**然后接在链表的尾部**。

2. 节点被访问后，表示put的key已经存在了，调用 afterNodeAccess(e) 方法进行排序：如果accessOrder是true，且这个节点不在链表的尾部，需要将它移动到尾部。modCount++；

3. 节点被插入后，afterNodeInsertion(evict)

   ```java
   void afterNodeInsertion(boolean evict){
     LinkedHashMapEntry<K,V> first;
     // removeEldestEntry方法返回默认 false，所以不会删除最老的节点
     // 构建LRUcache的时候 override这个removeEldestEntry方法，让他打到capacity时返回true
     if (evict && (first = head) != null && removeEldestEntry(first)) {
       K key = first.key;
       removeNode(hash(key), key, null, false, true);
   	}
   }
   ```



##### **get**

get或者getOrDefault的时候，与hashmap的不同在于，如果accessOrder为true，会调用afterNodeAccess方法移动被访问节点到链表尾部。

LinkedHashMap的contaisValue比一般HashMap的效率要高，因为只需要沿着链表遍历一次就好，而hashmap先遍历数组，嵌套遍历链表。



**TreeMap和LinkedHashMap是如何保证它的顺序的？**

TreeMap是通过实现SortMap接口，能够把它保存的键值对根据key排序，基于红黑树，从而保证TreeMap中所有键值对处于有序状态。LinkedHashMap则是通过插入排序（就是你put的时候的顺序是什么，取出来的时候就是什么样子）和访问排序（改变排序把访问过的放到底部）让键值有序。

**哪种效率更高？**

Treemap的排序是红黑树的排序，插入一个新数据时间复杂度logn；



#### Collections.synchronized

`Collections.synchronizedXXX ( Collection<T> t)`

+ Collection
+ List
+ Map
+ Set

以上几个方法是Collections工具类把普通集合变为多线程安全集合的主要方法。

**原理**

SynchronizedMap类

```java
private static class SynchronizedMap<K,V> implements Map<K,V>{
  private final Map<K,V> m;
  final Object mutex; // 锁对象
  // 之后所有的方法，都先synchronized（mutex）然后调用map的方法
}
```



#### CopyOnWriteArrayList

##### **COW是什么？**

Copy-on-write的意思就是，当你想要修改某个东西，不是在他的原件上修改，而是将他拷贝一份，在复印件上修改然后替换原件。是程序设计中的一种优化策略，一种延时懒惰策略。

从JDK1.5开始Java并发包里提供了两个使用`CopyOnWrite`机制实现的并发容器,它们是`CopyOnWriteArrayList`和`CopyOnWriteArraySet`。`CopyOnWrite`容器非常有用，可以在非常多的并发场景中使用到。



##### **add**

`add()`在添加集合的时候加上了锁，保证了同步:**写的时候加锁，复制；读取的时候不加锁**

```java
final ReentrantLock lock = this.lock;
lock.lock(); //加锁
try{
  Object[] elements = getArray();
  int len = elements.length;
  Object[] newElements = Arrays.copyOf(elements, len+1); // 拷贝新数组
  newElements[len] = e;
  setArray(newElements); //替换新数组
  return true;
} finally {
  lock.unlock();
}
```

**使用场景**

+ 读多写少的时候，因为频繁的写，会导致复制的开销；
+ 集合不大，写的时候会复制，占用内存；
+ 实时性要求不高，因为有时候会读到旧的数据集合；

##### **为什么没有ConcurrentArrayList？**

很难去开发一个通用并且没有并发瓶颈的线程安全的List。拿contains() 这样一个操作来说，当你进行搜索的时候如何避免锁住整个list？

了解写时复制机制、了解其适用场景、思考为什么没有ConcurrentArrayList。另一方面，Queue 和Deque (基于Linked List)有并发的实现是因为他们的接口相比List的接口有更多的限制，这些限制使得实现并发成为可能。

#### BlockingQueue

BlockingQueue 继承于 Queue 接口：

+ 插入：add 队列满了会抛异常；offer返回boolean并且队列满了不会抛异常；put当队列满了该方法会被阻塞，直到队列有空位再进行插入；offer（timeout）类似put方法，但是限制了超时时间。
+ 删除：remove（obj）返回boolean；poll当队列为空返回null；take当队列为空获取队头数据的线程会被阻塞；poll（timeout）类似take但是限制了超时时间；
+ 查看：element查看队头元素，队列为空抛出异常；peek队列为空返回null

| LinkedBlockingQueue   | 链表 | 有界     | FIFO 默认非公平                              | 1lock |
| --------------------- | ---- | -------- | -------------------------------------------- | ----- |
| ArrayBlockingQueue    | 数组 | 有界     | 默认/最大Integer.MAX 、非公平                | 2lock |
| PriorityBlockingQueue |      | 无界     | 默认采用自然顺序进行排序                     |       |
| DelayQueue            |      | 无界     |                                              |       |
| SynchronousQueue      |      | None     | 一个put需要等待take，默认非公平              |       |
| LinkedTransferQueue   | 链表 | 无界     | transfer方法                                 |       |
| LinkedBlockingDeque   | 链表 | 双向有界 | 只有当数据对象的延时时间达到，才能插入到队列 |       |

##### LinkedBlockingQueue

用链表实现的有界阻塞队列，默认和最大长度为Integer.MAX_VALUE。按照先进先出的原则排序，队列有两个锁，生产和消费，默认都是非公平锁。

+ 单向链表存储元素；

+ 初始化时可以指定大小，默认为int的最大值；有界队列

+ 两把可重入锁和两个条件分别控制入队出队，不需要对整个队列上锁；

  

继承自AbstractQueue

```java
static class Node<E> {
  E item;
  Node<E> next;
  Node<E x> { item = x ;}
}
private final int capacity;
private final AtomicInteger count = new AtomicInteger();
transient Node<E> head;
private transient Node<E> last;
private final ReentrantLock takeLock = new ReentrantLock();
private final ReentrantLock putLock = new ReentrantLock();
private final Condition notEmpty = takeLock.newCondition();
private final Condition notFull = putLock.newCondition();
```



1. 构造方法：无参构造，默认大小MAX_VALUE；有参构造，设置capacity，创建一个item为null的Node，头尾都指向他；

2. offer方法【带超时限制的阻塞，一般非阻塞】：参数e不能为null，通过count和capacity比较，如果已经满了，返回false；

   putlock上锁，再判断一次count是否小于capacity，调用enqueue插入链表，count.getAndIncrement；

   判断还有没有存储容量，有，notFull.signal 唤醒下一个线程；

   释放putlock；

   如果队列中已经有1个元素了，signal. notEmpty 需要唤醒一个等待消费的线程；

3. put方法【阻塞】：参数e不能为null；

   putlock上锁（lockInterruptibly），while（count大小等于capacity，满了）循环notFull.await()；

   调用enqueue插入队列，count.getAndIncrement();

   判断还有么有存储容量，有，notFull.signal 唤醒等待队列的线程；

   释放putlock；

   如果队列中已经有1个元素了，signal. notEmpty 需要唤醒一个等待消费的线程；

4. poll方法【带超时限制的阻塞，一般非阻塞】：takeLock上锁（lockInterruptibly）；

   while( count.get == 0 ，队列为空) notEmpty.awaitNanos(nanos) 循环非空condition等待，并检查时间到没到，超时返回null；

   调用dequeue出队列；count.getAndDecrement

   如果队列还有元素，notEmpty.signal 唤醒一个等待获取的线程；

   takelock解锁；

   如果队列中有空位，唤醒等待入队的线程，signalNotFull

5. take【阻塞】类似poll



| take  put  | 阻塞方法   |      |
| ---------- | ---------- | ---- |
| poll offer | 非阻塞方法 |      |

 

**比较**

- LinkedBlockingQueue底层用链表实现：ArrayBlockingQueue底层用数组实现
- LinkedBlockingQueue**支持不指定容量的无界队列**（长度最大值Integer.MAX_VALUE）；ArrayBlockingQueue必须指定容量，无法扩容
- LinkedBlockingQueue支持懒加载：ArrayBlockingQueue不支持
- ArrayBlockingQueue入队时不生成额外对象：LinkedBlockingQueue需生成Node对象，消耗时间，且GC压力大
- LinkedBlockingQueue的入队和出队分别用两把锁保护，无竞争，二者不会互相影响；ArrayBlockingQueue的入队和出队共用一把锁，入队和出队存在竞争，一方速度高时另一方速度会变低。不考虑分配对象、GC等因素的话，ArrayBlockingQueue并发性能要低于LinkedBlockingQueue

可以看到，LinkedBlockingQueue整体上是优于ArrayBlockingQueue的。**在小数据量大竞争的情况下使用LinkedBlockingQueue更快，但是在大数据量的情况下，ArrayBlockingQueue更快**



##### ArrayBlockingQueue

+ 通过数组来存储元素；
+ 通过takeIndex和putIndex来标记下一次操作的位置；
+ 利用重入锁+2个condition来保证并发安全；非空、非满条件
+ **初始化的时候必须指定大小，通过参数来控制是公平锁还是非公平锁；**【所以不需要扩容】
+ 入队方法：抛出异常的add，阻塞的put，非阻塞，可控制超时的offer；
+ 出队方法：抛出异常的remove，阻塞的take， 非阻塞、可控制超时的poll；
+ 



##### PriorityBlockingQueue

+ 使用数组来存储元素，PriorityQueue域只是用来序列化的；
+ 使用一个锁+1个notEmpty条件控制并发【因为是无界的，会自动扩容，不存在满了的情况】；
+ 扩容的时候使用一个volatile int 变量的CAS操作来控制只有一个线程能够进行扩容；
+ 入队的时候使用自下而上的堆化；
+ 出队的时候使用自上而下的堆化；
+ 默认初始容量11，最大容量Integer.MAX_VALUE-8
+ 入队的时候检查size是否大于等于queue的长度，如果是就需要扩容：旧容量小于64则翻倍，旧容量大于64则增加一半



##### DelayQueue

实现了BlockingQueue接口，还实现了Delayed接口，DelayQueue中存储的所有元素必须实现Delayed接口

Delayed继承自Comparable接口，并且定义了一个getDelay方法，表示还有多少时间到期，到期了返回一个小于等于0的数。

```java
// 用于控制并发的锁
private final transient ReentrantLock lock = new ReentrantLock();
// 优先级队列
private final PriorityQueue<E> q = new PriorityQueue<E>();
// 用于标记当前是否有线程在排队（仅用于取元素时）
private Thread leader = null;
// 条件，用于表示现在是否有可取的元素
private final Condition available = lock.newCondition();
//
// 延时队列主要是通过优先级队列和ReentrantLock和condition一起实现的，重入锁和条件用来控制并发安全。
// 因为优先级队列是无界的，所以只需要一个条件

// 因为延迟队列是无界的，所以入队操作不会阻塞不会超时，四个入队方法没有区别
public boolean offer(E e) {
    final ReentrantLock lock = this.lock;
    lock.lock(); // 先上锁
    try {
        q.offer(e); // 加到优先级队列
        if (q.peek() == e) { // 如果添加的元素是堆顶，通知等待在available条件的线程
            leader = null;
            available.signal();
        }
        return true;
    } finally {
        lock.unlock(); // 解锁
    }
}

// 出队列：抛出异常的；阻塞的take；不阻塞的 || 超时的 poll
public E poll() {
    final ReentrantLock lock = this.lock;
    lock.lock();
    try {
        E first = q.peek();
      // 比一般的优先级队列多检查getDelay，延时有没有到期
        if (first == null || first.getDelay(NANOSECONDS) > 0) 
            return null;
        else
            return q.poll();
    } finally {
        lock.unlock();
    }
}

// take方法需要阻塞
// 1. 加锁 lockInterruptibly
// 2.1 判断堆顶的元素是否为空，空的话直接available.await阻塞等待；
// 2.2.1 判断堆顶元素是否延时到期，到期了直接poll；
// 2.2.2 没到期， 判断前面是否有元素在等待 leader != null, 直接等待available.await();
// 2.2.3 没有线程在等待，把leader设为当前线程，等待delay时间到available.awaitNanos(delay);
// 2.3 成功出队列之后，如果队列中还有元素，把等待的线程放到AQS队列中, available.signal();
// 2.4 解锁

```

**java中的线程池实现定时任务是直接用的DelayQueue吗？**

**当然不是，ScheduledThreadPoolExecutor中使用的是它自己定义的内部类DelayedWorkQueue，其实里面的实现逻辑基本都是一样的，只不过DelayedWorkQueue里面没有使用现成的PriorityQueue，而是使用数组又实现了一遍优先级队列，本质上没有什么区别。**



##### SynchronousQueue

无缓冲阻塞队列，用来在两个线程之间移交元素

+ 队列里面会自旋

+ 使用volatile transfer来交换元素；

+ 有两种实现方式：公平（队列）/  非公平（栈）

   内部定义了Transferer抽象类，主要定义了一个transfer方法用来传输元素。这个抽象类有两种实现，TransferStack和TransferQueue；

  默认使用非公平模式，栈结构；公平模式使用的是堆结构。

  

+ 无缓冲队列并不是真的无缓冲，他内部使用队列或者栈来存储好汉线程和元素的节点。





#### 常见问题

- ConcurrentHashMap是如何在保证并发安全的同时提高性能？
- ConcurrentHashMap是如何让多线程同时参与扩容？
- LinkedBlockingQueue、DelayQueue是如何实现的？
- CopyOnWriteArrayList是如何保证线程安全的？



## JUC锁/并发 

#### 怎么保证三大要素？



#### LockSupport

LockSupport的核心函数都是基于Unsafe类中定义的park和unpark函数

+  park函数，阻塞线程，并且该线程在下列情况发生之前都会被阻塞：① 调用unpark函数，释放该线程的许可。② 该线程被中断。③ 设置的时间到了。并且，当time为绝对时间时，isAbsolute为true，否则，isAbsolute为false。当time为0时，表示无限等待，直到unpark发生。
+ unpark函数，释放线程的许可，即激活调用park后阻塞的线程。这个函数不是安全的，调用这个函数时要确保线程依旧存活。
+ 使用的时候直接用LockSupport.park() ， 因为这个类的构造函数是private，无法实例化。



#### CAS

CAS是Conmpare And Swap(比较和交换)的缩写，是一个原子操作指令。

JUC下的atomic包提供了一组原子类。

##### AtomicInteger实现

原子integer类继承了Number类，底层使用unsafe的cas方法。

```java
public class AtomicInteger extends Number implements java.io.Serializable {
  private volatile int value;

  public final int get() {
    return value;
  }
  public final int getAndlncrement{
    for (;;) { //CAS自旋，一直尝试，直达成功
      int current = get。；
      int next = current + 1;
      if (compareAndSet(current, next))
        return current;
      }
    }
  public final boolean compareAndSet(int expect, int update) {
    return unsafe.compareAndSwaplnt(this, valueOffset, expect, update);
  }
} 
```



##### ABA问题

尽管线程one的CAS操作成功，但是不代表这个过程就是没有问题的。

部分乐观锁的实现是通过版本号（version）的方式来解决ABA问题，乐观锁每次在执行数据的修 改操作时，都会带上一个版本号，一旦版本号和数据的版本号一致就可以执行修改操作并对版本 号执行+1操作，否则就执行失败。因为每次操作的版本号都会随之增加，所以不会出现ABA问 题，因为版本号只会增加不会减少。



**了解AtomicInteger实现原理、CAS适用场景、如何实现乐观锁**

#### AQS

AQS的功能：通过继承AQS可以实现自定义同步锁，子类重写tryAcquire等一系列方法来控制AQS内部一个final int state的同步变量。使用Lock聚合AQS可以实现一系列锁。

##### 内部实现

###### CLH锁

CLH锁是自旋锁的一种改良，将竞争资源的线程排成一个队列，每个线程只在前一个线程的标志位上进行自旋，当头节点释放锁，就把自己的标志位设为false，这样后继线程在自旋的过程中发现标志位变成false，就能获取锁进入临界区。

CLH锁这种思想叫做Local Spin，最大化减少CPU缓存失效次数。现在的多核CPU一般每一个物理核都会有自己的缓存，假如是普通的自旋锁，所有CPU核都自旋在一个标志位上，因为这个标志位竞争非常激烈，所以标志位经常会变化，每当标志位变化时，所有CPU的缓存就会失效，这样显然无法最大程度上利用CPU的缓存，而在CLH锁的设计中，每个线程只需要在自己的前继的标志位上自旋即可，而前继的标志位仅仅在前继释放锁的时候会发生变化，这样每个CPU核就可以一直在自己的本地缓存上自旋（所以称之为Local Spin）而不会出现频繁的缓存失效，减少了缓存失效，锁算法效率自然就提高了。

```java
// 标准的CLH锁实现，实现Lock接口，利用ThreadLocal来存储线程相关的数据，前驱节点和代表当前线程的节点
public class CLHLock implements Lock {
    AtomicReference<QNode> tail = new AtomicReference<QNode>(new QNode());
    ThreadLocal<QNode> myPred;//代表前继的节点
    ThreadLocal<QNode> myNode;//代表当前线程的节点
    public CLHLock() {
        tail = new AtomicReference<QNode>(new QNode());
        myNode = new ThreadLocal<QNode>() {
            protected QNode initialValue() {
                return new QNode();
            }
        };
        myPred = new ThreadLocal<QNode>() {
            protected QNode initialValue() {
                return null;
            }
        };
    }
    
    public void lock() {
        QNode qnode = myNode.get();
        qnode.locked = true;
        QNode pred = tail.getAndSet(qnode);
        myPred.set(pred);
        //在前继节点的标志位上自旋
        while (pred.locked) {}
    }
    
    public void unlock() {
        QNode qnode = myNode.get();
        //将当前线程节点的标志位置为false
        qnode.locked = false;
        //此时代表前继节点的QNode对象已经没有用了,这里将其复用
        myNode.set(myPred.get());
    }
}
```

###### 源码

抽象队列同步器，内部定义了很多锁相关的方法。volatile int state +  CLH队列

```java
// AQS中的CLH队列中的节点Node ，每个节点对应着一个等待资源的线程
static final class Node { 
	/** 标记节点正在共享模式中等待 */ 	/**标记节点正在独占模式中等待 */ 
	static final Node SHARED = new Node();
	static final Node EXCLUSIVE = null; 
 
	static final int CANCELLED = 1; // 等待超时或者中断，需要从同步队列中取消该节点
	static final int SIGNAL = -1; // 唤醒状态，只要它的前驱节点释放了锁，就会通知signal状态的节点线程执行
  static final int CONDITION = -2;  // 这个节点的线程等待在condition上，当其他线程调用condition.signal之后，这个节点才会从condition状态变成等待获取同步锁的状态
  static final int PROPAGATE = -3; // 共享模式使用的状态，表示可运行，后继节点会传播唤醒操作
	volatile int waitStatus; // 初始为0，取值范围是上面4个final int
  
	volatile Node prev; //CLH队列的前继
  volatile Node next; //CLH队列的后继
  volatile Thread thread; //这个节点所代表的线程
  Node nextWaiter; //Condition队列的后继
 // AQS中维护着两个队列（两个队列都是由Node组成），一个队列就是CLH锁算法中的那个队列（CLH队列），另一个是由AQS的内部类ConditionObject维护的Condition队列（用于支持线程间的同步，提供await, signal, signalAll方法）
}
```

**acquire方法--独占模式**

acquire方法在几次尝试获得锁失败后成功地将前继线程的waitStatus设置为SIGNAL，然后阻塞自己。之后在某个时刻会被前继线程唤醒，然后有经过几次争抢后可能会成功地获得锁。

```java
//独占模式获取锁
public final void acquire(int arg) {
  // 1. 调用tryAcquire方法获取线程同步状态
  //    获取失败则会被构造成Node并通过addWaiter方法加入同步队列的尾部
    if (!tryAcquire(arg) && acquireQueued(addWaiter(Node.EXCLUSIVE), arg))
        selfInterrupt();
}
// 2. 进入acquireQueued方法：
// 		2.1 先检查自己的前驱节点是不是头节点，如果是则我有获取锁的可能，调用tryAcquire；获取成功了就把自己对应的Node设为head头节点，把原来的头节点出队；
//    2.2 如果我的前驱不是head ｜｜ 上面的代码获取锁失败了，那就要考虑是不是要把自己阻塞了：调用shouldParkAfterFailedAcquire
//		如果我的前驱节点处于signal的状态，那我现在一定没有机会，直接用lockSupport的park方法阻塞自己；
//		如果我的前驱节点是Cancelled状态，他被取消了，所以我就跳过他，往前找直到前驱不是cancelled；
// 		如果我的前驱节点是0或者其他情况，调用compareAndSetWaitStatus(pred, ws, Node.SIGNAL);把前驱节点状态设为signal。【因为这些操作都在循环内，所以我要么获得锁（前驱节点是head并且释放了锁），要么阻塞】
final boolean acquireQueued(final Node node, int arg) {
    boolean failed = true;
    try {
        boolean interrupted = false;
        for (;;) { // 死循环的方式获取锁
            final Node p = node.predecessor();
            if (p == head && tryAcquire(arg)) {
                setHead(node);
                p.next = null; // help GC
                failed = false;
                return interrupted;
            }
            if (shouldParkAfterFailedAcquire(p, node) &&
                parkAndCheckInterrupt())
                interrupted = true;
        }
    } finally {
        if (failed)
            cancelAcquire(node);
    }
}
```

**release--独占模式**

```java
// 释放方法：tryRelease成功，就将头节点的下一个节点对应的线程唤醒
public final boolean release(int arg) {
    if (tryRelease(arg)) {
        Node h = head;
        if (h != null && h.waitStatus != 0)
            unparkSuccessor(h);// 将h节点的后继唤醒
        return true;
    }
    return false;
}
// unparkSuccessor：如果发现node的后继节点为null或者CANCELLED时，会找出离tail最远（或者说离node节点最近）的一个非CANCELLED节点唤醒
// 唤醒使用lockSupport.unpark(s.thread);
// 线程被唤醒之后就会回到acquire方法中的循环，不断去获取锁
```

**acquireShared--共享模式**

而共享功能是只要头节点获取锁成功，就在唤醒自身节点对应的线程的同时，继续唤醒AQS队列中的下一个节点的线程，每个节点在唤醒自身的同时还会唤醒下一个节点对应的线程，以实现共享状态的“**向后传播**”，从而实现共享功能。

1. 多个线程通过调用tryAcquireShared方法获取共享资源，返回值大于等于0就是成功；
2. 如果小于0，失败通过调用addWaiter吧该节点添加到队列尾部；
3. 如果该节点的前驱就是头节点，那么该节点再次尝试获取资源，成功，则调用setHeadAndPropagate方法把该节点设置为新的头【这就是说之前获取到资源的某个线程释放了，给他空出位置】，并唤醒队列中其他共享节点来获取资源。如果失败，再次回到队列。
4. 如果前驱不是头节点，自旋等到前驱成为头节点。

**区别**

+ addWaiter中把节点标记为shared模式
+ acquire方法在tryAcquire成功时，直接setHead将自己置为CLH队列的队头，而这里调用了一个叫做**setHeadAndPropagate**的方法。除了将自己置为头节点外，还会**继续尝试唤醒后继节点（doReleaseShared）**，让他们也来尝试争抢锁。

1. doReleaseShared方法：如果head改变就循环执行：当前节点是signal，就把自己设为0，并唤醒后继节点；如果当前节点是0，就设为propagate
2. 共享模式获取锁的自旋过程中，成功获取同步状态并退出自旋的条件就是该方法的返回值不小于0

**releaseShared--共享模式**

tryReleaseShared方法成功之后，调用doReleaseShared



**值得注意的是，acquire()和acquireShared()两种方法下，线程在等待队列中都是忽略中断的。AQS也支持响应中断的，acquireInterruptibly()/acquireSharedInterruptibly()即是，相应的源码跟acquire()和acquireShared()差不多。**



**ConditionObject内部类**

在使用ReentrantLock的时候，会调用newCondition来获取一个condition对象来作为同步条件。这个方法实际上是**委托给Sync的一个实例来获取，返回的是ConditionObject对象**。

ConditionObject内部也维护了一个队列，同样是Node节点组成。

**await方法**



**signal方法**



**signalAll方法**



###### AQS和CLH锁的区别

1. CLH是不断自旋的，而AQS在几次自旋失败之后就会阻塞，为了避免不必要的占用CPU；
2. CLH是自旋在前驱节点的标志位上，而AQS是自旋在p==head上（不断判断前驱是不是头节点），只有前驱是头节点才会尝试获取锁。



#### ReentrantLock

**⚠️ReentrantLock最常用的方法lock、unlock、newCondition都委派给sync内部类对象，而Sync就是AQS的实现。ReentrantLock实现了Lock接口，还有两个Sync的子类内部类：NonfairSync和FairSync，分别实现非公平锁和公平锁。**

###### 非公平

**lock方法**

1. 使用unsafe的CAS系列方法，尝试立刻获取锁。【非公平的地方！先尝试插队！】
2. 如果失败了就调用acquire(1) 方法，这个acquire是父类AQS提供的。【详细看👆的acquire独占模式】
3. aquire方法内部使用了tryAcquire：tryAcquire(arg)在NonfairSync中的实现(这里arg=1)：进一步调用了nonfairAcquire方法。【子类重写了tryAcquire，从而实现了非公平/公平逻辑有所区别】
4. 如果发现锁处于空闲状态（`state == 0`），则尝试获得锁，否则的话，先判断一下重入的情况，如果是重入的情况（`current == getExclusiveOwnerThread()`），则将同步状态state加1（`int nextc = c + acquires;`）

**unlock方法**

1. 委派给 sync.release(1)【也就是说公平和非公平用的是同一个tryrelease方法】
2. 将同步状态state减1，如果发现减到了零的话，则通过`setExclusiveOwnerThread`将AQS的`exclusiveOwnerThread`变量置空，如果已经减到零了，线程再次调用unlock方法的话，则会因为`Thread.currentThread() != getExclusiveOwnerThread()`的判断条件抛出`IllegalMonitorStateException`异常。

###### 公平

**lock方法**

1. 直接调用AQS提供的 acquire(1)方法;

2. 因为AQS的acquire方法依然调用了tryAcquire的方法尝试插队，所以【公平锁重写的tryAcquire方法】，采用了hasQueuedPredecessor条件来**防止插队**：

   这个条件先判断当前线程是不是CLH队列的头【有前驱，true；没有，false】，只有当它是head的时候才有资格进行CAS获取锁；

#### Semaphore

**Semaphore用来在并发下管理数量有限的资源，是典型的共享模式下的AQS的实现。也分为公平和非公平模式。**

![image-20200723134951730](/Users/chengleiyi/Library/Application Support/typora-user-images/image-20200723134951730.png)

Semaphore的内部类和ReentrantLock的内部类结构相同，Sync继承AQS，FairSync和NonfairSync继承自Sync。

+ 非公平/公平内部类里重写了AQS的tryAcquireShared，从而实现了调用AQS中【Sync继承了的】同样的acquireSharedInterruptibly 方法，却能实现不同的非公平或者公平的策略。
+ Sync类中重写了共享模式下释放tryReleaseShared，供公平/非公平共同使用。并且实现了nonfairTryAcquireShared方法供非公平类来使用。
+ 之所以没有fairTryAcquireShared方法，是公平类里的tryAcquireShared已经完成了整个逻辑，通过自旋、判断是否存在前驱节点，完成许可的获取。
+ acquireSharedInterruptibly：此方法从信号量获取一个（多个）许可permits，在提供一个许可前一直将线程阻塞，或者线程被中断。

<img src="/Users/chengleiyi/Library/Application Support/typora-user-images/image-20200723141259078.png" alt="image-20200723141259078" style="zoom:90%;" />

release方法的流程

![image-20200723141931956](/Users/chengleiyi/Library/Application Support/typora-user-images/image-20200723141931956.png)



#### CyclicBarrier

CyclicBarrier没有显示继承或实现，存在一个内部类Generation，包含一个broken属性来表示屏障是否破坏。

```java
public class CyclicBarrier {
   
    private final ReentrantLock lock = new ReentrantLock();
    private final Condition trip = lock.newCondition();

    private final int parties; // 参与的线程数量
    private final Runnable barrierCommand; // 由最后一个进入 barrier 的线程执行的操作 
    private Generation generation = new Generation(); // 当前代
    private int count;  // 正在等待进入屏障的线程数量
}
//  构造函数：parties指定关联该CyclicBarrier的线程数量，并且可以指定在所有线程都进入屏障之后的执行动作barrierAction

```

**dowait方法**

CyclicBarrir对外提供的await方法底层都是调用了dowait函数，主要流程是上锁之后判断屏障是否被损坏，线程是否被中断，等待进入屏障的线程数量是否为0，都不是就进入屏障等待。

+ lock上锁；
+ 保存当前generation，如果屏障被破坏，抛出异常；如果线程被中断，就破坏当前屏障，并且唤醒所有线程，抛出异常；
+ --count减少正在等待进入屏障的线程数量；如果数量为0，表示所有线程都进入，存在barrierAction就command.run运行，调用nextGeneration进入下一代；
+ 等待trip.await() 

**nextGeneration方法**

此函数在所有线程进入屏障后会被调用，即生成下一个版本，所有线程又可以重新进入到屏障中。

+ trip.signalAll 唤醒所有线程；恢复正在等待进入线程的数量；generation = new Generation；
+ signalAll函数会调用dosignalAll，把condition队列中的节点转移到sync队列中；

**能够导致 BrokenBarrierException 的操作：**

1. await() 当调用该方法的线程进入等待，一旦其他等待的线程被中断，或者其他等待的线程超时，或者其他线程调用CyclicBarrier.reset方法，当前线程都会抛出BrokenBarrierException；
2. 抛出异常之后，线程会停止等待，并且继续执行；
3. **如果因为中断、失败或者超时等原因，导致线程过早地离开了屏障点，那么在该屏障点等待的其他所有线程也将通过 BrokenBarrierException（如果它们几乎同时被中断，则用 InterruptedException）以反常的方式离开。**



#### CountDownLatch

CountDownLatch用于同步一个或多个任务，强制他们等待由其他任务执行的一组操作完成。CountDownLatch典型的用法是将一个程序分为n个互相独立的可解决任务，并创建值为n的CountDownLatch。当每一个任务完成时，都会在这个锁存器上调用countDown，等待问题被解决的任务调用这个锁存器的await，将他们自己拦住，直至锁存器计数结束。

CountDownLatch没有显示的继承和实现。存在一个内部类Sync，它继承自AQS。**这个Sync类重写了共享模式下的tryAcquireShared和tryReleaseShared。**

+ 初始化的时候，给定初始化状态数；

+ await方法：利用AQS在共享模式下的acquireSharedInterruptibly；

+ countDown方法：递减锁存器的计数，如果计数达到0，就释放所有的等待线程；

  > 调用sync的releaseShared（1）；进而使用AQS的doreleaseShared方法来释放



#### ReentrantReadWriteLock

实现了ReadWriteLock接口，







#### ThreadPoolExecutor

Java 通过用户线程与内核线程结合的 1:1 线程模型来实现，Java 将线程的调度和管理设置在了用户态。在 HotSpot VM 的线程模型中，Java 线程被一对一映射为内核线程。Java 在使用线程执行程序时，需要创建一个内核线程；当该 Java 线程被终止时，这个内核线程也会被回收。因此 Java 线程的创建与销毁将会消耗一定的计算机资源，从而增加系统的性能开销。

ThreadPoolExecutor的构造函数共有四个，但最终调用的都是同一个：

```java
public ThreadPoolExecutor(int corePoolSize, // 核心线程数量
                          int maximumPoolSize, // 线程池最大数量
                          long keepAliveTime, // 空闲线程存活时间
                          TimeUnit unit, // 时间单位
                          BlockingQueue<Runnable> workQueue, // 线程池所使用的缓冲队列
                          ThreadFactory threadFactory, // 线程池创建线程使用的工厂
                          RejectedExecutionHandler handler) // 拒绝策略
```

##### 线程池执行任务逻辑

1. 判断核心线程数是否已满，没有满则创建一个新的线程执行任务；
2. 如果已经超过核心线程数，判断工作队列有没有满，没有满则暂存在工作队列；
3. 如果工作队列已经满了，线程数小于最大线程数，则创建新的线程执行任务；
4. 如果超过最大线程数量，按照拒绝策略来处理任务，4中拒绝策略
   + abort：抛出RejectedExecutionException异常
   + callerRuns：在正在运行的线程池中处理该任务
   + discardOldestr：放弃等待中的最老的未处理任务
   + discard：丢弃任务

##### 几种线程池比较

Executors创建线程池实际上还是使用了【ThreadPoolExecutor的构造函数】

1. Executors#newCachedThreadPool：可缓存的线程池。

   核心线程：0；

   最大线程数量：Integer.MAX_VALUE;

   工作队列：synchronousQueue<Runnable>()

   空闲存活时间：60s

   所以这个线程池会一直创建新的线程，在资源有限的情况下会OOM（stack）

   >  java.lang.OutOfMemoryError: unable to create new native thread  不能再创建新线程

2. Executors#newFixedThreadPool方法：固定核心线程数量

   核心线程：指定nThreads

   最大线程数：同核心线程数

   工作队列：LinkedBlockingQueue<Runnable> 无界队列（Integer.MAX_VALUE），所以不会创建非核心线程，请求堆积在工作队列，也可能OOM。

   >  java.lang.OutOfMemoryError: GC overhead limit exceeded GC开销超过限制

3. Executors#newSingleThreadExecutor：单线程

   核心线程：1；

   工作队列：LinkedBlockingQueue<Runnable> 可以认为是无界队列，资源有限的时候会OOM

   因为无界队列，所以最大线程数量和空闲存活时间都无效，压根就不会创建非核心线程

4. newScheduledThreadPool：

   最大线程池数量：Integer.MAXVALUE;

   DelayQueue（延迟队列）是一个任务定时周期的延迟执行的队列。根据指定的执行时间从小到大排序，否则根据插入到队列的先后排序。

5. newWorkStealingPool：返回的是ForkJoinPool。根据所需的并发数量来动态创建和关闭线程。能够合理使用CPU对任务进行并发操作，所以适合使用在很耗时的任务。

   **ForkJoinPool**是个什么东西？

   使用一个无界队列来保存需要执行的任务，可以传入线程的数量；不传入，则默认使用当前计算机中可用CPU数量。使用分治来解决问题，fork() 和 join() 来进行调用。

6. 

##### submit和execute的区别

1. execute：使用 execute 提交任务，线程池内抛出异常会导致线程退出，线程池只能重新创建一个线程。如果每个异步任务都以异常结束，那么线程池可能完全起不到线程重用的作用。而且主线程无法捕获（catch）到线程池内抛出的异常。
2. submit：线程不会退出，但是异常不会记录，会被生吞掉。查看 FutureTask 源码可以发现，在执行任务出现异常之后，异常存到了一个 outcome 字段中，只有在调用 get 方法获取 FutureTask 结果的时候，才会以 ExecutionException 的形式重新抛出异常。所以我们可以通过捕获 get 方法抛出的异常来判断线程的任务是否抛出了异常。

|                                  | execute                  | submit                           |
| -------------------------------- | ------------------------ | -------------------------------- |
| 提交                             | runnable                 | callable/runnable                |
| 返回值                           | 无                       | future                           |
| 异常                             | 直接抛出                 | 异常暂存，调用future.get()才抛出 |
| 老线程                           | 老线程退出，立即新建线程 | 老线程不会退出                   |
| 线程池的UncaughtExceptionHandler | 生效                     | 不生效                           |

submit方法的流程：

1. 通过Runnable对象构建FutureTask对象；
2. execute(futureTask)；进入执行方法，如果小于核心线程数，addWorker(command, **true**)；否则workQueue.offer(command)；再否则addWorker(command, **false**)；再否则reject(command);
3. FutureTask中的run方法，setException设置异常，从而通过Futrue.get()可以捕捉到异常；

![img](https://user-gold-cdn.xitu.io/2019/7/14/16bec33ca5559c93?imageslim)



##### 线程数设置？

1. CPU密集：线程的数量一般会设置为“CPU 核数 +1”，因为偶尔的内存页失效或其他原因导致阻塞时，这个额外的线程可以顶上，从而保证 CPU 的利用率。
2. IO密集：最佳线程数 =CPU 核数 * [ 1 +（I/O 耗时 / CPU 耗时）]
3. 在一些非核心业务中，我们可以将核心线程数设置小一些，最大线程数量设置为CPU核心数量，阻塞队列大小根据具体场景设置；不要过大，防止大量任务进入等待队列而超时，应尽快创建非核心线程执行任务；也不要过小，避免队列满了任务被拒绝丢弃。
4. 在一些核心业务中，核心线程数设置为CPU核心数，最大线程数可根据公式 最佳线程数 =CPU 核数 * [ 1 +（I/O 耗时 / CPU 耗时）] 来计算。阻塞队列可以根据具体业务场景设置，如果线程处理业务非常迅速，我们可以考虑将阻塞队列设置大一些，处理的请求吞吐量会大些；如果线程处理业务非常耗时，阻塞队列设置小些，防止请求在阻塞队列中等待过长时间而导致请求已超时。

##### Executors和ThreadPoolExecutor

Executors是线程池的静态工厂类，通过调用ThreadPoolExecutor的构造方法来实现一些类型的线程池。但是这种方法并不安全，所以推荐直接使用ThreadPoolExecutor。这样可以自行控制核心线程数，最大线程数以及工作队列的类型。

##### 终止线程和线程池

线程：

1. 避免使用Thread的stop方法，这很不安全，万一这个线程正持有锁会导致锁不能释放，如果在操作数据库可能导致数据不一致；

2. 优雅地终止线程，不是自己终止自己，而是在一个线程 T1 中，终止线程 T2；这里所谓的“优雅”，指的是给 T2 一个机会料理后事，而不是被一剑封喉。两阶段终止模式，就是将终止过程分成两个阶段，其中第一个阶段主要是线程 T1 向线程 T2发送终止指令，而第二阶段则是线程 T2响应终止指令。

   Java 线程进入终止状态的前提是线程进入 RUNNABLE 状态，而实际上线程也可能处在休眠状态，也就是说，我们要想终止一个线程，首先要把线程的状态从休眠状态转换到 RUNNABLE 状态。如何做到呢？这个要靠 Java Thread 类提供的 interrupt() 方法，它可以将休眠状态的线程转换到 RUNNABLE 状态。

   线程转换到 RUNNABLE 状态之后，我们如何再将其终止呢？RUNNABLE 状态转换到终止状态，优雅的方式是让 Java 线程自己执行完 run() 方法，所以一般我们采用的方法是设置一个标志位，然后线程会在合适的时机检查这个标志位，如果发现符合终止条件，则自动退出 run() 方法。这个过程其实就是第二阶段：响应终止指令。终止指令，其实包括两方面内容：interrupt() 方法和线程终止的标志位。

   如果我们在线程内捕获中断异常(如Thread.sleep()抛出了中断一次)之后，需通过 Thread.currentThread().interrupt() 重新设置线程的中断状态，因为 JVM 的异常处理会清除线程的中断状态。

线程池：

1. shutdown()：线程池执行shutdown之后，决绝接受新的任务，但是会等待线程池中未完成的任务执行完毕之后关闭线程池；**会处理阻塞队列中的任务；**
2. shutdownNow()：拒绝接受新的任务，同时还会中断正在执行的任务，已经进入阻塞队列的任务也被剥夺了执行的机会，但是这些被剥夺执行机会的任务会作为shutdownNow方法的返回值返回。因此提交到线程池的任务需要能够正确的处理线程中断。

##### 线程池的状态

1. Running
2. Stop
3. ShutDown
4. Tidying：所有的任务已经运行终止
5. Terminated

![img](https://user-gold-cdn.xitu.io/2019/7/15/16bf3b10e39a52d0?imageslim)

##### 源码

<img src="/Users/chengleiyi/Documents/Lernen/笔记/image-20200804212810546.png" alt="image-20200804212810546" style="zoom:50%;" />

Executor接口提供了execute方法运行新任务，ExecutorService扩展了Executor接口，添加了一些用来管理执行生命周期和任务生命周期的方法，例如submit、shutdown。AbstractExecutorService是对ExecutorService接口的抽象类实现，ThreadPoolExecutor是线程池的核心实现类。

ThreadPoolExecutor内部持有一个原子类变量ctl控制线程池的状态，高3位表示线程池的5种状态（-1～3），低29位表示活动的线程数量workcount。它还持有阻塞队列类型的变量作为工作队列。

```java
// AtomicInteger是原子类  ctlOf()返回值为RUNNING；
private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));
// 高3位表示线程状态
private static final int COUNT_BITS = Integer.SIZE - 3;
// 低29位表示workerCount容量
private static final int CAPACITY   = (1 << COUNT_BITS) - 1;
// runState is stored in the high-order bits
// 能接收任务且能处理阻塞队列中的任务
private static final int RUNNING    = -1 << COUNT_BITS;
// 不能接收新任务，但可以处理队列中的任务。
private static final int SHUTDOWN   =  0 << COUNT_BITS;
// 不接收新任务，不处理队列任务。
private static final int STOP       =  1 << COUNT_BITS;
// 所有任务都终止
private static final int TIDYING    =  2 << COUNT_BITS;
// 什么都不做
private static final int TERMINATED =  3 << COUNT_BITS;
// 存放任务的阻塞队列
private final BlockingQueue<Runnable> workQueue;
```

execute(Runnable)方法：【submit中是execute(futureTask)】通过ctl成员变量中记录的线程池状态runstate和工作线程数量wokercount来控制新的任务如何处理：如果小于核心线程数，addWorker(command, **true**)新建核心线程；否则workQueue.offer(command)加入到工作队列中；再否则addWorker(command, **false**)新建线程来处理；再否则reject(command)按照拒绝策略来处理这项任务。

```java
public void execute(Runnable command) {
    if (command == null)
        throw new NullPointerException();
    // clt记录着runState和workerCount
    int c = ctl.get();
    //workerCountOf方法取出低29位的值，表示当前活动的线程数，然后拿线程数和 核心线程数做比较
    if (workerCountOf(c) < corePoolSize) {
        // 如果活动线程数<核心线程数
        // 添加到核心线程池
        //addWorker中的第二个参数表示限制添加线程的数量是根据corePoolSize来判断还是maximumPoolSize来判断
        if (addWorker(command, true)) // 如果成功则返回
            return;
        c = ctl.get();  // 如果失败则重新获取 runState和 workerCount
    }
    // 如果当前线程池是运行状态并且任务添加到队列成功
    if (isRunning(c) && workQueue.offer(command)) {
        int recheck = ctl.get();
      // 入队成功之后还需要做两件事情：第一件事是检查线程池的状态，
        // 如果不是运行状态，尝试remove这个任务，remove方法内部调用了tryTerminate尝试终止线程池
        if (! isRunning(recheck) && remove(command))
            reject(command);
        else if (workerCountOf(recheck) == 0) // 第二件事情就是检查工作线程的数量，如果工作线程都死光光了，就addWoker（null，false）来新建worker线程清理队列中剩余的任务
            addWorker(null, false);
    }
    else if (!addWorker(command, false))
        reject(command);
}
```

addWorker方法：

1. retry代码：首先检查线程池的状态，如果当前的runstate大于shutdown，即属于stop、tidying、terminated中的一种，就不再接受任务；另外如果runstate处于shutdown状态，一般情况下不接受新的任务，除非addworker传入的是null，这是为了启动额外的线程处理队列中剩余的任务，这时候需要额外检查workqueue，如果队列已经空了，则这个firsttask是null的额外线程也没必要启动。以上情况直接返回false。

   之后，通过一个CAS循环不断尝试将workercount+1，其中需要检查工作线程的数量，如果大于核心线程数或者最大线程数就不再添加线程，返回false；另外线程池的状态也可能会发生变化，此时需要重新跑retry代码块，以重新检查线程池的状态。

2. retry中的准备工作完成之后，剩下的工作就是新建新的Worker并执行。Worker是一个内部类，代表一个工作线程，实现了Runnable接口，继承AQS。创建这个worker之后，如果线程池处于运行状态，或者是shutdown状态且传入的task是null，把woker添加到workers这个hashset类型变量中，这个过程会有一个mainlock的reentrantlock锁住，【因为hashset不是线程安全的】，最后如果worker创建加入成功，就启动这个线程。t.start()方法调用了Worker类中的run()方法。

```java
// 主要工作是在线程池中创建一个新的线程并执行
// 如果为true则将corePoolSize 作为线程池线程数量的边界，否则将maximumPoolSize作为线
    private boolean addWorker(Runnable firstTask, boolean core) {
        //每当线程池的状态发生变化都会执行retry代码块
        retry:
        for (;;) {
            int c = ctl.get();
            int rs = runStateOf(c);
            //一：检测线程池状态
            if (rs >= SHUTDOWN &&
                ! (rs == SHUTDOWN &&
                   firstTask == null &&
                   ! workQueue.isEmpty()))
                return false;
            //二：CAS递增workerCount
            for (;;) {
                int wc = workerCountOf(c);
                if (wc >= CAPACITY ||
                    wc >= (core ? corePoolSize : maximumPoolSize))
                    return false;
                if (compareAndIncrementWorkerCount(c))
                    break retry;
                c = ctl.get();  // Re-read ctl
                //如果发现线程池状态发生变化则要重跑整个retry代码块
                if (runStateOf(c) != rs)
                    continue retry;
            }
        }

       //三：新建新的Worker并执行
        boolean workerStarted = false;
        boolean workerAdded = false;
        Worker w = null;
        try {
            w = new Worker(firstTask);
            final Thread t = w.thread;
            if (t != null) {
                final ReentrantLock mainLock = this.mainLock;
                mainLock.lock();
                try {
                    // 如果线程池处于运行状态，或者是shutdown状态且传入的task是null
                    int rs = runStateOf(ctl.get());
                    if (rs < SHUTDOWN || (rs == SHUTDOWN && firstTask == null)) {
                        if (t.isAlive()) // precheck that t is startable
                            throw new IllegalThreadStateException();
                        workers.add(w);
                        int s = workers.size();
                        if (s > largestPoolSize)
                            largestPoolSize = s;
                        workerAdded = true;
                    }
                } finally {
                    mainLock.unlock();
                }
                if (workerAdded) {
                    t.start();
                    workerStarted = true;
                }
            }
        } finally {
            if (! workerStarted)
                addWorkerFailed(w);
        }
        return workerStarted;
    }
```

Worker内部类：

1. 继承了AQS，是一个不可重入锁，重写了tryAcquire和tryRelease方法，利用CAS改变state状态加锁。同时它也实现了Runnable接口，持有一个final Thread 类型的成员变量，这就是用来执行任务的线程。
2. 

```java
// 构造方法
//发现它先将同步状态（state）设置为-1（现在看来有点奇怪，待会再解释原因），然后将任务设为自己的firstTask字段，这是这个工作线程即将要执行的第一个任务，最后调用之前构造函数传入的ThreadFactory新建线程
Worker(Runnable firstTask) {
    setState(-1); // inhibit interrupts until runWorker
    this.firstTask = firstTask;
    this.thread = getThreadFactory().newThread(this);
}
```

3. run方法：直接调用了ThreadPoolExecutor中的runWorker方法，做的事情其实就是先调用getTask方法将任务从队列中取出来，然后执行，循环往复。如果getTask返回null，while循环就结束。执行任务前检查一下线程池的状态。执行任务的顺序是`beforeExcute`->`task.run`->`afterExecute`。等循环结束，worker线程退出直线，调用processWorkerExit做一些退出前准备工作。
4. getTask方法：先检查线程池的状态，如果线程池是stop或者是shutdown且队列为空的情况，直接返回null，结束线程。如果线程池处于running状态，或者shutdown状态但是队列中还有任务要执行，就要继续检查工作线程数有没有超，或者是否超时。两轮检查都通过就去队列中取任务。

![image-20200805102331254](/Users/chengleiyi/Documents/Lernen/笔记/image-20200805102331254.png)



> **Worker为什么不使用ReentrantLock来实现呢？**

tryAcquire方法它是不允许重入的，而ReentrantLock是允许重入的。对于线程来说，如果线程正在执行是不允许其它锁重入进来的。线程只需要两个状态，一个是独占锁，表明正在执行任务；一个是不加锁，表明是空闲状态。

> **在runWorker方法中，为什么要在执行任务的时候对每个工作线程都加锁呢？**

shutdown方法与getTask方法存在竞态条件。shutdown方法会中断空闲的线程，interruptIdleWorker方法中使用条件**if** (!t.isInterrupted() && w.tryLock()) 来判断，线程有没有被中断或者是不是空闲的。这里的w.tryLock就会和runWorker中加锁产生冲突，如果trylock失败就表示线程正在执行任务，不能中断。

#### FutureTask

```java
 * NEW -> COMPLETING -> NORMAL
 * NEW -> COMPLETING -> EXCEPTIONAL
 * NEW ->    ==>     -> CANCELLED
 * NEW -> INTERRUPTING -> INTERRUPTED
// 1个初始态:NEW
// 2个中间态:COMPLETING,INTERRUPTING
// 4个终止态:NORMAL,EXCEPTIONAL,INTERRUPTED,CANCELLED
private volatile int state;
private static final int NEW          = 0;//任务的初始状态
private static final int COMPLETING   = 1;//正在设置运行的结果
private static final int NORMAL       = 2;//任务正常执行完成
private static final int EXCEPTIONAL  = 3;//任务执行过程中发生异常
private static final int CANCELLED    = 4;//任务被取消
private static final int INTERRUPTING = 5;//正在中断运行中的任务
private static final int INTERRUPTED  = 6;//任务被中断

//要执行的任务
private Callable<V> callable;
//返回的结果或者异常
private Object outcome; 
//当前执行任务的线程
private volatile Thread runner;
//Treiber栈,先进后出,线程安全的栈,存储等待任务执行完毕的线程信息 
private volatile WaitNode waiters;
```
##### 整体执行流程

1. 将当前线程设置为runner
2. 判断任务的状态是NEW则执行任务
3. 任务执行成功,outcome赋返回值,任务执行失败,outcome赋异常,给outcome赋值之前会先将状态设置为COMPLETING.
4. outcome赋值完成后,设置最终状态NORMAL 或者 EXCEPTIONAL
5. 唤醒waiters中所有的线程,并做资源的释放
6. 检查是否有中断被遗漏, 如果有, 等待中断状态完成

#### ForkJoinPool



#### ThreadLocal 

ThreadLocal对象可以提供线程局部变量，每个线程Thread拥有一份自己的副本变量，多个线程互不干扰。

```java
// 使用demo
ThreadLocal<String> mLocal = new ThreadLocal<>();
mLocal.set("hello");
mLocal.get();

// 调用threadlocal的set方法，其实是先获取当前线程，调用getMap方法获取线程的ThreadLocalMap
public void set(T value){
  Thread t = Thread.currentThread();
  ThreadLocalMap map = getMap(t);
  if(map != null) map.set(this, value); // this是调用set方法的我定义的这个mLocal
  else createMap(t, value); // 但是ThreadLocalMap threadlocals 这个map是属于当前线程Thread的
}
// 我new出来的这个threadLocal对象mLocal只不过是个工具人，其实并不保存变量
void createMap(Thread t, T firstValue){
  t.threadLocals = new ThreadLocalMap(this, firstValue);
}

// get方法
public T get(){
    Thread t = Thread.currentThread();
    //获取当前线程内部的ThreadLocalMap对象
    ThreadLocalMap map = getMap(t);
    if (map != null) {
        ThreadLocalMap.Entry e = map.getEntry(this); // ThreadLocalMap的getEntry
        if (e != null) {
            @SuppressWarnings("unchecked")
            T result = (T)e.value;
            return result;
        }
    }
    //执行到这里的两种情况：1）map没初始化；2）map.getEntry返回null
    return setInitialValue(); 
    // 这个方法中使用initValue来获得默认初始值，如果map已经初始化，但是对应的Entry为空的情，此时将键值对存入底层数组；如果map没有初始化，使用createMap方法初始化
  // 在对threshold初始化的时候使用了2/3作为系数
}
```

![image-20200722211214569](/Users/chengleiyi/Library/Application Support/typora-user-images/image-20200722211214569.png)

##### 使用场景

1. 线程处理复杂业务，可以用threadLocal来避免一些参数的显示传递；
2. 用来存储用户的session；
3. 多线程的情况下，如果用线程同步的方式，当并发度比较高的时候性能不佳，可以改用threadlocal的方式。
4. 线程内的上下文管理器，数据库连接可以用到；
5. 【AOP 动态 切换数据源】

##### 内存泄漏问题

由于ThreadLocalMap的Entry中的key是对ThreadLocal对象的弱引用，在下一次垃圾回收的时候就会被清除。所以当我定义的mLocal对象没有被强引用了，这个对象只剩下弱引用，会被gc，key变成null，但是value还在。

在调用set、get、remove方法的时候，会清理部分key为null 的记录。如果出现了null的情况，而没有手动调用remove，那么就会出现内存泄漏。

##### 线程还活着的时候，弱引用会GC吗

##### ThreadLocalMap

Thread类有一个类型为ThreadLocal.ThreadLocalMap【ThreadLocal的一个静态内部类】的实例变量threadLocals，也就是说每个线程有一个自己的ThreadLocalMap。

ThreadLocalMap本质上是一个Map，能够保存多个`ThreadLocal:value` 的键值对，并且不同的ThreadLocal对象可能会产生冲突。当遇到hash冲突的时候，采用线性探测法来解决冲突，底层使用数组结构存储。

```java
initial_capacity; // 初始容量默认16
Entry[] table; // 存储键值对的数组
size; //数组内存储的元素个数
threshold;// 扩容阈值

// ThreadLocalMap是Entry类型的键值对
// （实际上key并不是ThreadLocal本身，而是它的一个弱引用）
static class Entry extends WeakReference<ThreadLocal<?>>{
  Object value;
  Entry(ThreadLocal<?> k, Object v){
    super(k);
    value = v;
  }
}
// Entry 是 ThreadLocalMap的内部类，继承了weakReference意味着Entry的key引用的对象，在没有其他强引用的情况下，在下一次GC的时候就会被回收。

```



**ThreadLocalMap的Hash算法**

```java
int i = key.threadLocalHashCode & (len-1); // 本质上就是对entry数组的长度取余

public class ThreadLocal<T>{
  private final int threadLocalHashCode = nextHashCode();
  private static AtomicInteger nextHashCode = new AtomicInteger();
  private static final int HASH_INCREMENT = 0x61c88647;
  private static int nextHashCode(){
    return nextHashCode.getAndAdd(HASH_INCREMENT);
  }
}
// threadLocal的hashcode是通过nextHashcode方法来得到的，每次创建一个ThreadLocal对象，这个ThreadLocal.nextHashCode 这个原子类型的值就会以一种安全的方式增长 HASH_INCREMENT 大小
// HASH_INCREMENT这个值的大小也很特殊，是斐波那契数，也就是黄金分割数。选用这个数字，可以使hash分布更加均匀

```

解决hash冲突的办法，**线性探测法**。在`set`过程中，如果遇到了`key`过期的`Entry`数据，实际上是会进行一轮**探测式清理**操作。



**ThreadLocalMap的set方法**

1. 如果当前key有对应的entry，更新他的value；
2. 如果entry的key为null，表明这是一个失效的entry，执行replaveStaleEntry方法；
3. 找到一个空位置，把新添加的entry放进去；
4. 调用cleanSomeSlots，如果这个方法没有清理任何桶，并且达到了扩容阈值，就执行rehash进行扩容；



**ThreadLocalMap的getEntry方法**

先通过hashcode来定位key，获取目标桶的Entry，判断是否为空或者不等于目标key。

如果不满足要求，说明产生了hash冲突，导致目标key被放到其他位置；那就调用getEntryAfterMiss (key, i, e)

```java
//ThreadLocalMap的getEntry方法
private Entry getEntry(ThreadLocal<?> key) {
    //计算key位于哪个桶
    int i = key.threadLocalHashCode & (table.length - 1);
    Entry e = table[i];
    if (e != null && e.get() == key)
        return e;
    //执行到这里的两种情况：1）e=null，即桶内没有存数据；2）桶内有数据，
    //但不是当前这个ThreadLocal对象的，说明产生了hash冲突，导致键值对被放到了其他位置
    else
        return getEntryAfterMiss(key, i, e);
}
// 由于hash冲突，没有得到正确的key，调用getEntryAfterMiss方法继续找，向后遍历
// 如果碰到等于目标key的k，就返回当前entry
// 如果碰到key==null，就调用expungeStaleEntry删除已经作废的entry【expungeStaleEntry清空的是部分失效entry，碰到第一个部位null的桶就会停止】，并且把因为hash冲突放在后面的元素尽可能挪回靠近理想位置的地方。

```



#### Synchronized

**了解偏向锁、轻量级锁、重量级锁的概念以及升级机制、以及和ReentrantLock的区别**

Java中提供了两种实现同步的基础语义：`synchronized`方法和`synchronized`块，**javap -v 查看字节码**

+ 同步块：javac在编译的时候会生成对应的monitorenter和monitorexit指令，分别对应同步块的进入和退出，会有两个monitorexit是为了保证抛异常的情况下也能释放锁，所以javac对同步代码块添加了一个隐式try- finally，在finally中会调用monitorexit释放锁。
+ 方法：javac为其生在flags中成了一个ACC_SYNCHRONIZED标记，在JVM进行方法调用的时候，发现被这个关键字，会先尝试获取锁。
+ JVM中对这两种synchronized语义的实现大致相同

**对象头**

对象头中存放了mark word和类元数据，锁信息也是存在于对象的`mark word`中的。

+ 无锁状态：01，偏向锁bit0

+ 当对象状态为偏向锁（biasable）时，`mark word`存储的是偏向的线程ID；锁标志位：01，偏向锁bit1
+ 当状态为轻量级锁（lightweight locked）时，`mark word`存储的是指向线程栈中`Lock Record`的指针；锁标志位：00
+ 当状态为重量级锁（inflated）时，为指向堆中的monitor对象的指针。锁标志位10

**锁优化升级**

传统的锁，依赖于**操作系统的同步机制**，在linux上是有mutex互斥锁，最底层实现依赖于futex。这些同步函数都涉及到系统调用，需要用户态和内核台之间切换，成本较高。

jdk1.6之前synchronized只有传统的锁，因此性能不好。

jdk1.6之后引入偏向锁和轻量级锁，他们可以解决在没有多线程竞争【偏向】或者基本没有竞争【轻量级】的场景下加锁的开销问题。锁的升级是单向的，不会出现降级。

轻量级锁失败之后会尝试自旋，因为大多数线程持有锁的时间不会太久，所以经过若干次循环后很有可能获得锁。实在不行就挂起。

锁消除：编译的时候取出不可能存在竞争的锁，通过这种方式消除没有必要的锁。StringBuffer的append是同步方法，但是如果在某些使用情况下，StringBuffer属于局部变量，并不会被其他线程使用，所以JVM会自动将锁消除。

**synchronized可重入！！一个线程调用synchronized方法的同时在其方法内部调用该对象的另一个synchronized方法是允许的。monitor中的计数器会加1**

**中断**

+ 当线程处于阻塞状态或者试图执行一个阻塞操作时，我们可以使用实例方法interrupt()进行线程中断，执行中断操作后将会抛出interruptException异常(该异常必须捕捉无法向外抛出)并将中断状态复位
+ 当线程处于运行状态时，我们也可调用实例方法interrupt()进行线程中断，但同时必须在run方法中编写判断是否中断的代码，手动判断中断状态。
  

##### 重量级锁



重量级锁状态下，**对象的markword指向堆中一个monitor对象的指针**。当一个 monitor 被某个线程持有后，它便处于锁定状态。在Java虚拟机(HotSpot)中，monitor是由**ObjectMonitor**实现的，其主要数据结构如下

ObjectMonitor中有两个队列，WaitSet 和 _EntryList，用来保存ObjectWaiter对象列表( 每个等待锁的线程都会被封装成ObjectWaiter对象)，owner指向持有ObjectMonitor对象的线程。

+  当多个线程同时访问一段同步代码时，首先会进入 _EntryList 集合
+ 当线程获取到对象的monitor 后进入 _Owner 区域并把monitor中的owner变量设置为当前线程同时monitor中的计数器count加1
+ 若线程调用 wait() 方法，将释放当前持有的monitor，owner变量恢复为null，count自减1，同时该线程进入 WaitSet集合中等待被唤醒。
+ 若当前线程执行完毕也将释放monitor(锁)并复位变量的值，以便其他线程进入获取monitor(锁)。
  

![image-20200721203722113](/Users/chengleiyi/Library/Application Support/typora-user-images/image-20200721203722113.png)

##### *轻量级锁

1.6之后增加的轻量级锁和偏向锁

线程在执行同步块之前，JVM会先在当前的线程的栈帧中创建一个lock Record，其中包括一个用于**存储对象头中的mark word** 以及**一个指向对象的指针**。

<img src="/Users/chengleiyi/Library/Application Support/typora-user-images/image-20200721205552845.png" alt="image-20200721205552845" style="zoom:67%;" />

**加锁过程**

1. 在线程栈中创建一个Lock Record，将其obj字段指向锁对象；
2. 直接通过CAS指令将Lock Record的地址存储在对象头的mark word中，如果对象处于无锁状态则修改成功，代表该线程获得了轻量级锁。
3. 失败，可能是当前线程已经持有该锁了，代表这是一次锁重入。设置Lock Record第一部分为null，祈祷一个重入计数器的作用。
4. 如果都不是以上情况，表示发生了竞争，需要膨胀为重量级锁。

**解锁过程**

1. 遍历线程栈，找到所有obj字段指向当前锁对象的Lock Record；
2. 如果lock record的displaced mark word为null，代表这是一次重入，将obj设置为null之后continue；
3. 如果lock record的displaced mark word不是null，则利用CAS指令将对象头的mark word恢复称displaced    mark word。如果成功continue；否则，膨胀成重量级锁。















##### 偏向锁

**提高一个对象在一段很长的时间内都只被一个线程用做锁对象场景下的性能**，引入了偏向锁，在第一次获得锁时，会有一个CAS操作，之后该线程再获取锁，只会执行几个简单的命令，而不是开销相对较大的CAS命令。

**对象创建**

+ 1.6以上的版本，JVM的默认开启偏向锁模式。一个新创建的对象，他的mark word是可偏向状态，此时mark word里的thread id为0，表示没有偏向任何线程，也叫做匿名偏向。

**加锁过程**

+ 当一个对象第一次被线程获得锁的时候，thread id 为 0 ，则会用CAS指令，将mark word 中的thread id改为当前线程的id。如果成功，表示获得了偏向锁，继续执行同步代码。否则，撤销偏向锁，升级为轻量级锁。
+ 当被偏向的线程再次进入同步块的时候，发现锁偏向的就是自己，他会在当前线程的栈中添加一条displaced mark word为空的Lock record，然后继续执行同步代码。【此时开销很小】
+ 当其他线程进入同步块时，发现已经有偏向的线程了，则会进入到**撤销偏向锁**的逻辑里。一般情况下，会在安全点去查看偏向锁的线程是否还存活，如果还**活着且还在同步中**，那么原偏向锁的线程继续持有锁，其他线程进入锁升级的逻辑。如果偏向的线程已经**不存活，或者 不在同步块中**，则将对象头的mark word 改为无锁状态，之后再升级为轻量级锁。

**解锁过程**

+ 当有其他线程尝试获得锁时，是根据遍历偏向线程的`lock record`来确定该线程是否还在执行同步块中的代码。因此偏向锁的解锁很简单，仅仅将栈中的最近一条`lock record`的`obj`字段设置为null。需要注意的是，偏向锁的解锁步骤中**并不会修改对象头中的thread id。**

  

**批量重偏向/撤销**

偏向锁的撤销是有一定成本的，如果说运行时的场景本身存在多线程竞争的，那偏向锁的存在不仅不能提高性能，而且会导致性能下降。因此，JVM中增加了一种批量重偏向/撤销的机制。

1. 一个线程创建了大量对象并执行了初始的同步操作，之后在另一个线程中将这些对象作为锁进行之后的操作。这种case下，会导致大量的偏向锁撤销操作。

2. 存在明显多线程竞争的场景下使用偏向锁是不合适的，例如生产者/消费者队列。

批量重偏向（bulk rebias）机制是为了解决第一种场景。批量撤销（bulk revoke）则是为了解决第二种场景。

其做法是：以class为单位，为每个class维护一个偏向锁撤销计数器，每一次该class的对象发生偏向撤销操作时，该计数器+1，当这个值达到重偏向阈值（默认20）时，JVM就认为该class的偏向锁有问题，因此会进行批量重偏向。每个class对象会有一个对应的`epoch`字段，每个处于偏向锁状态对象的`mark word中`也有该字段，其初始值为创建该对象时，class中的`epoch`的值。每次发生批量重偏向时，就将该值+1，同时遍历JVM中所有线程的栈，找到该class所有正处于加锁状态的偏向锁，将其`epoch`字段改为新值。下次获得锁时，发现当前对象的`epoch`值和class的`epoch`不相等，那就算当前已经偏向了其他线程，也不会执行撤销操作，而是直接通过CAS操作将其`mark word`的Thread Id 改成当前线程Id。

当达到重偏向阈值后，假设该class计数器继续增长，当其达到批量撤销的阈值后（默认40），JVM就认为该class的使用场景存在多线程竞争，会标记该class为不可偏向，之后，对于该class的锁，直接走轻量级锁的逻辑。





##### 自旋锁







**多线程调用加锁方法，核心是判断这些调用需要获取的锁是不是同一个。**

当一个线程正在访问**一个对象的 synchronized 实例方法**，那么其他线程不能访问该对象的其他 synchronized 方法，毕竟一个对象只有一把锁，当一个线程获取了该对象的锁之后，其他线程无法获取该对象的锁，所以无法访问该对象的其他synchronized实例方法

但是其他线程还是可以访问该实例对象的其他非synchronized方法，当然如果是一个线程 A 需要访问实例对象 obj1 的 synchronized 方法 f1(当前对象锁是obj1)，另一个线程 B 需要访问实例对象 obj2 的 synchronized 方法 f2(当前对象锁是obj2)，这样是允许的，因为两个实例对象锁并不同相同。

如果一个线程A调用一个实例对象的非static synchronized方法，而线程B需要调用这个实例对象所属类的静态 synchronized方法，是允许的，



#### volatile

有volatile修饰的变量，赋值之后多执行一个lock开头的操作，相当于生成了一个**内存屏障**，通过这个空操作使本cpu的cache写入内存，同时引起别的cpu或者别的内核缓存无效。保证了可见性。

内存屏障能阻止屏障两侧的指令重排序【第一个操作volatile读、第二是volatile写、第一写第二读不能重排序】，强制缓冲区的数据写回到主内存。

JMM 采取保守策略。下面是基于保守策略的 JMM 内存屏障插入策略：

- 在每个 volatile 写操作的前面插入一个 StoreStore 屏障。
- 在每个 volatile 写操作的后面插入一个 StoreLoad 屏障。
- 在每个 volatile 读操作的后面插入一个 LoadLoad 屏障。
- 在每个 volatile 读操作的后面插入一个 LoadStore 屏障。



**缓存一致性协议**

MESI协议保证每个缓存中使用的共享变量的副本是一致的。

##### 原子操作指令

+ lock 锁住主内存
+ unlock 释放主内存变量
+ read 读主内存
+ load 放入工作内存
+ use 使用工作内存
+ assign 赋值工作内存
+ store 存储工作内存，传给主内存
+ write 写入主内存





#### 常见问题

- 同步阻塞、同步非阻塞、异步的区别？
- select、poll、eopll的区别？
- java NIO与BIO的区别？
- reactor线程模型是什么?

- synchronized与ReentrantLock的区别？
- 乐观锁和悲观锁的区别？
- 如何实现一个乐观锁？
- AQS是如何唤醒下一个线程的？
- ReentrantLock如何实现公平和非公平锁是如何实现？
- CountDownLatch和CyclicBarrier的区别？各自适用于什么场景？
- 适用ThreadLocal时要注意什么？比如说内存泄漏?
- 说一说往线程池里提交一个任务会发生什么？
- 线程池的几个参数如何设置？
- 线程池的非核心线程什么时候会被释放？
- 如何排查死锁？

**1.现在有T1、T2、T3三个线程，你怎样保证T2在T1执行完后执行，T3在T2执行完后执行？**

这个线程问题通常会在第一轮或电话面试阶段被问到，目的是检测你对”join”方法是否熟悉。这个多线程问题比较简单，可以用join方法实现。

**2. 在Java中Lock接口比synchronized块的优势是什么？你需要实现一个高效的缓存，它允许多个用户读，但只允许一个用户写，以此来保持它的完整性，你会怎样去实现它？**

lock接口在多线程和并发编程中最大的优势是它们为读和写分别提供了锁，它能满足你写像ConcurrentHashMap这样的高性能数据结构和有条件的阻塞。Java线程面试的问题越来越会根据面试者的回答来提问。我强烈建议在你去参加多线程的面试之前认真读一下Locks，因为当前其大量用于构建电子交易终统的客户端缓存和交易连接空间。

**3.在java中wait和sleep方法的不同？**

通常会在电话面试中经常被问到的Java线程面试问题。最大的不同是在等待时wait会释放锁，而sleep一直持有锁。Wait通常被用于线程间交互，sleep通常被用于暂停执行。

**4.用Java实现阻塞队列。**

这是一个相对艰难的多线程面试问题，它能达到很多的目的。第一，它可以检测侯选者是否能实际的用Java线程写程序；第二，可以检测侯选者对并发场景的理解，并且你可以根据这个问很多问题。如果他用wait()和notify()方法来实现阻塞队列，你可以要求他用最新的Java 5中的并发类来再写一次。

**5.用Java写代码来解决生产者——消费者问题。**

与上面的问题很类似，但这个问题更经典，有些时候面试都会问下面的问题。在Java中怎么解决生产者——消费者问题，当然有很多解决方法，我已经分享了一种用阻塞队列实现的方法。有些时候他们甚至会问怎么实现哲学家进餐问题。

**6.用Java编程一个会导致死锁的程序，你将怎么解决？**

这是我最喜欢的Java线程面试问题，因为即使死锁问题在写多线程并发程序时非常普遍，但是很多侯选者并不能写deadlock free code（无死锁代码？），他们很挣扎。只要告诉他们，你有N个资源和N个线程，并且你需要所有的资源来完成一个操作。为了简单这里的n可以替换为2，越大的数据会使问题看起来更复杂。通过避免Java中的死锁来得到关于死锁的更多信息。

**7.什么是原子操作，Java中的原子操作是什么？**

非常简单的java线程面试问题，接下来的问题是你需要同步一个原子操作。

**8.Java中的volatile关键是什么作用？怎样使用它？在Java中它跟synchronized方法有什么不同？**

自从Java 5和Java内存模型改变以后，基于volatile关键字的线程问题越来越流行。应该准备好回答关于volatile变量怎样在并发环境中确保可见性、顺序性和一致性。





**9.什么是竞争条件？你怎样发现和解决竞争？**

这是一道出现在多线程面试的高级阶段的问题。大多数的面试官会问最近你遇到的竞争条件，以及你是怎么解决的。有些时间他们会写简单的代码，然后让你检测出代码的竞争条件。可以参考我之前发布的关于Java竞争条件的文章。在我看来这是最好的java线程面试问题之一，它可以确切的检测候选者解决竞争条件的经验，or writing code which is free of data race or any other race condition。关于这方面最好的书是《Concurrency practices in Java》。

**10.你将如何使用thread dump？你将如何分析Thread dump？**

在UNIX中你可以使用kill -3，然后thread dump将会打印日志，在windows中你可以使用”CTRL+Break”。非常简单和专业的线程面试问题，但是如果他问你怎样分析它，就会很棘手。

**11.为什么我们调用start()方法时会执行run()方法，为什么我们不能直接调用run()方法？**

这是另一个非常经典的java多线程面试问题。这也是我刚开始写线程程序时候的困惑。现在这个问题通常在电话面试或者是在初中级Java面试的第一轮被问到。这个问题的回答应该是这样的，当你调用start()方法时你将创建新的线程，并且执行在run()方法里的代码。但是如果你直接调用run()方法，它不会创建新的线程也不会执行调用线程的代码。

**12.Java中你怎样唤醒一个阻塞的线程？**

这是个关于线程和阻塞的棘手的问题，它有很多解决方法。如果线程遇到了IO阻塞，我并且不认为有一种方法可以中止线程。如果线程因为调用wait()、sleep()、或者join()方法而导致的阻塞，你可以中断线程，并且通过抛出InterruptedException来唤醒它。我之前写的《How to deal with blocking methods in java》有很多关于处理线程阻塞的信息。

**13.在Java中CycliBarriar和CountdownLatch有什么区别？**

这个线程问题主要用来检测你是否熟悉JDK5中的并发包。这两个的区别是CyclicBarrier可以重复使用已经通过的障碍，而CountdownLatch不能重复使用。

**14.什么是不可变对象，它对写并发应用有什么帮助？**

另一个多线程经典面试问题，并不直接跟线程有关，但间接帮助很多。这个java面试问题可以变的非常棘手，如果他要求你写一个不可变对象，或者问你为什么String是不可变的。

**15.你在多线程环境中遇到的共同的问题是什么？你是怎么解决它的？**

多线程和并发程序中常遇到的有Memory-interface、竞争条件、死锁、活锁和饥饿。问题是没有止境的，如果你弄错了，将很难发现和调试。这是大多数基于面试的，而不是基于实际应用的Java线程问题。





## Mysql

事务隔离级别、锁、索引的数据结构、聚簇索引和非聚簇索引、最左匹配原则、查询优化（explain等命令）

推荐文章：http://hedengcheng.com/?p=771

https://tech.meituan.com/2014/06/30/mysql-index.html

http://hbasefly.com/2017/08/19/mysql-transaction/

- Mysql(innondb 下同) 有哪几种事务隔离级别？
- 不同事务隔离级别分别会加哪些锁？
- mysql的行锁、表锁、间隙锁、意向锁分别是做什么的？
- 说说什么是最左匹配？
- 如何优化慢查询？
- mysql索引为什么用的是b+ tree而不是b tree、红黑树
- 分库分表如何选择分表键
- 分库分表的情况下，查询时一般是如何做排序的？



#### 三大范式

1. 每一列不可分割；即属性具有原子性
2. 每一行都要有唯一标识，这个唯一属性列被称为主关键字，实体的属性完全依赖于主键；不存在部分依赖
3. 属性不依赖于其他非主属性，不存在传递依赖（冗余）：学号--学生--学院--学院电话



#### 隔离级别

MySQL默认的隔离级别：可重复读。看不到晚于本事务开始的更新。

**如何实现？**

+ MVCC多版本并发控制。
+ MVCC只在**可重复读**和**读已提交**两个隔离级别下工作。
+ MVCC解决了一部分幻读的情况：插入数据的情况。即A事务进行时，B事务插入数据，A事务不会读出B新插入的数据。
+ 但是没有解决另一种幻读：A事务进行时，B事务插入数据，A事务update了这条新数据【update属于当前读】，导致版本号变小了，A就能读出这条新数据。
+ 幻读还是需要MVCC+锁来解决。

**快照读&当前读**

+ 快照读：读取的是快照版本，即事务开启的时候的版本；只有select是快照读。
+ 当前读：读的是最新的版本。

**锁定读&一致性非锁定读**

+ 一致性非锁定读：MVCC实现的就是一致性非锁定读，这是RR和RC隔离级别下的默认select模式。
+ 锁定读：select ... lock in share mode 或者 select ... for update 就会给记录或者索引加锁。

**悲观锁&乐观锁**

+ 悲观锁：总是需要加锁，实现依靠数据库底层；
+ 乐观锁：只有提交更新的时候才检查数据的状态，通常给数据增加字段来标识版本。

**3种锁**

1. Record Locks：记录锁，在索引记录上加锁；
2. Gap Locks：间隙锁，在索引记录之间加锁；
3. Next-Key Locks：在索引记录上加锁，并且在索引记录之前的间隙加锁；

**自增锁**

如果表中存在自增字段，mysql就会自动维护一个自增锁。**主键自增情况下写数据是属于哪一隔离级别？读未提交。**因为如果事务A（id 4）添加的数据还没提交，事务B添加的数据（id 5）提交了，中间的4是空出来的。



#### MVCC

InnoDB存储引擎的特性有，默认隔离级别`REPEATABLE READ`，行级锁，实现了MVCC，Consistent nonlocking read(默认读不加锁，一致性非锁定读)，Insert Buffer，Adaptive Hash Index，DoubleWrite，**Cluster Index。**
**InnoDB中通过UndoLog实现了数据的多版本，而并发控制通过锁来实现。**

##### Redo log、bin log、undo log

+ binlog，是mysql服务层产生的日志，常用来进行数据恢复、数据库复制，常见的mysql主从架构，就是采用slave同步master的binlog实现的，另外通过解析binlog能够实现mysql到其他数据源（如ElasticSearch)的数据复制。

  有3种日志格式：

  1. 基于sql语句的复制：每一条会修改数据的sql；
  2. 基于数据行的复制：每行被修改的记录；
  3. mixed：一般情况使用sql，当sql没办法直接体现数据上的修改，比如使用了函数，就采用数据行的形式

  刷新binlog到磁盘的时机：sync_binlog来控制，0表示mysql不管，由文件系统来控制；大于0，表示每sync_binlog次事务就要刷新到磁盘1次。

+ redo log：记录了数据操作在物理层面的修改，mysql中使用了大量缓存，缓存存在于内存中，修改操作时会直接修改内存，而不是立刻修改磁盘，当内存和磁盘的数据不一致时，称内存中的数据为脏页(dirty page)。为了保证数据的安全性，事务进行中时会不断的产生redo log，在事务提交时进行一次flush操作，保存到磁盘中，redo log是按照顺序写入的，磁盘的顺序读写的速度远大于随机读写。当数据库或主机失效重启时，会根据redo log进行数据的恢复，如果redo log中有事务提交，则进行事务提交修改数据。这样实现了事务的原子性、一致性和持久性。

  redo log 可以分为两个部分：在内存中的日志缓冲 redo log buffer，在磁盘中的日志文件 redo log file。聚簇索引、二级索引、undo log页面的修改都需要记录redo日志。

  ![image-20200730162317735](/Users/chengleiyi/Documents/Lernen/笔记/image-20200730162317735.png)

  **这种做法称为Write-ahead log 预先日志持久化**，在持久化一个数据页之前，现将内存中响应的日志页持久化。事务提交的时候必须调用一次fsync操作，保证redo buffer写入redo log file（默认情况）。

  **redo log 在innobd中的实现：**

  通过mini-transaction来保证并发事务操作下以及数据库异常时数据页中数据的一致性，但是不属于事务。

  主要遵循以下3种协议：

  1. the fix rules：修改数据页时需要获取排他锁x-latch，获取一个数据页的时候需要获取共享锁s-latch或者排他锁x-latch，持有锁知道修改、访问完成；
  2. Write-ahead log ；预先日志持久化，保证单个数据页的一致性，但是无法保证事务的持久性；
  3. Force-log-at-commit：保证一个事务提交的时候，所有日志都刷新到磁盘中。

+ Undo Log: 除了记录redo log外，当进行数据修改时还会记录undo log，undo log用于数据的撤回操作，它记录了修改的反向操作，比如，插入对应删除，修改对应修改为原来的数据，通过undo log可以实现事务回滚，并且可以根据undo log回溯到某个特定的版本的数据，实现MVCC。

  undo log是一种逻辑日志，用于事务的回滚和MVCC，从而保障了原子性。DML修改聚簇索引前，记录undo log；二级索引记录的修改，不记录undo log。undo log 存储在回滚段 rollback segment中，每个回滚段有1024个undo log segment，在每个undo log segment中进行undo 页的申请。

  **两种类型：**

  1. insert undo log：插入操作的undo log，由于插入操作的记录，只对事务本身可见，对其他事务不可见，所以undo log 在事务提交之后直接删除，不用进行purge操作；
  2. update undo log：对delete和update操作产生的undo log，事务提交的时候放入undo log链表，等待purge线程进行最后的删除。

  purge线程：清理undo页、清理数据页内带有delete_bit的标志的数据行。真正的删除是purge线程完成的。

  

+ redo log 和 bin log的一致性，为了防止写完bin log 但是 redo log的事务还没提交导致的不一致，innodb使用了两阶段提交。

+ ![image-20200730154451923](/Users/chengleiyi/Documents/Lernen/笔记/image-20200730154451923.png)

+ undo log不是redo log 的逆过程，undo log是逻辑日志，对事务的回滚只是从逻辑上恢复到原来的样子；而redo log是物理日志，记录的是数据页的物理变化。

##### 原理

**每一行记录都有隐藏列：trx_id、roll_ptr、row_id**

+ trx_id：最近更新这条记录的事务id；
+ Roll_ptr：指向该行回滚段rollback segment的指针，innodb通过这个指针找到之前版本的数据。该行记录上的旧版本都在undo log上通过链表的形式存在；
+ row_id：行标识，如果没有主键，innodb生成这个列作为隐藏主键。每条记录的头信息（record header）都会有一个专门的bit来表示当前记录是否被删除。

多个事务并行操作某行数据的情况下，不同事务对该行数据的update会产生多个版本，通过回滚指针组织成一条undo log链表。

**事务A对一行数据（row_id = 1）进行update操作的具体步骤：**

+ 对row_id为1的这行记录加排他锁；
+ 把该行原本的值拷贝到undo log 中，trx_id 和 roll_prt 都不变；
+ 修改该行的值，产生一个新版本，更新trx_id 为事务A的id，将roll_ptr指针指向刚刚拷贝到undo log中的旧版本记录；
+ 记录redo log， 包括undo log 中的修改。

如果是insert操作，只需要产生新的记录，trx_id为当前事务的id；

如果是delete操作，可以当作一种特殊的update，真正的删除操作执行在commit的时候，trx_id记录当前事务id。

**RR、RC下的ReadView**

在RU隔离级别下，直接读取版本的最新记录就可以；在Serializable级别，通过加互斥锁来访问数据，都不需要MVCC。只有在RR、RC两个级别，select数据的时候需要用到版本链。

ReadView是当前【Read View创建时其他未提交的活跃事务ID列表】的事务id列表，trx_ids，

up_limit_id即低水位【活跃事务列表trx_ids中最小的事务ID，如果trx_ids为空，则up_limit_id 为 low_limit_id】，

low_limit_id即高水位【目前出现过的最大的事务ID+1，即下一个将被分配的事务ID】

1. 如果当前被访问版本的trx_id < up_limit_id，说明该版本的事务在readview生成前就已经提交了，可以被当前事务访问；
2. 如果当前被访问版本的trx_id >= low_limit_id，说明该版本在当前readview之后才生成，并不能访问，需要根据undo log链表找到前一个版本，然后根据该版本的trx_id 重新判断可见性；
3. 如果当前被访问版本的trx_id 在readview列表的最大最小值之间，那就需要判断在不在列表中，如果存在列表中，表示创建readview的时候，该版本所属事务还在活跃中，还没提交，不能访问【需要在undo log找上一个版本，重新计算可见性】；如果不存在，说明这时候已经提交了，可以被访问。
4. 最后检查这条记录的delete_flag标志位，如果是true，表示已经被删除，否则说明这条记录可以返回给客户端。

innodb中的MVCC其实并不是最理想状态的多版本共存，而只是在修改数据的时候加排他锁，把历史数据串起来。理想化的MVCC纯粹通过乐观锁来代理两段式提交。

**RR隔离级别**

每个事务执行第一个select语句的时候，会将当前系统的所有活跃事务拷贝到一个列表，生成readView。之后的select都是复用这个readView，而其他操作，update、delete、insert和一致性非锁定读snapshot没有关系。

**RC隔离级别**

每个 `SELECT` 语句开始时，都会重新将当前系统中的所有的活跃事务拷贝到一个列表生成 `ReadView`。二者的区别就在于生成 `ReadView` 的时间点不同，一个是事务之后第一个 `SELECT` 语句开始、一个是事务中每条 `SELECT` 语句开始。

#### 存储引擎

##### MYISAM&InnoDB

|              | MyISAM                                  | InnoDB                               |
| ------------ | --------------------------------------- | ------------------------------------ |
| **存储结构** | 3个文件：存储表定义、数据文件、索引文件 | 几乎所有的表都保存在同一个数据文件中 |
| 存储空间     | 可压缩                                  | 建立缓冲池用于高速缓冲数据和索引     |
| 锁           | 表级锁                                  | 行级锁                               |
| 事务         | 不支持                                  | 支持                                 |
| 索引         | 非聚簇索引                              | 聚簇索引                             |
| 全文索引     | 支持Fulltext                            | 5.6.4以后版本开始支持                |
| 外键         | 不支持                                  | 支持                                 |
| 哈希索引     | 不支持                                  | 支持                                 |
| 保存行数     | 保存                                    | 不保存，要扫描全表来计算             |



##### MyISAM读的快？

1. InnoDB 是根据主键构建的 B+tree 的聚簇索引，要维护操作的东西比非聚簇索引要多。
2. MyISAM引擎在SELECT COUNT(*) FROM TABLE时只需要直接读取已经保存好的值而不需要进行全表扫描



#### 索引

##### 数据结构

###### 为什么用B+树，不用红黑树，B树，二叉搜索树？

1. 首先，二叉搜索树虽然平均查找的平均复杂度是logn，但是不能保证平衡，最糟糕的情况可能会退化为链表，时间复杂度n。所以它首先排除。
2. 相比之下，红黑树/B+树一定程度上查找时间复杂度更稳定了，但是由于红黑树的每个节点只有一个数据，会导致树的高度比B/B+树要高。而数据库的查找涉及到磁盘IO操作，为了尽可能减少IO次数，B树比红黑树更合适。
3. 最后B树和B+树相比，B树每个节点都有data域，而B+树只有叶子结点存在date域，意味着同样的大小，使用B+树结构一次能够读取的节点数量更多，IO次数更少一些。另外，B+树的叶子结点有链表连接，更方便范围查询，B+树的查询效率也更稳定一些。

###### B+树和哈希索引的区别

+ hash索引的检索效率很高，不需要像b树一样从根节点到叶子结点；但是hash索引**只能用于等值过滤**，而不能查找范围，因为hash算法处理后的大小关系并不对应原值的大小关系。同样的原因，哈希索引**不能排序**。
+ 由于hash值的计算，联合索引只能组合使用，并不能使用部分索引键来检索。
+ hash索引在遇到大量hash碰撞的时候，定位记录就很麻烦了，效率并不比b树要高。



##### 索引分类

**主键索引 PRIMARY KEY**

它是一种特殊的唯一索引，不允许有空值。**一般是在建表的时候同时创建主键索引**。注意：一个表只能有一个主键。

**唯一索引 UNIQUE**

唯一索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。

可以通过ALTER TABLE table_name ADD UNIQUE (column);创建唯一索引：

可以通过ALTER TABLE table_name ADD UNIQUE (column1,column2);创建唯一组合索引：

**普通索引 INDEX**

这是最基本的索引，它没有任何限制。

可以通过ALTER TABLE table_name ADD INDEX index_name (column);创建普通索引：

**组合索引 INDEX**

即一个索引包含多个列，多用于避免回表查询。

可以通过ALTER TABLE table_name ADD INDEX index_name(column1,column2, column3);创建组合索引：

**全文索引 FULLTEXT**

也称全文检索，是目前搜索引擎使用的一种关键技术。

可以通过ALTER TABLE table_name ADD FULLTEXT (column);创建全文索引：

索引一经创建不能修改，如果要修改索引，只能删除重建。可以使用DROP INDEX index_name ON table_name;删除索引。





##### 索引优化

+ 重复少优先，选择性好的优先；
+ 数据量少优先；
+ 经常用的优先；
+ =在前，范围在后；

1. 选择唯一性索引。唯一性索引能快速从索引中定位到值，过多相同的值会降低查询效率。
2. 为经常排序，分组，联合查询（外键）的字段建立索引。排序操作会浪费很多时间，建立索引可以有效避免排序操作。
3. 为经常查询的字段建立索引。经常查询的字段会影响整个表的查询速度，为该字段建立索引能提高整表的查询速度。
4. 尽量使用数据量少的索引字段。char(100)字段进行全文检索明显比char(10)索引花费时间要多。
5. 限制索引的数量，索引并不是越多越好，索引需要存储，消耗磁盘空间，同时对于修改更新数据需要重建索引带来额外的消耗与麻烦。
6. 尽量使用前缀来索引。如果索引的字段很长，只能放text或blog,进行全文检索耗费时间会很长。如果只是检索字段前面的值会提高检索速度。
7. 删除不再使用或很少使用的索引。索引会占用磁盘空间与影响数据的更新，删除这些索引能减少对数据更新的影响。
8. 最左前缀匹配原则，很重要的原则。mysql会一直向右匹配制导遇到范围查询（> < between like）就停止匹配。例如查询a=1,b=5,c<8,d=9，同时建立了（a,b,c,d）复合索引，d是用不到索引的。如果建立（a,b,d,c）顺序索引a,b,d顺序可以任意调整。
9. =和in可以乱序。例如查询a=1,b=5,c=9.建立（a,b,c）索引可以任意顺序。sql查询优化器会将a,b,c顺序调整成可以识别的顺序。
10. 尽量选择区分度高的列作为索引。选择重复值少的列作为索引查询速度会更快。
11. 索引列不能参与运算，要保持干净。参与计算的列不能运用索引，原因是如果使用索引b+树每一个值都要应用函数才能比较，显然成本会很高。
12. 尽量尽量扩展索引，不要新建索引。比如表中已经有了a索引，现在要加(a,b)索引，那么只要修改原索引即可。
  

##### 索引失效的情况

1. **隐式类型转换**， WHERE 条件中字符串没有加引号，可能会自动转换为int型，使索引无效，产生全表扫描。

2. or语句前后没有同时使用索引。当or左右查询字段只有一个是索引，该索引失效，只有当or左右查询字段均为索引时，才会生效

3. like 以%开头，索引无效；当like前缀没有%，后缀有%时，索引有效。但是如果想让以‘%’开头仍然使用索引，则需要使用覆盖索引，即只查询带索引字段的列。

4. 对索引字段进行计算操作、字段上使用函数。

5. 对于多个范围条件查询，MySQL 无法使用第一个范围列后面的其他索引列，对于多个等值查询则没有这种限制。

6. 如果 MySQL 判断全表扫描比使用索引查询更快，则不会使用索引。回表次数太多会严重影响SQL性能，如果回表次数太多，就不应该走索引扫描，应该直接走全表扫描。

7. 最左前缀匹配特性，组合索引，不是使用第一列索引，索引失效。**注意，最左原则并不是说是查询条件的顺序**

8. 在索引列上使用 IS NULL 或 IS NOT NULL操作。索引是不索引空值的，所以这样的操作不能使用索引，可以用其他的办法处理，例如：数字类型，判断大于0，字符串类型设置一个默认值，判断是否等于默认值即可。

9. 在索引字段上使用not，<>，!=。**不等于操作符是永远不会用到索引的**，因此对它的处理只会产生全表扫描。 优化方法： key<>0 改为 key>0 or key<0。

10. 左连接查询或者右连接查询查询关联的字段编码格式不一样

11. **mysql5.7中，只有小于等于和小于才会触发索引。**

12. 第二个字段使用范围查找，导致第二个索引用途不是检索而是排序，这样导致第三个索引失效。**范围列可以用到索引（联合索引必须是最左前缀），但是范围列后面的列无法用到索引，索引最多用于一个范围列，如果查询条件中有两个范围列则无法全用到索引**

13. 索引下推：mysql5.6版本之前没有加入index condition pushdown，所以索引逻辑还是这样的：

    即便对于复合索引，从第一列开始先确定第一列索引范围，如果范围带=号，则对于=号情况，确定第二列索引范围加入索引结果集里，每列的处理方式都是一样的。

    确定完索引范围后，则回表查询数据，再用剩下的where条件进行过滤判断。

    mysql5.6后加入了ICP，对于确定完了索引范围后，会用剩下的where条件对索引范围再进行一次过滤，然后再回表，再用剩下的where条件进行过滤判断。（减少回表记录数量）。

    ```mysql
    索引命中规则详解：
     
    t这张表 a,b,c 三个字段组成组合索引 
    select * from t where a=? and b=? and c=?  全命中
    select * from t where c=? and b=? and a=?  全命中 解析MySQL的查询优化器会自动调整where子句的条件顺序以使用适合的索引
    select * from t where a=?  命中a  解析:最左前缀匹配
    select * from t where a=? and b=?  命中a和b  解析:最左前缀匹配
    select * from t where a=? or b=?  一个没命中 解析or无法命中
    select * from t where a=? and c=?  命中a 解析:最左前缀匹配，中间没有则无法使用索引
    select * from t where a=? and b in ( x, y, z) and c=?  全部命中 in精确匹配可以使用索引
    select * from t where b=?  一个没命中  解析:最左前缀匹配原则
    select * from t where b=? and c=?  一个没命中  解析:最左前缀匹配原则
    select * from t where a=? and b like 'xxx%'   命中a和b
    select * from t where a=? and b like '%xxx'  命中a
    select * from t where a<? and b=?   命中a 解析这个是范围查找
    select * from t where a between ? and ?  and b=?  命中a和b 解析BETWEEN相当于in操作是精确匹配
    select * from t where a between ? and ?  and b=? and c  and between ? and ?    全部命解析中同上
    select * from where a-1=?   函数和表达式无法命中索引
    ```

    

#### 语法

##### truncate、delete、drop

+ 都是用来删除的；
+ truncate：只删数据，不删表结构，数据库定义语言，立即生效不能回滚
+ delete：只删数据，不删表结构，数据库操作语言，**事务提交生效**，可以回滚
+ drop：删除表结构约束、索引，数据库定义语言，直接生效不能回滚

##### join、left join、right join

![img](https://coolshell.cn/wp-content/uploads/2011/01/SQL-Join.jpg)



##### Union

SELECT vend_id,prod_id,prod_price FROM products WHERE prod_price>5 

UINON 

SELECT vend_id,prod_id,prod_price FROM products WHERE vend_id IN (1001,1002) 

ORDER BY vend_id

两个子查询的结果必须包含相同的列、表达式或者聚合函数，union之后是去重的结果，不去重用union all。

组合查询只有一个orderby，是对最后汇总的结果进行排序

##### Where、having

WHERE 是行级过滤，子句在`GROUP BY`分组和聚合函数**之前**对数据行进行过滤；被 WHERE 过滤掉的数据不会出现在分组中。

而 HAVING 是组级过滤，`HAVING`子句对`GROUP BY`分组和聚合函数**之后**的数据行进行过滤。另一方面，`HAVING`子句中不能使用除了分组字段和聚合函数之外的其他字段

SELECT region, SUM(population), SUM(area)
FROM bbc
GROUP BY region
HAVING SUM(population)>1000000

此时就不能用where，因为表中根本不存在sum(population) 这样的记录

##### Limit

 `LIMIT M, N` ：M是输出记录的初始位置，N是输出的个数；

`LIMIT 10 OFFSET 2`：初始位置偏移2，输出10个。

##### 函数

```mysql
concat(name1,'(',name2,')')
upper()
ltrim()
rtrim()
date()
abs()
cos()
avg()
count()
min()
max()
sum()
```

##### 插入、更新、删除

```mysql
insert into customers(cust_name,cust_email) values ('name1','email1'),('','');
update customers set cust_name='namenew', cust_email='emailnew' where cust_id=001
delete from coustomers where cust_id=001
```

##### Create table xxx

```mysql
create table customers(
	cust_id int not null auto_increment,
	cust_name CHAR(50) not null,
	cust_age int null default 18,
	primary key(cust_id)
)engine=INNODB

create table orders(
	ord_id int not null,
  cust_id int,
  ord_num int not null,
  primary key(ord_id),
  foreign key(cust_id) references customers(cust_id)
)
alter table customers
add constraint fk_costomer_orders
foreign key(cust_id) references orders(order_cust)
```

```mysql
# one to many
create table classes(
	id int primary key auto_increment,
  classname char(20)
);
create table student(
	id int primary key auto_increment,
  name varchar(20),
  sex enum('male','female') default 'male',
  class_id int,
  foreign key(class_id) references classes(id)
);
#many to many
create table course(
	id int primary key auto_increment,
  name varchar(20)
);
create table student2course(
	id int primary key auto_increment,
  student_id int,
  course_id int,
  foreign key(student_id) references student(id) on delete cascade on update cascade,
  foreign key(course_id) references course(id) on delete cascade on update cascade,
  score int
);
```

##### 执行顺序

```mysql
from 
on 
join
where
group by
having
select
distinct
order by
limit
```

##### 索引

![image-20200727112557143](/Users/chengleiyi/Documents/Lernen/笔记/image-20200727112557143.png)

unique唯一索引，fulltext全文索引，spatial空间索引，默认普通索引

![image-20200727112841594](/Users/chengleiyi/Documents/Lernen/笔记/image-20200727112841594.png)

![image-20200727113026014](/Users/chengleiyi/Documents/Lernen/笔记/image-20200727113026014.png)



##### 语法练习

1. 查出所有成绩都大于80分的名单：

   Selece distinct name from student where name not in ( select name from student where score < 80 );

   先找出成绩有小于80的人名（不论是一门科目还是多门），只要不在这些人名中，这个人就是都高于80

2. 查询出所有并列排名第二的学生姓名：

   Select name from student where score = (select distinct score from student order by score desc limit 1,1)

   先找出第二名的成绩，然后找出成绩等于这个值的人。因为可能有多个人并列，所以不能直接对人limit

3. 查询出人数在60以上的各个班级的平均年龄：

   select class, avg(age) from student group by class having count(*) > 60

4. 查询1班以外比一班年龄最小的学生还要小的学生:

   Select * from student where class <>1 and age < ALL(select age from student where class = 1);

   **ALL ANY**

5. 查询选修了全部课程的学生姓名:

   我选了所有的课，也就是说我没选的课list为空，所以去查询我没选的课有哪些

   ```mysql
   select name from student where not exists(
     # 我没选的课的列表，这个列表为空，表示我选了所有的课
   	select * from course where not exitst(
       # 如果sc表中有这样的数据，表示我选了这个课，查询结果不为空，返回false
   		select * from sc where sc.s_id = student.id and sc.c_id=course.id
   	)
   )
   ```

   

6. 查询至少选修了id为1的学生选修的全部课程的学生id

   也就是说，id 为1 的学生选的课，我都选了，所以不存在id为1 学生选的课我没选

   ```mysql
   select s_id from sc scx where not exists(
   	# 我没选的课列表，列表为空，表示我选了所有id为1 学生选的课程
     select * from sc scy where scy.s_id = 1 and not exists(
       # 我（scx的id）选了 学生1选的这门课（scy.c_id），返回false
     	select * from sc scz where scx.s_id = scz.s_id and scy.c_id = scz.c_id
     )
   )
   ```

   

#### *分页查询问题



#### 分库分表问题

分表键相关（不会。。） 分表有什么需要考虑，怎么去分表，横向分表与纵向分表需要考虑的地方。淘宝有1000T的数据，单机存不下，怎么办？有没有办法只访问一次服务器？一致性Hash懂多少。 MySQL的高可用了解多少，高性能了解多少 。MySQL数据备份怎么做 

英文+数字的自增主键在分布式下如何解决（类似于MVCC机制，使用标识符和预知量判断）

如果使用redis怎么解决自增主键在分布式下高并发的问题（在redis中建立一个键值对，不管插入何值都存到redis的value中。等到合适的时间再存入数据库，因为主键唯一所以不符合自增要求或者重复的会被直接pass掉）

**读写分离：**不同的数据库，同步相同的数据，但是只负责数据的读或者写功能；

**分区：**指定分区列表达式，把记录拆分到不同的区域（但还是同意服务器，不同硬盘），应用看起来还是一张表；

**分库：**一个系统的多张数据表，存储到不同的数据库实例中；

**分表：**对于一张多行多列的表，可以分为垂直分表/水平分表：

+ 垂直分表：不同表存储不同的字段，可以把不常用或者大容量、或者不同业务的字段拆分出去；
+ 水平分表：按照特定的分片算法，不同分表存储不同的记录。

##### 全局主键

1. 设置主键自增的偏移量和步长：假设有10个分表，起始值分别1-10，指定步长10。这样10个表的主键都不会重复了。旧数据需要分散进入不同的所属分表。

2. 全局ID映射表：在全局redis中为每张表创建一个id的键，记录该表的当前最大id。redis的原子操作INCR来实现。

3. uuid：无序，性能欠佳，最常见的是微软的GUID。

4. 组合 GUID(10字节) 和时间(6字节)，达到有序的效果，提高索引性能。

5. Snowflake(雪花) 算法：分布式id生成算法，结果是long的数值，按时间大致有序，整个集群各结点不重复。时间序列+机器表示+计数顺序号

   

##### 分片策略

也可以引入一个新的节点，用来存储id和库的mapping关系，从而进行分片。

1. 连续分片range：根据指定字段的范围划分。集群如果扩容，指定新的范围在新的节点就可以，不需要进行数据迁移。如果按时间划分，数据热点分布不均匀，导致节点负荷不均匀。

2. 取模分片：扩容之后需要进行数据迁移；每个数据都要重新取模来确定所属节点。

3. 一致性hash：将整个hash空间组织成一个虚拟的环，数据key使用指定hash函数计算出来的hash值定位到环上，从这个位置顺时针行走，遇到的第一台存储节点，就是目标节点。这个方法扩容后，数据迁移量较小。

4. snowflake算法分片：？

   

##### 新问题

1. 分布式事务：事务补偿机制：对于那些性能要求很高，但对一致性要求不高的系统，往往不苛求系统的实时一致性，只要在允许的时间段内达到最终一致性即可，可采用事务补偿的方式。与事务在执行中发生错误后立即回滚的方式不同，**事务补偿是一种事后检查补救的措施，一些常见的实现方法有：对数据进行对账检查，基于日志进行对比，定期同标准数据来源进行同步等等**。事务补偿还要结合业务系统来考虑。

   一般可使用"XA协议"和"两阶段提交"处理

2. 跨节点join：尽量避免，可以用全局表、字段冗余、数据组装来空间换时间，避免分布式查询；

3. 节点扩容：可能需迁移数据。

##### 分布式事务

**CAP理论**：一个分布式系统不可能同时满足一致性、可用性、以及分区容错性。一致性是指在分布式系统中数据存在多个副本，这些副本内容都一致。可用性描述系统对用户的服务能力，响应时间够短。分区容错性的意思是指，由于网络等不可靠因素，系统仍然能够对外提供一致性的可用服务。

###### **两阶段提交协议2PC**

为分布式系统保证数据的强一致性，两阶段提交协议指定了一个节点为协调者，其他节点是参与者。

1. 协调者--》参与者 发送事务执行请求，并等待参与者反馈事务执行结果；
2. 参与者接到请求，执行事务，记录日志，但不提交，返回执行结果给协调者并阻塞等待下一步指令；
3. 协调者根据收到的反馈情况：
   + 所有参与者都正常执行，协调者commit通知，请求提交事务；参与者commit并释放资源返回结果。
   + 一个或多个参与者执行失败，或者协调者等待超时，协调者向参与者发送回滚通知，参与者rollback并且释放资源返回结果。
4. **仍然存在的问题：1. 单点问题：一旦协调者宕机，整个数据库集群都不行了；2. 同步阻塞，效率低下；3. 数据不一致：网络问题导致仅部分参与者收到通知。**
5. 所以才引入超时机制，互询机制：
   + 参与者A处于ready，超时没有收到第二阶段指令，询问身边的参与者B；
   + A参考B执行。如果其他参与者都处于ready，陷入长时间等待。

###### 三阶段提交协议3PC

三阶段提交协议通过引入一个 预询盘（can-commit） 阶段，以及超时策略来减少整个集群的阻塞时间。一共3个阶段：预询盘（can_commit）、预提交（pre_commit），以及事务提交（do_commit）。

1. 如果询问阶段就失败了，协调者直接abort通知，取消参与者的预备状态。
2. 其他类似两阶段提交协议。



##### 主从复制

读写分离就会用到主从复制，主要逻辑：

+ 在主库上，按照事务提交的顺序，把数据更改记录到二进制日志中；
+ 从库和主库建立一个客户端连接，主库的二进制日志复制到从库的本地中继日志中；
+ 从库从relay log 中继日志中读取事件并在从库中执行，从而实现数据更新。



#### 备份和恢复

##### 1. 确定备份恢复策略

+ 需要备份的表，存储引擎是事务型还是非事务型，两种不同的存储引擎备份方式在处理数据一致性有区别；
+ 全备份还是增量备份，全备份的优点是备份保持最新版本，恢复的时间短；增量备份则只需要备份每天的增量日志，备份时间短。
+ 备份的周期要考虑系统可承受的恢复时间。
+ 确保打开了log-bin选项，有了binlog才能在必要的时候做完整恢复。
+ 经常做备份恢复测试，确保备份有效。

##### 2. 逻辑备份

mysql的逻辑备份就是将数据库中的数据备份为一个文本文件，备份的文件可以被查看和编辑。

```mysql
mysqldump -uroot -p --all-database > all.sql # 备份所有数据库
mysqldump -uroot -p test > test.sql # 备份test数据库
mysqldump -uroot -p test emp dept > emp.sql # 备份数据库test下的表emp和表dept
#注意：
# 为了保证数据备份的一致性，myisam 存储引擎在备份时需要加上 -l 参数,表示将所有表加上读锁，在备份期间，所有表将只能读而不能进行数据更新。
# 但是对于事务存储引擎来说，可以采用更好的选项 --single-transaction，此选项使得 innodb 存储引擎得到一个快照(snapshot)，使得备份的数据能够保证一致性。

# 恢复
mysql -uroot -p db_name < backfile
# 备份恢复后数据并不完整，还需要将备份后执行的日志进行重做
mysqlbinlog binlog-file | mysql -uroot -p
./mysqlbinlog --no-defaults /data/mysql/mysql-bin.000002 | ./mysql t2
# 如果是上午 10 点发生了误操作，可以用以下语句用备份和 binlog 将数据恢复到故障前
mysqlbinlog --stop-date="2017-09-30 9:59:59" /data/mysql/mysql-bin.123456 | mysql -uroot -ppassword
```

##### 3. 物理备份

物理备份又分为冷备份和热备份两种，和逻辑备份相比，它的最大优点是备份和恢复的速度更快，因为物理备份的原理都是基于文件的 cp。

冷备份就是把数据库停掉，然后cp数据文件；

热备份对于不同的存储引擎方法不同：

+ myisam：使用 mysqlhotcopy db_name [/path/to/new_directory]
+ innodb：使用第三方工具 ibbackup、xtrabackup、innobacupex



#### mysql中的锁

##### 表级锁&行级锁

+ 表级锁：开销小，加锁快，粒度大，并发度低；适合并发低，查询为主的应用；
+ 行级锁：相反；
+ innoDB默认情况下采用行级锁；

##### 共享锁&排他锁

+ 共享锁 s-latch：A获取该行的共享锁，B也可以获取该行的共享锁；但是B不能获取该行的排他锁，必须等待A释放共享锁；
+ 排他锁 x-latch：A获取该行的排他锁，B就不能再获取该行的任何锁；
+ 默认情况下，mysql的行锁自动获取，也可以手动加锁。select lock in share mode (select for share) 共享锁； select for update 排他锁

##### 意向锁

+ 意向共享锁（IS）：如果想获取行级共享锁，就要先获取该表的意向共享锁；
+ 意向排他锁（IX）：如果想获取行级排他锁，就要先获取该表的意向排他锁；
+ 意向锁的引入是为了表锁和行锁能够共存，他的存在表明某个事务正在或者即将锁定表中的数据行。
+ innodb支持由mysql服务层实现的表级锁（lock tables ... write 在指定的表上加表级排他锁），如果A已经持有行级锁（读），B事务想获取表级锁（写），当B发现表已经存在IS锁，表明这个表已经存在行级S锁，B会阻塞。此时并不需要遍历每行以检查行锁，提高了效率。
+ 意向锁之间是兼容的，意向锁和表锁之间只有共享锁兼容，表级锁中只有共享锁兼容。其他情况都是冲突的。

##### 悲观锁&乐观锁

+ mysql中的表锁和行锁都是悲观锁，是数据库本身实现的；

+ mysql中的乐观锁通过提交更新时检查版本号或者时间戳是否符合，通过业务代码来实现。

##### 一致性非锁读（快照读）&当前读

+ 快照读：普通select语句；使用快照来读取
+ 当前读：select lock in share mode (select for share) / select for update / insert / update / delete 获取的都是最新数据。 

##### 行级锁的实现 

innodb通过给索引上的索引记录加锁的方式，实现行级锁。一共有三种行级锁的算法：记录锁、间隙锁、next-key lock。

使用 <u>**show engine innodb status**</u> 查看innodb监控中关于锁的事务数据。

###### RecordLock

记录锁：针对索引记录的锁定，会阻止其他事务对表中某一行的数据进行插入、更新以及删除。【记录锁永远锁定索引记录，锁定非聚簇索引会先锁定聚簇索引，如果表中没有定义索引，innodb默认为表创建一个隐藏的聚簇索引，并锁定它】

1. 通过主键操作单个值：select * from t where id = 1 for update (X锁)，表t上存在IX锁，主键索引X record lock；
2. 通过唯一索引操作单个值：select * from t where c1= 1 for update ，表t存在IX锁，索引c1上X record lock，主键索引 X record lock

###### Gap Lock

间隙锁：针对索引记录之间的间隙、第一个索引之前的间隙或者最后一个索引之后的间隙。间隙锁彼此不冲突，作用相同

1. 通过主键操作范围：select * from t where id between 1 and 10 for update; 锁定了1-+inf之间的位置，即使此时表中只有3行数据，id=4不存在，也不能再插入了。表t上存在IX锁，主键索引 id = 1 存在X记录锁，3个间隙锁(1，2 ], (2, 3 ], (3, +inf) supremum。实际上这里的间隙锁属于nexk-key。整体来说锁住了【1， +inf）范围。
2. 通过唯一索引操作范围：select * from t where c1 between 1 and 10 for update； 表t上存在IX锁，索引c1存在4个间隙锁 (-inf, 1 ], (1, 2], (2,3],(3, +inf) supremum，主键索引上存在3个记录锁，锁定了3行主键值。【此时无法插入任何值】 

###### Next-key Lock 为什么是锁之前的gap

相当于一个索引记录锁+该记录之前的一个间隙锁。RR隔离级别下，innoDB会通过next-key锁进行查找和索引扫描，用于防治幻读。因为他会锁定范围，不会导致两次查询结果的数量不同。

1. 通过普通索引操作单个值：select * from t where c2 = 1; 表t上存在IX锁，索引c2存在一个next-key lock 和 X gap lock，锁定(-inf,1] 和 (1,3)，主键索引上存在X record lock。
2. 通过普通索引操作范围：和唯一索引操作范围相同。
3. 无索引操作单个值、范围：表t上存在IX锁，主键索引4个nexk-key lock 锁住了所有的主键范围，无法插入任何数据。【这就是为什么mysql最好通过主键来操作的原因】

【普通索引  +  单条记录】

![image-20200804143944889](/Users/chengleiyi/Documents/Lernen/笔记/image-20200804143944889.png)

【唯一索引  +  单条记录】【此时降级，只有record lock，gap lock失效】

![image-20200804144037027](/Users/chengleiyi/Library/Application Support/typora-user-images/image-20200804144037027.png)

【普通索引  +  范围查询】

![image-20200804144902353](/Users/chengleiyi/Library/Application Support/typora-user-images/image-20200804144902353.png)

【唯一索引  +   范围查询】

![image-20200804145214242](/Users/chengleiyi/Library/Application Support/typora-user-images/image-20200804145214242.png)

![image-20200804145835211](/Users/chengleiyi/Library/Application Support/typora-user-images/image-20200804145835211.png)



| 行级锁 | 主键         | 唯一索引                       | 普通索引                     | 非索引     |
| ------ | ------------ | ------------------------------ | ---------------------------- | ---------- |
| 单个值 | Record lock  | 唯一record，主键record         | 普通范围左右延伸，主键record | 主键全范围 |
| 范围   | 范围左右延伸 | 唯一范围左右延伸，主键锁record | 普通范围左右延伸，主键record | 主键全范围 |

规则包括：两个“原则”、两个“优化”和一个“bug”。

1. 原则1：加锁的基本单位是 next-key lock。next-key lock 是前开后闭区间。
2. 原则2：查找过程中访问到的对象才会加锁。
3. 优化1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。
4. 优化2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。
5. 一个 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。

###### 插入意向锁

Insert intention lock，再插入数据行之前，insert操作设置的一种间隙锁。插入意向锁表示一种插入意向，如果存在索引4，7，这时候想插入5，6，两个事务分别使用插入意向锁来锁定4，7之间的间隙，此时不会互斥，因为插入的是不同的行。

插入意向锁的作用是为了提高并发插入的性能，间隙锁不允许多个事务同时插入同一个索引间隙，但是插入意向锁允许多个事务同时插入同一个索引间隙内的不同数据行。



##### 死锁

```mysql
## A
start transaction 
select * from t where i = 1 for share

## B
start transaction
delete from t where i = 1

## A
delete from t where i = 1

# B
ERROR 1213 (40001): Deadlock found when trying to get lock; try restarting transaction
```

当启用了死锁检测时（默认设置），InnoDB 自动执行事务的死锁检测，并且回滚一个或多个事务以解决死锁。InnoDB 尝试回滚**更小的事务**，事务的大小由它所插入、更新或者删除的数据行数决定。

Show variables like 'innodb_deadlock_detect';  查看innobd deadlock detect是否开启，默认on

set global innodb_deadlock_detect=off；关闭死锁检查，改用innodb_lock_wait_timeout 依靠超时参数来进行回滚。避免死锁检查导致的性能下降。

1. 查看死锁：show engine innodb status \G
2. 自动检测机制，超时自动回滚代价较小的事务（innodb_lock_wait_timeout 默认50s）
3. 人为解决，kill阻塞进程（show processlist）
4. wait for graph 等待图（主动检测）

**如何避免**

1. 加锁顺序一致，尽可能一次性锁定所需的数据行
2. 尽量基于primary（主键）或unique key更新数据
3. 单次操作数据量不宜过多，涉及表尽量少
4. 减少表上索引，减少锁定资源
5. 尽量使用较低的隔离级别
6. 尽量使用相同条件访问数据，这样可以避免间隙锁对并发的插入影响
7. 精心设计索引，尽量使用索引访问数据
8. 借助相关工具：pt-deadlock-logger



**show status like 'table%';**

1. table_locks_waited
出现表级锁定争用而发生等待的次数（不能立即获取锁的次数，每等待一次值加1），此值高说明存在着较严重的表级锁争用情况
2. table_locks_immediate
产生表级锁定次数，不是可以立即获取锁的查询次数，每立即获取锁加1

**show status like 'innodb_row_lock%';**

1. innodb_row_lock_current_waits //当前正在等待锁定的数量
2. innodb_row_lock_time //从系统启动到现在锁定总时间长度
3. innodb_row_lock_time_avg //每次等待所花平均时间
4. innodb_row_lock_time_max //从系统启动到现在等待最长的一次所花时间
5. innodb_row_lock_waits //系统启动后到现在总共等待的次数

#### mysql优化

1. 5.6之后innodb支持全文索引

2. 索引下推（ICP，index condition pushdown）：5.6之后的功能，优化数据查询。

   **适用于辅助索引（二级索引）、且查询条件都是联合索引中的列**，目的是减少全行读取的数量，从而减少IO操作。

   例如根据（name，age）联合索引进行查询**“select * from user_table where a>1 and b=2"**。

   + 首先找到满足条件a>1的索引；这一过程使用index filter

   + 之后如果没有索引下推，会先回表查出对应的全行数据，然后再筛选出满足b的数据。table filter。

     这时候不能完全利用索引，只能通过using where来进行数据过滤。**最左匹配，遇到非等值判断时匹配停止**。因为联合索引中，只能保证局部有序，其他字段通过回表之后，进行过滤，**即使联合索引树上已经包含了所有条件字段**

   + 如果使用索引下推，会先应用where的剩余条件b进一步筛选；之后再回表查询全行数据；这时候执行计划的extra字段会有using index condition说明。

   + 函数、触发器、子查询的查询条件无法使用ICP；

     

3. 覆盖索引：当sql语句的所求查询字段（select列）和查询条件字段（where子句）全都包含在一个索引中，可以直接使用索引查询而不需要回表。这就是覆盖索引，通过使用覆盖索引，可以减少搜索树的次数

4. 联合索引：

## JVM

#### 引用

##### 原理

```java
public abstract class Reference<T> {
    private T referent;  //引用的对象       
    volatile ReferenceQueue<? super T> queue; //回收队列，由使用者在Reference的构造函数中指定
  // 提供enqueue（头插）、poll、remove操作
  
 	// 当该引用被加入到queue中的时候，该字段被设置为queue中的下一个元素，以形成链表结构
  // next 仅在放到queue中才会有意义
  // 因为ReferenceQueue并不提供节点存储能力，仅仅保存head节点的引用，需要靠next来维持链表结构
    volatile Reference next;
    //在GC时，JVM底层会维护一个叫DiscoveredList的链表，存放的是Reference对象，discovered字段指向的就是链表中的下一个元素，由JVM设置
    transient private Reference<T> discovered;  
	//进行线程同步的锁对象
    static private class Lock { }
    private static Lock lock = new Lock();
	//等待加入queue的Reference对象，在GC时由JVM设置，会有一个java层的线程(ReferenceHandler)源源不断的从pending中提取元素加入到queue
    private static Reference<Object> pending = null;
}
```

<img src="/Users/chengleiyi/Library/Application Support/typora-user-images/image-20200728165908625.png" alt="image-20200728165908625" style="zoom:77%;" />

Reference对象的生命周期如图，主要分为两部分，native和java部分。

###### 四种状态

1. Active，一般来说内存一开始被分配的状态都是 Active，

2. Pending 大概是指快要被放进队列的对象，也就是马上要回收的对象，

3. Enqueued 就是对象的内存已经被回收了，我们已经把这个对象放入到一个队列中，方便以后我们查询某个对象是否被回收，

4. Inactive就是最终的状态，不能再变为其它状态。

###### ReferenceQueue

1. 创建一个引用队列 queue；
2. 创建 WeakReference 对象，并关联引用队列 queue；此时状态为Active，并且Reference.pending为空；
3. 当GC执行后，由于是弱引用，所以回收该对象，并且置于pending上，此时reference的状态为PENDING 
4. ReferenceHandler从pending中取下该元素，并且将该元素放入到queue中，此时Reference状态为ENQUEUED
5. 当从queue里面取出该元素，则变为INACTIVE。
6. leekCanary利用这种机制来检查内存泄漏。

###### referenceHandler线程

当 Reference 类被加载的时候，会执行静态代码块。在静态代码块里面，会启动 ReferenceHandler 线程。

ReferenceHandler 这个类继承了Thread线程类，run方法中死循环执行**tryHandlePending**方法。

1. 在 tryHandlePending 方法里面，检查 pending 是否为 null，如果pending不为 null，则将 pending 进行 enqueue，否则线程进入 wait 状态。
2. 而Reference对象中的discovered和pending是由垃圾回收器来控制、赋值的。ReferenceHandler线程会负责移除，并把reference入队enqueue。native部分在GC的时候发现要被回收的reference对象，然后将对象加入到discoveredList，然后将discoveredList中的元素移动到pendingList，PendingList的队首就是reference类中的pending对象。
3. java部分，referenceHandler线程将pending引用加入到referencequeue中，对于Cleaner类型（继承自虚引用）的对象会有额外的处理：在其指向的对象被回收时，会调用clean方法，该方法主要是用来做对应的资源回收，**在堆外内存DirectByteBuffer中就是用Cleaner进行堆外内存的回收，这也是虚引用在java中的典型应用。**



##### 强

Object a = new Object()，传统意义上的引用，并没有对应的Reference类。

##### 软

内存不足的时候回收，所以什么情况是内存不足？

ReferencePolicy一共有4种实现：NeverClearPolicy，AlwaysClearPolicy，LRUCurrentHeapPolicy，LRUMaxHeapPolicy。其中NeverClearPolicy永远返回false，代表永远不回收SoftReference，在JVM中该类没有被使用，AlwaysClearPolicy则永远返回true。

+ **SoftReference到底什么时候被被回收了，和使用的策略（默认应该是LRUCurrentHeapPolicy），堆可用大小，该SoftReference上一次调用get方法的时间都有关系。有个一个相关的计算公式。**

##### 弱

WeakReference在Java层只是继承了Reference，没有做任何的改动。



##### 虚

- PhantomReference的get方法无论在上面情况下都是返回null。这个在PhantomReference源码中可以看到。
- 在上面的代码中，如果obj被置为null，当GC发现虚引用，GC会将把 PhantomReference 对象pr加入到队列ReferenceQueue中，注意此时pr所指向的对象并没有被回收，在我们现实的调用了 rq.poll() 返回 Reference 对象之后当GC第二次发现虚引用，而此时 JVM 将虚引用pr插入到队列 rq 会插入失败，此时 GC 才会对虚引用对象进行回收。
- **DirectByteBuffer中是用虚引用的子类`Cleaner.java`来实现堆外内存回收的**



#### 内存区域

能说清jvm的内存划分

##### 堆外内存回收

###### 申请&释放

JDK的`ByteBuffer`类提供了一个接口`allocateDirect(int capacity)`进行堆外内存的申请，底层通过`unsafe.allocateMemory(size)`实现，在JVM层面是通过`malloc`方法申请的，但是这块内存需要进行手动释放，JVM并不会进行回收，`Unsafe`提供了另一个接口`freeMemory`可以对申请的堆外内存进行释放。

###### 回收机制

JDK中使用`DirectByteBuffer`对象来表示堆外内存，每个`DirectByteBuffer`对象在初始化时，都会创建一个对用的`Cleaner`对象，这个`Cleaner`对象会在合适的时候执行`unsafe.freeMemory(address)`，从而回收这块堆外内存。【cleaner继承了虚引用类，持有Deallocator属性thunk，有方法clean】【Deallocator是Cleaner的一个属性。Deallocator继承了Runnable接口，run方法中调用unsafe. freememory】【referenceHandler线程在处理cleaner类的时候，会调用他的clean方法，所以实际上这个释放堆外内存的操作实际上是referenceHandler执行的。】

`Cleaner`对象的`clean`方法主要有两个作用：
 1、把自身从`Clener`链表删除，从而在下次GC时能够被回收
 2、释放堆外内存

> 如果JVM一直没有执行FGC的话，无效的`Cleaner`对象就无法放入到ReferenceQueue中，从而堆外内存也一直得不到释放，内存岂不是会爆？

其实在初始化`DirectByteBuffer`对象时，如果当前堆外内存的条件很苛刻时，会主动调用`System.gc()`强制执行FGC。



**总结一下**：DirectByteBuffer对象指向堆外的内存，它保存了一块内存的基本属性和Cleaner和Deallocator对象等。当DirectByteBuffer对象被回收，Cleaner对象也就是虚引用被加入到Pending list，PendingHandlerThread线程执行Cleaner的clean方法，最终释放堆外内存。这也就解释了为什么执行gc可以回收堆外内存了。也可以手动释放，首先拿到DirectByteBuffer的Cleaner对象，执行它的clean方法。

由于cleaner是private访问权限，所以自然想到使用反射来实现。
DirectByteBuffer实现了DirectBuffer接口，这个接口有cleaner方法可以获取cleaner对象

**由于堆外内存并不直接控制于JVM，因此只能等到full GC的时候才能垃圾回收！（direct buffer归属的的JAVA对象是在堆上且能够被GC回收的，一旦它被回收，JVM将释放direct buffer的堆外空间。前提是没有关闭DisableExplicitGC）**

代码中显示调用System.gc()，只是建议JVM进行垃圾回收，但是到底会不会执行垃圾回收是不确定的，可能会进行垃圾回收，也可能不会。

#### OOM情况

```shell
-XX:+PrintCommandLineFlags -version 查看默认参数
-XX:InitialHeapSize=268435456 -XX:MaxHeapSize=4294967296 -XX:+PrintCommandLineFlags -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseParallelGC 
$ java -XX:+PrintFlagsFinal -version | grep ThreadStackSize
     intx CompilerThreadStackSize   = 0      {pd product}
     intx ThreadStackSize     			= 1024            {pd product}
     intx VMThreadStackSize    			= 1024       {pd product}

-Xss JDK5.0以后每个线程堆栈大小为1M， 每个线程的stack大小（栈）
-Xmx：JAVA HEAP的最大值、默认为物理内存的1/4 
-Xms：JAVA HEAP的初始值，server端最好Xms与Xmx一样，以避免每次垃圾回收完成后JVM重新分配内存。
-Xmn：JAVA HEAP young区的大小
-XX:MetaspaceSize=64m -XX:MaxMetaspaceSize=128m

-XX:MaxTenuringThreshold=0：设置垃圾最大年龄。
-XX:SurvivorRatio=4：设置年轻代中Eden区与Survivor区的大小比值
-XX:NewRatio=4:设置年轻代（包括Eden和两个Survivor区）与年老代的比值（除去持久代）
```

OOM发生的情况，调整



#### 垃圾回收

##### CMS

- **CMS GC回收分为哪几个阶段？分别做了什么事情？**

  CMS垃圾回收主要分为4个阶段，初始标记，暂停应用线程，标记GC Roots直接关联的对象；并发标记阶段，从GCRoots直接关联的对象开始遍历整个对象图，这时候应用线程还是在工作的；重新标记阶段，暂停应用线程，为了修正在并发标记阶段，由于应用线程继续工作导致标记变动的那一部分；最后并发清除阶段，清除之前标记为死亡的对象，由于这一阶段并不需要移动活对象，所以也是并发的。

- **CMS有哪些重要参数？**

  ###### useCMSCompactAtFullCollection

  在完成FullGC后是否要进行一次内存碎片的整理

  ###### CMSFullGCBeforeCompaction

  要做多少次FullGC进行一次碎片整理

  ###### CMSInitiatingOccupancyFraction=70

  在老年代空间被使用多少后触发，由于使用CMS会造成垃圾浮动问题，所以需要早点触发。

  ###### UseCMSInitiatingOccupancyOnly

  只有达到了阈值才会触发

  ###### CMSScavengeBeforeRemark

  在CMS GC前启动一次ygc，目的在于减少old gen对ygc gen的引用，降低remark时的开销-----一般CMS的GC耗时 80%都在remark阶段

  ###### CMSIncrementalMode

  设置为增量模式，适合单cpu的情况。

- **Concurrent Mode Failure和 promotion failed什么情况下会发生？**

  ###### promotion failed

  当minor GC的时候，suvivor 空间放不下，所以要放到老年代中，而此时老年代也放不下，就会产生promotion failed 晋升失败问题。

  ###### concurrent mode failure

  如果在CMS GC的过程中，业务向老年代放入对象（可以是promotion failed的情况下放进来的，也可能是别的原因），导致老年代空间不足，就会产生concurrent model failure问题。从而启用Serial Old收集器来重新进行老年代的垃圾收集（full GC）。

  ###### 解决方法

  经常发生promotion failed，可能是由于浮动垃圾、老年代碎片化严重，导致了提前full GC，使用serial-old导致的stop-the-world。解决这个问题的办法就是在一次或者几次CMS GC之后进行一次整理，解决碎片化问题。

  解决这个问题的通用方法是调低触发CMS GC执行的阀值，CMS GC触发主要由CMSInitiatingOccupancyFraction值决定，可考虑调小这个值，提前CMS GC的触发，以保证旧生代有足够的空间。

- **CMS的优缺点？**

  并发、停顿时间短；处理器资源敏感，cpu少的时候影响用户线程；浮动垃圾和空间碎片化。

##### G1

**Java的G1回收器详细讲一讲 。 G1什么时候会stop the world** ？

garbage-first是全功能的垃圾收集器，目标是在延迟可控的情况下获取更高的吞吐量，适合配备多核处理器大容量内存的机器。

G1的空间划分是基于Region的，每个Region大小相等，可能是eden、survivor、old，同一时刻只能属于某个代。当对象大小超过Region的一般，则认为是巨型对象，直接被分配到老年代巨型对象区，这些巨型区域是一个连续的区域，每个Region最多有一个巨型对象。

从局部来看，两个Region之间使用的是复制算法，而整体来看G1使用的是整理算法。

**使用Region的意义：**

+ 每次GC不必去处理整个堆空间，只需要处理一部分的Region；
+ 通过计算每个Region的回收价值，优先收集回收率高的Region，因此可以控制停顿时间在配置的范围内。

**G1的工作模式：**

+ Young GC：当新生代空间不足的时候，G1触发YoungGC来回收新生代空间。选定所有的新生代Region，同时计算下一次所需的eden和survivor空间，动态调整新生代所占Region个数来控制youngGC的开销。
+ Mixed GC：当老年代空间达到阈值，使用全局并发标记，统计出收集收益高的若干老年代Region，在用户指定的停顿时间内，尽可能选择收益高的老年代Region进行GC。

**工作阶段：**

1. 初始标记：暂停应用线程，并发标记从GC Roots可以直接可达的对象。当达到全局并发标记的触发条件时，并不会马上开始并发标记，而是等待下一次新生代收集，利用Minor GC的STW时间，完成初始标记【借道】
2. 根区域扫描：在初始标记结束之后，所有新复制到survivor区域的对象，需要找出哪些对象存在对老年代对象的引用，把这些对象标记成Root根。
3. 并发标记：标记Region中存活对象的信息。
4. 再次标记：标记在并发标记阶段发生变化的对象，同时完成存活数据的统计。
5. 筛选回收：负责更新Region的统计数据，对各个Region的回收价值和成本进行排序，根据用户所期望的停顿时间来制定回收计划，可以自由选择任意多个Region构成回收集，然后把**决定回收的那一部分Region的存活对象复制到空的Region中**，再清理掉整个旧Region的全部空间。

G1的记忆集remember set 是一个hashmap结构，key是指向本region的对象的引用，value是指向本Region的具体card区域，使用记忆集可以确定Region中对象存活信息，避免全堆扫描。

**调优注意点：**

1. 类似CMS，G1在垃圾回收处理不过来的时候，启动Serial-old进行fullGC，会导致长时间的停顿。可能导致这种情况的原因，可能是系统主动执行System.gc，晋升失败或者并发模式失败。
2. 减小 `InitiatingHeapOccupancyPercent` 提前启动标记周期；
3. 增大预留内存，`G1ReservePercent` 默认是10，当suvivior没有足够空间的时候会尝试使用预留内存，减少晋升失败的可能。
4. 增大线程数量， `concGCThreads` 增加并发标记的线程数量。
5. 增加Region的大小 `G1HeapRegionSize` ，减少巨型对象，从而减少空间碎片化;

**G1优化的思路：尽可能让对象在新生代中分配和回收，避免对象进入老年代，导致老年代频繁进行垃圾回收，同时给系统足够的内存减少新生代垃圾回收次数。**





##### GC调优

###### JVM调优工具

- **jps**：JVM Process Status Tool ,显示指定系统内所有的HotSpot虚拟机进程
- **jstat**: JVM Statistics Monitoring Tool ,用于收集HotSpot虚拟机各方面的运行数据。
- **jinfo**: Configuration Info forJava,显示虚拟机配置信息
- **jmap**: Memory Map for Java，生成虚拟机的内存转储快照（heapdump文件）
- **jhat**: JVM Heap Dump Browser ,用于分析heapdump文件，它会建立一个HTTP/HTML服务器，让用户可以在浏览器上查看分析结果
- **jstack**: Stack Trace forJava，显示虚拟机的线程快照

###### GC调优相关

**分析系统的运行状况**

- 系统每秒请求数、每个请求创建多少对象，占用多少内存
- Young GC触发频率、对象进入老年代的速率
- 老年代占用内存、Full GC触发频率、Full GC触发的原因、长时间Full GC的原因

**jstat命令行工具：jstat -gc pid 统计时间间隔 统计次数**

![image-20200729103428185](/Users/chengleiyi/Library/Application Support/typora-user-images/image-20200729103428185.png)

**jmap命令行工具：**

+ jmap -histo pid ：输出类 名 类数量 类占用内存大小
+ jmap -dump:live, format=b, file=dump.hprof pid 生成堆内存储快照，在当前目录下导出dump的二机制文件

**Jinfo pid：查看正在运行的java程序扩展参数，包括命令行参数**

**GC日志分析：GCviewer**



##### 垃圾回收算法

- 为什么要划分成年轻代和老年代？

  

- 年轻代为什么被划分成eden、survivor区域？

  

- 年轻代为什么采用的是复制算法？

  

- 老年代为什么采用的是标记清除、标记整理算法

  



#### 类加载

##### 类加载的时机

需要立即初始化的5种情况，**主动引用**：

1. 遇到new、getstatic、putstatic、invokestatic这4种字节码的指令；
2. 对这个类使用反射；
3. 被初始化的类，他的父类还没初始化，先初始化父类；
4. 虚拟机启动的时候，要先初始化指定的主类，即包含main方法的类；
5. 1.7使用动态语言支持的时候，解析出getstatic、putstatic、invokestatic方法句柄的时候；

**其他情况属于被动引用：**

1. 通过子类引用父类中的静态变量：Child.staticfathervalue，只会触发父类的初始化，子类没有；
2. 常量在编译阶段会存入调用它的类的常量池中，本质上没有直接引用到定义该常量的类，因此不会触发定义常量的类的初始化；
3. 通过数组定义来引用类，不会触发类的初始化：Content[] contents = new Content[6]; 但是这段代码里出发了一个对数组引用类型的初始化，它是由虚拟机自动生成的，直接继承于Object的子类，由字节码指令newarray触发。
4. 接口的初始化，不存在static语句块，且不要求其父类接口全都完成了初始化，只有在真正使用到父接口的时候，比如引用接口中定义的常量时，才会初始化该父接口。



##### 类的生命周期

加载--使用--卸载

其中加载可以分为5个步骤：加载、验证、准备、解析、初始化。

##### 类加载过程

- 加载

  加载就是把class字节码文件从各个来源通过类加载器载入内存中；通过全限定类名找到class文件，把字节流所代表的静态存储结构转换为方法区的运行时数据结构，在java堆中生成一个代表这个类的Class对象。

  + 字节码的来源：本地路径下编译生成的class文件，从jar包虫来的class文件，从远程网络来的，动态代理实时编译出来的；
  + 类加载器：启动类加载器、扩展类加载器、应用类加载器、自定义类加载器；其中为什么需要自定义类加载器？一方面是为了防止java代码被反编译，需要对自己的代码加密；另一方面也有可能从非标准的来源加载代码，需要自己实现对应的类加载器。
  +  加载的多种方式：
    + 命令行启动应用时候由JVM初始化加载 
    + 通过Class.forName()方法动态加载，*还会对类进行解释，执行类中的static块*
    + 通过ClassLoader.loadClass()方法动态加载，不会执行static的内容

- 验证

  主要是为了保证加载进来的字节流符合虚拟机规范，不会造成安全错误。

  + 文件格式：魔数？是否有不被支持的常量，文件中有没有不符合规范的附加信息；
  + 元数据：语义分析。类是否继承了final类？类中的字段和方法是否与父类冲突？是否出现了不合理的重载？
  + 字节码：数据流和控制流分析。保证程序语义的合理性，比如类型转换是否合理？
  + 符号引用：检查符号引用中通过全限定名是否能够找到对饮的类？校验符号引用中的访问性private、public是否能被当前类访问？

- 准备

  为静态变量（类变量）分配内存，并赋予不同变量类型的默认初始值。【进一步的赋值要等到初始化，putstatic，除了public static final int value = 123，这个是constantValue属性，在这里就被赋值，可以理解为static final**常量在编译期就将其结果放入了常量池中**。】

- 解析【解析阶段发生的时间不一定】

  将class常量池内的符号引用替换为直接引用的过程。

  + 符号引用：就是一个字符串，代表能够唯一识别一个方、变量、类的相关信息；
  + 直接引用：内存地址，能够指向某个内存区域。**引用的目标必定已经在内存中存在。**

- 初始化

  类变量初始化，执行静态代码块，类构造器【<clinit>() 方法】的过程。【父类先执行】

  如果多个线程去同时初始化一个类，只有一个线程会执行类的clinit方法，其他线程都需要阻塞等待。

  

##### 类加载器

- 启动类加载器（Bootstrap ClassLoader）：JAVA_HOME\lib目录下并且能够被虚拟机识别的类库到JVM内存中。
- 扩展类加载器：JAVA_HOME\lib\ext目录中，或者是系统变量指定的路径下的类库，开发者可以直接使用。
- 应用程序类加载器：类路径下指定的类库，开发者可以直接使用。
- 自定义类加载器：

**机制：**

**全盘负责**，当一个类加载器负责加载某个Class时，该Class所依赖的和引用的其他Class也将由该类加载器负责载入，除非显示使用另外一个类加载器来载入

**父类委托**，先让父类加载器试图加载该类，只有在父类加载器无法加载该类时才尝试从自己的类路径中加载该类 

**缓存机制**，缓存机制将会保证所有加载过的Class都会被缓存，当程序中需要使用某个Class时，类加载器先从缓存区寻找该Class，只有缓存区不存在，系统才会读取该类对应的二进制数据，并将其转换成Class对象，存入缓存区。这就是为什么修改了Class后，必须重启JVM，程序的修改才会生效

**子类加载器和父类加载器不是以继承（Inheritance）的关系来实现，而是通过<u>组合（Composition）</u>关系来复用父加载器的代码。**



##### 双亲委派模型

（Parents Delegation Model）

双亲委派模型的工作过程：

+ 如果一个类加载器收到了类加载的请求，它首先不会自己尝试去加载这个类，而是把这个请求委派给父类加载器去完成。因此所有的类加载请求都会传递给顶层的启动类加载器，只有当父类加载器反馈说自己无法完成加载工作，子加载器才会尝试自己去加载。

作用：

+ Java类随着它的类加载器一起具备了一种带有优先级的层次关系，比如说Object类，它一定是交给启动类加载器来处理的，因此Object类在各种类加载器环境中都是同一个类。否则，系统中会出现多个Object类，内存中出现多份同样的字节码，类间关系会变的很混乱。
+ 处于安全考虑，防止核心类库被随意篡改。
+ 比较两个类是否“相等”，`只有在这两个类是由同一个类加载器加载的前提下才有意义`，否则，即使这两个类来自同一个Class文件，被同一个虚拟机加载，只要加载他们的类加载器不同，那这个两个类就必定不相等。

**破坏双亲委派：**

+ 如果加载的基础类中需要回调用户代码，而此时顶层的类加载器无法识别这些用户代码，那么就需要破坏双亲委派模型；

+ **Spring破坏模型：**

  OverridingClassLoader 是 Spring 自定义的类加载器，默认会先自己加载(excludedPackages 或 excludedClasses 例外)，只有加载不到才会委托给双亲加载，这就破坏了 JDK 的双亲委派模式。

  > 入口：AbstractApplicationContext.prepareBeanFactory：
  >
  > ​			beanFactory.setBeanClassLoader(getClassLoader())
  >
  > DefaultResourceLoader.getClassLoader()：
  >
  > ​			如果制定了类加载器就返回指定的，否则ClassUtils.getDefaultClassLoader()获取；
  >
  > ClassUtils.getDefaultClassLoader()：
  >
  > ​			获取当前线程类加载器，不存在则获取当前类加载器【如果还是null，说明是启动类加载器来加载的】，再通过 ClassLoader.getSystemClassLoader();获取当前系统的类加载器

+ **JDBC破坏模型：**

  JDBC4.0之后，只需要把驱动jar包放在工程的类加载路径中，驱动就会被自动加载。这个自动加载采用的技术叫做SPI (Service Provider Interface)，是JDK内置的一种服务提供发现机制，动态替换发现的机制。

  JDBC的Driver接口定义在JDK，但是它的实现类是服务商来提供的。DriverManager类中要加载各个实现类Driver接口的类，然后进行管理。但是DriverManager是启动类加载器来加载的，按照类加载机制，启动类加载器需要全盘负责，也就是说启动类加载器需要去加载jar包中的Driver实现类，这超出了启动类加载器的能力范围，所以需要子类加载器来加载这些实现类，这就破坏了双亲委派模型。

  这里子类加载器是通过线程上下文加载器来进行加载的。线程上下文类加载器默认情况下是系统加载器。

  

- **Tomcat的classloader结构**

  **Tomcat为什么要破坏模型？**一个容器里有可能有多个应用，不同的应用可能会依赖同一个类库的不同版本，不能要求同一个类库在同一个容器中只有一份，因此要保证应用程序的类库是相互隔离的；另外出于安全考虑，要把容器自己的类库和程序的类库隔离开来；为了jsp文件修改后不用重启自动重新装载。

  前面3个类加载和默认的一致，CommonClassLoader、CatalinaClassLoader、SharedClassLoader和WebappClassLoader则是Tomcat自己定义的类加载器，它们分别加载`/common/*`、`/server/*`、`/shared/*`（在tomcat 6之后已经合并到根目录下的lib目录下）和`/WebApp/WEB-INF/*`中的Java类库。其中WebApp类加载器和Jsp类加载器通常会存在多个实例，每一个Web应用程序对应一个WebApp类加载器，每一个JSP文件对应一个Jsp类加载器。

  common类加载器能够加载的类可以被容器和应用共同使用，而catalina类加载器加载的是容器自己使用的，shared类加载器是应用程序公用的部分，webapp类加载器是一个应用一个，相互隔离，jsp类加载器一个jsp文件一个，当jsp文件被修改，就会替换原来的内容。

  

<img src="/Users/chengleiyi/Documents/Lernen/笔记/image-20200730085947165.png" alt="image-20200730085947165" style="zoom:67%;" />

　	1 使用bootstrap引导类加载器加载

　　2 使用system系统类加载器加载

　　3 使用应用类加载器在WEB-INF/classes中加载

　　4 使用应用类加载器在WEB-INF/lib中加载

　　5 使用common类加载器在CATALINA_HOME/lib中加载



- 如何自己实现一个classloader打破双亲委派

  继承ClassLoader类，如果要符合双亲委派，重写findClass方法就好了（用户自定义类加载逻辑），如果要破坏模型，重写loadClass方法。loadClass方法内部原本实现类双亲委派的逻辑，即现委托给父类加载器来加载，父类不存在就委托给启动类加载器，如果父类无法加载，调用findClass来尝试自身加载。

  ```java
  @Override
  protected Class<?> findClass(String name) throws ClassNotFoundException {
      //获取类的class文件字节数组
      byte[] classData = getClassData(name);
      if (classData == null){
          throw new ClassNotFoundException();
      } else {
          //直接生成class对象
          return defineClass(name,classData,0,classData.length);
      }
  }
  ```

  

- 能不能自己写个类叫java.lang.System？

  答案：通常不可以，但可以采取另类方法达到这个需求。
  解释：为了不让我们写System类，类加载采用委托机制，这样可以保证爸爸们优先，爸爸们能找到的类，儿子就没有机会加载。**而System类是Bootstrap加载器加载的，就算自己重写，也总是使用Java系统提供的System，自己写的System类根本没有机会得到加载。**

  但是，我们可以自己定义一个类加载器来达到这个目的，为了避免双亲委托机制，这个类加载器也必须是特殊的。由于系统自带的三个类加载器都加载特定目录下的类，如果我们自己的类加载器放在一个特殊的目录，那么系统的加载器就无法加载，也就是最终还是由我们自己的加载器加载

  

- 那我可以直接使用自定义加载器加载 String 类吗?

  答案是：不可以。因为JVM会检测，即使你不遵守双亲委派模型，你也无法加载核心类库的类，JVM自己会检测。



#### 常见问题

- 软引用什么时候会被释放
- 弱引用什么时候会被释放

- 什么情况下使用堆外内存？要注意些什么？
- 堆外内存如何被回收？
- jvm内存区域划分是怎样的？
- 类的加载机制ClassLoader，生命周期。双亲委派是什么，怎么打破双亲委派。class类打包编译后加载到jvm中的过程？方法区保存了类的哪一部分？.jvm双亲委派，最大的优势是什么？关于双亲委派引申的一些框架知道吗？

## JDK其他

### NIO 结合netty，reactor的优势

Java NIO

### IO 读写文件的写法

### 反射

反射提供了一种在运行时动态获取类的完整构造，包括属性、方法、接口等内部信息的机制。通过反射可以在运行时实例化对象，即使是private的方法或者属性也可以。

允许开发人员在对类未知的情况下，获取类相关的信息，调用类中方法的方式更灵活。

> 反射的源头？
>
> 通过一个Class对象来进行操作而去的该类的所有方法和属性，反射的源头在Class对象。这个对象是class文件被JVM加载进内存的，存储了类相关的信息。

##### JDK提供的API

```java
Class clz = Class.forName("com.xxx.xxx"); // 获取class对象实例，字节码文件对象
Method method = clz.getMethod("setPrice", int.class); // 获取方法的Method对象
Constructor constructor = clz.getConstructor(); // 获取constructor对象
Object object = constructor.newInstance(); // 利用constructor对象的newInstance方法获取反射类对象
method.invoke(Object, 4); // 利用invoke方法调用方法
```

```java
// 获取class对象有三种方法
Class clz = Class.forName("java.lang.String"); // 只执行静态代码块
Class clz = String.class; // 只执行静态代码块
Class clz = new String("Hello").getClass(); // 需要执行静态代码块、动态代码块、构造方法

// 通过反射创建类对象
// 通过class对象的newInstance方法，这样只能使用默认的无参构造
Apple apple = (Apple) clz.newInstance();
// 通过constructor对象的newInstance方法，这样可以调用有参数的构造方法
Constructor constructor = clz.getConstructor();
Apple apple = (Apple) constructor.newInstance();
Constructor constructor = clz.getConstructor(String.class, int.class);
Apple apple = (Apple) constructor.newInstance("xxx", 15);

// 通过反射获取属性、方法、构造器
Field[] fields = clz.getFields(); // 不能获取私有属性
for(Field field : fields){
  System.out.println(field.getName());
}
Field[] fields = clz.getDeclaredFields(); // 使用 declared 方法才能获取私有属性
// 构造方法：getConstructors(), getDeclaredConstructors()
// 方法：getMethods(), getDeclaredMethods()
// getMethods 方法不仅获取到了我们声明的公有方法setStudentAge，还获取到了很多 Object 类中的公有方法。这是因为我们前文已说到：Object 是所有 Java 类的父类。所有对象都默认实现了 Object 类的方法。 而getDeclaredMethods是无法获取到父类中的方法的。

// 获取类的父类、所有接口
Class<?> superclass = clz.getSuperclass();
Class<?>[] interfaces = clz.getInterfaces();
```

##### 反射的原理

1. Class当中经常用到的内部类ReflectionData

**Method#getMethod**

+ checkMemberAccess 检查方法的权限；checkMemberAccess 传入的是 `Member.PUBLIC`，而 getDeclaredMethod 传入的是 `Member.DECLARED` ； `Member.PUBLIC` 包括public方法和父类方法；后者则包括public、private、protected，但不包括父类；

+ getMethod0 获取方法；getMethod -> getMethod0 -> getMethodsRecursive -> privateGetDeclaredMethods； getMethodsRecursive 获取到 MethodList 对象，然后通过 `MethodList#getMostSpecific` 方法筛选出返回值类型最为具体的方法；例如child和parent则返回child；

  privateGetDeclaredMethods获取public方法，如果reflectiondata缓存命中则返回缓存，否则native方法获取

+ getReflectionFactory().copyMethod(method) 返回方法的拷贝；这里会给Method设置MethodAccessor，用于后面的invoke方法。

  

**Metho#invoke**

+ Invoke 方法的前半部分主要在做一些权限的检查工作，这里对 override 变量进行判断，如果 override == true，就跳过检查 我们通常在 Method#invoke 之前，会调用 Method#setAccessible(true)，就是设置 override 值为 true。
+ 然后获取MethodAccessor，如果ma为空就新创建一个；
+ 最后调用了MethodAccessor类的invoke方法；

**MethodAccessor**

**它是一个接口，定义了方法调用的具体操作，有3个具体的实现类：**

1. MethodAccessorImpl：java版本的ma，MethodAccessorGenerator#generate 生成动态字节码然后动态加载到 JVM 中的。**其中生成 invoke 方法字节码的是 MethodAccessorGenerator#emitInvoke。**

2. NativeMethodAccessorImpl：native版本的ma实现，又一个阈值控制表示被调用的次数（默认15），如果调用次数超过15，就会使用java版本的ma；

3. DelegatingMethodAccessorImpl：native版本ma的代理对象，真正的实现还是native版本；

4. Java 版本的 MethodAccessorImpl **调用效率**比 Native 版本要快 20 倍以上，但是 Java 版本**加载时**要比 Native 多消耗 3-4 倍资源，所以默认会调用 Native 版本，如果调用次数超过 15 次以后，就会选择运行效率更高的 Java 版本。**这是HotSpot的优化方式带来的性能特性，同时也是许多虚拟机的共同点：跨越native边界会对优化有阻碍作用，它就像个黑箱一样让虚拟机难以分析也将其内联，于是运行时间长了之后反而是托管版本的代码更快些。**

   

##### 反射效率低？

1. Method#invoke方法会对参数做封装和解封操作；
2. 需要检查方法可见性；
3. 需要校验参数；
4. 反射方法难以内联；
5. JIT无法优化；反射涉及到动态加载的类型，所以无法进行优化。



### Java注解原理

注解提供了一种标记说明的烦事，类似于打标签。能够方便理解代码，代替一些配置文件的功能，编译时进行格式检查。

所有注解类型都继承自java.lang.annotation.Annotation接口，具体实现类是java运行时生成的动态代理类，而我们通过反射获取注解的时候，getAnnotation方法去获取注解的时候，得到的是代理类。

也可以通过注解处理器来检索注解，保留政策retentionPolicy参数会影响检索。

+ 源码注解：只存在源码中，编译成class文件后不存在；RetentionPolicy.SOURCE，只能通过注解处理器读取
+ 编译时注解：在源码和class文件中存在，@Override； RetentionPolicy.CLASS，同样不能反射
+ 运行时注解：运行时阶段还能起作用，@Autowired；RetentionPolicy.RUNTIME，可以反射

```java
// override 注解的定义
@Target(ElementType.METHOD)
@Retention(RetentionPolicy.SOURCE)
public @interface Override{
  
}
@SuppressWarnings - 告诉编译器禁止特定警告
// 四个元注解
 @Documented – 注解是否将包含在JavaDoc中
 @Retention – 什么时候使用该注解
 @Target – 注解用于什么地方
 @Inherited – 是否允许子类继承该注解

  // 创建 自定义注解
public @interface SimpleAnnotation {
  String value() default "This is an element";
  int[] types();
}

```

从JVM的角度来看，虚拟机规范定义了一些和注解相关的属性表，无论是字段、方法还是类本身，如果被注解修饰了，就可以被写进字节码文件。



### jdk新版本的特性



### Try-catch-finally

1. try中没有异常，try、catch、finally中都有return语句：执行try，执行到return的时候，暂存返回值，但不直接返回，先转到finally，遇到finally中的return，执行并返回，此时返回的是finally中的return值【try被截胡了】

2. try中没有异常，try、catch中有return：执行try，执行完return之后，暂存返回值，先不返回转到finally块，执行完finally返回try，返回try当时暂存的值【finally可以继续改变值的大小，但是返回的是try当年存下来的值】

3. try中有异常，try、catch、finally中都有return语句：执行try，抛出异常之后转到catch，catch块的return执行完之后，并不返回而是转到finally，由于finally有return，所以返回值是finally执行之后的结果【catch被截胡】

4. try中有异常，try、catch中有return语句：try抛出异常之后转到catch，catch执行之后暂存return的返回值，转到finally，即使finally修改了值，但是最终返回的仍然是catch执行之后暂存的返回值；

5. try、catch中都有异常，在finally中有return：try块中出现异常，到catch块，catch块出现异常到finally，finally返回，不检查异常；

6. 只在函数最后有return，try、catch、finally中都没有：按规则执行3个块，最后返回return的执行结果；

7. ```java
   public class ZeroTest {
       public static void main(String[] args) {
        try{
          int i = 100 / 0;
          System.out.print(i);
         }catch(Exception e){
              System.out.print(1);
              throw new RuntimeException();
         }finally{
              System.out.print(2);
         }
             System.out.print(3);
        }
    }
   // 只会输出1，2
   // 因为补货异常之后抛出了，不会执行3的位置
   ```

### String的hashcode

在String类中有个私有实例字段hash表示该串的哈希值，在第一次调用hashCode方法时，字符串的哈希值被计算并且赋值给hash字段，之后再调用hashCode方法便可以直接取hash字段返回。

String类中的hashCode计算方法还是比较简单的，就是以31为权，每一位为字符的ASCII值进行运算，用自然溢出来等效取模。

哈希计算公式可以计为s[0]*31^(n-1) + s[1]*31^(n-2) + ... + s[n-1]

主要是因为31是一个奇质数，所以31*i=32*i-i=(i<<5)-i，这种位移与减法结合的计算相比一般的运算快很多。

### Stream

### lambda

## Spring

bean的生命周期、循环依赖问题、AOP的实现、spring事务传播

- java动态代理和cglib动态代理的区别（经常结合spring一起问所以就放这里了）
- 属性注入和构造器注入哪种会有循环依赖的问题？

### Spring容器的启动

**本质就是创建和初始化Bean的工厂BeanFactory，加载bean的过程**。BeanFacoty是一个接口，有很多实现类。如 AbstractBeanFactory、DefaultListableBeanFactory、XmlBeanFactory、ApplicationContext 等。

+ 定位：获取配置文件路径
+ 加载注册：把配置文件读取成BeanDefinition，并注册到beanfactory中，添加几个BeanPostProcessor
+ 实例化：根据BeanDefinition创建实例

在Web项目中，一般从web.xml文件的载入开始。web.xml文件中配置了ContextLoaderListener

**ServletContext**上下文被**ContextLoaderListener**这个对象监听，这个监听对象继承了ContextLoader，并实现了ServletContextListener接口，在web容器初始化的时候，会触发**ServletContextListener接口中的contextInitialized方法**，同理在容器关闭的时候，会触发对应的contextDestroyed方法；内容初始化方法中进一步调用contextLoader类中的**initWebApplicationContext**方法，检查是不是已经创建过WebApplicationContext，如果没有就通过createWebApplicationContext方法创建。

在一般创建ClassPathXmlApplicationContext的过程：

1. super（parent）最终交给AbstractApplicationContext 来执行，目的是获取 pathMatchingResourcePatternResolver 获取一个能加载多个路径下的source的加载器来读取bean定义资源文件；这样除了DefaultResourceLoader，一共有两个资源加载器。另外还设置父级上下文，这里是null
2. setConfigLocations：设置配置路径， 支持多个配置文件以数组方式同时传入 
3. refresh方法：
4. 其中比较重要的是**obtainFreshBeanFactory**方法通过refreshBeanFactory重置AbstractApplicationContext的beanFactory，新建一个并且通过调用**loadBeanDefinition**方法处理不同场景bean定义的加载（xml、注解、web），并返回新的beanFactory。
5.  **finishBeanFactoryInitialization**遍历已经解析出来的beanDefinitionNames，如果不是抽象、是单例并且不是懒加载，就要进行实例化和初始化；在

#### Web启动

1. 优先读取web.xml文件，找到listener和context-param节点；
2. 创建servletContext上下文，解析context-param节点，存入上下文；
3. 创建listener实例，执行listener实例中的contextInitialized方法
4. 执行filter节点信息
5. 创建serlvet

### IOC

+ **Resource** 主要负责对资源的抽象，它的每一个实现类都代表了一种资源的访问策略，如 ClasspathResource【类路径】 、 URLResource ，FileSystemResource，ServletContextResource【 Web 容器上下文资源】 等。有了资源，就需要有资源加载模块，Spring 利用 **ResourceLoader** 来进行统一资源加载。

+ 资源加载完毕之后就需要 **BeanFactory** 来进行加载解析，它是一个 bean 容器，其中  BeanDefinition 是它的基本结构，它内部维护着一 个 BeanDefinition map ，并可根据 BeanDefinition 的描述进行 bean 的创建和管理。BeanFacoty 有三个直接子类 `ListableBeanFactory`、`HierarchicalBeanFactory` 和 `AutowireCapableBeanFactory`，`DefaultListableBeanFactory` 为最终默认实现，它实现了所有接口。

  **BeanDefinition** 用来描述 Spring 中的 Bean 对象。

  **BeanDefinitionReader** 的作用是读取 Spring 配置文件中的内容，将其转换为 IoC 容器内部的数据结构：BeanDefinition。

+ **ApplicationContext** 是个 Spring 容器，也叫做应用上下文。它继承  BeanFactory，同时也是  BeanFactory 的扩展升级版。由于 ApplicationContext 的结构就决定了它与 BeanFactory 的不同，其主要区别有

  + 继承 MessageSource ，提供国际化的标准访问策略；
  + 继承 ApplicationEventPublisher，提供强大的事件机制；
  + 扩展 ResourceLoader，可以用来加载多个 Resource，可以灵活访问不同的资源；
  + 对 Web 应用的支持。

#### 资源管理

##### Resource

继承InputStreamSource接口，为所有的资源统一抽象。子类AbstactResource提供统一的默认实现。不同的资源类型有不同的实现类。

+ **AbstractResource** 为 Resource 接口的默认实现，它实现了 Resource 接口的大部分的公共实现，作为 Resource 接口中的重中之重。**如果我们想要实现自定义的 Resource**，应该继承 AbstractResource 抽象类，然后根据当前的具体资源特性覆盖相应的方法。

+ **ClassPathResource**：Bean 定义将会从 classpath 中加载然后形成一个 ClassPathResource 来使用。ClassPathResource 类是对 classpath 下资源的封装，或者是说对 `ClassLoader.getResource()`方法或 `Class.getResource()`方法的封装 ，它支持在当前 classpath 中读取资源文件。
  + `ClassLoader.getResource("")`获取的是 classpath 的根路径
  + `Class.getResource("")`获取的是相对于当前类的相对路径
  + `Class.getResource("／")`获取的是 classpath 的根路径
+ **FileSystemResource**：对java.io.file类型的资源做封装，使用NIO进行读写交互。
+ **ServletContextResource**：为访问Web容器上下文中的资源而设计的类，负责以相对于Web应用根目录的路径加载资源。
+ eg. 一个资源在Web应用的类路径下，有几种方式来访问资源？FileSystemResource以文件系统绝对路径的方式；ClassPathResource以类路径的方式进行访问；通过ServletContextResource以相对于Web应用根目录的方式进行访问。

##### ResourceLoader

为Spring资源加载的统一抽象类。具体的资源加载由相应的实现类来完成，所以可以将ResourceLoader成为统一资源定位器。

+ getResource方法：根据锁提供的路径location返回Resource实例，但是它不确保Resource一定存在。这个方法主要是DefaultResourceLoader来实现。
+ getClassLoader方法：返回ClassLoader实例。

实现类：

+ DefaultResourceLoader：构造函数的参数是一个ClassLoader类，否则使用默认的ClassLoader。【一般通过Thread.currentThread().getContextClassLoader()】

  提供了ResourceLoader的核心资源加载策略：首先通过getProtocolResolvers()获取ProtocolResolver的集合，如果存在ProtocalResolver就用它来加载资源，并返回Resource：

  1. location以“/” 开头，调用getResourceByPath方法构造ClassPathContextResource类型资源并返回；
  2. 若location以“classpath” 开头，构造classPathResource资源返回
  3. 否则以URL的方式进行资源定位，再否则判断是够味fileURL

+ ResourcePatternResolver批量加载资源

  1. 先判断资源路径是否在类路径下，然后再去判断路径是否允许存在多个匹配的资源？或者*
  2. 只要资源路径允许多个匹配的资源，就会通过findPathMatchingResources方法来寻找所有的匹配资源
  3. 若路径只存在单个匹配，就通过类加载器寻找类路径下的资源，findAllClassPathResource，其他资源则通过ResourceLoader的getResource方法获取。



#### BeanFactory

BeanFactory接口的本质是个对象工厂，也就是IOC容器的核心，他有很多种实现，ApplicationContext就是其中之一，而ApplicationContext支持的功能比BeanFactory更多，包括国际化、支持Web。

可以判断bean是否存在，是否为单例，还可以通过name或者type获取bean。

**对比FactoryBean？**

+ 工厂bean，能在需要的时候生产一个对象，本质上还是个bean，只不过能生产对象，也归beanFactory管理
+ 这两个其实没有可比性，只是名字相似。BeanFactory是提供IOC容器的最基本的形式，给具体的IOC容器实现提供了规范。FactoryBean是为bean的实现提供更灵活的方式，可以继承FactoryBean重写getObject方法。





### 循环依赖问题

循环依赖就是N个类中循环嵌套引用，如果在日常开发中我们用new 对象的方式发生这种循环依赖的话程序会在运行时一直循环调用，直至内存溢出报错。

spring对循环依赖的处理有三种情况： ①构造器的循环依赖：这种依赖spring是处理不了的，直接抛出BeanCurrentlylnCreationException异常。 ②单例模式下的setter循环依赖：通过“三级缓存”处理循环依赖。 ③非单例循环依赖：无法处理。

**构造器的循环依赖**

【singletonsCurrentlyInCreation.add()】Spring容器会将每一个正在创建的Bean标识符放在一个“当前创建Bean池”中，直到初始化完成会被移除，因此如果在创建Bean过程中发现自己已经在“当前创建Bean池”里时将抛出BeanCurrentlyInCreationException异常表示循环依赖；

**非单例循环依赖**

对于prototype作用域的bean，spring无法完成依赖注入，应为容器不进行prototype作用域bean的缓存，从而无法提前暴露一个创建中的bean。

**单例setter的循环依赖**

spring中bean是实例化之后再设置对象属性的。spring会把实例化结束的对象放到一个map中，

+ 一级缓存：singletonObjects 完成初始化的单例对象缓存；
+ 二级缓存：earlySingletonObjects 完成实例化但是尚未初始化的，提前曝光的单例对象缓存；
+ 三级缓存：singletonFactories 进入实例化阶段的单例工厂对象的缓存；

```java
protected Object getSingleton(String beanName, boolean allowEarlyReference) {
  // 1. 首先试图从一级缓存 singletonObjects中获取实例
    Object singletonObject = this.singletonObjects.get(beanName);
    // 2. 如果singletonObject中不存在，判断单例bean是不是正在创建中
    if (singletonObject == null && isSingletonCurrentlyInCreation(beanName)) {
        synchronized (this.singletonObjects) {
          // 3. 试图从二级缓存 earlySingletonObjects 中获取bean
            singletonObject = this.earlySingletonObjects.get(beanName);
            //allowEarlyReference 是否允许从singletonFactories中通过getObject拿到对象
          // 4. 如果二级缓存中还是没有，并且允许从三级缓存中获取引用
            if (singletonObject == null && allowEarlyReference) {
              // 5. 试图从三级缓存 singletonFactories 中获取ObjectFactoriy
                ObjectFactory<?> singletonFactory = this.singletonFactories.get(beanName);
                if (singletonFactory != null) {
                    singletonObject = singletonFactory.getObject();
                    //从singletonFactories中移除，并放入earlySingletonObjects中。
                    //6. 如果三级缓存中存在，就从三级缓存移动到了二级缓存
                    this.earlySingletonObjects.put(beanName, singletonObject);
                    this.singletonFactories.remove(beanName);
                }
            }
        }
    }
    return (singletonObject != NULL_OBJECT ? singletonObject : null);
}

```



### bean作用域

Spring框架支持以下五种bean的作用域：

- **singleton :** bean在每个Spring ioc 容器中只有一个实例。
- **prototype**：一个bean的定义可以有多个实例。 
- **request**：每次http请求都会创建一个bean，该作用域仅在基于web的Spring ApplicationContext情形下有效。
- **session**：在一个HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。
- **global-session**：在一个全局的HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。

缺省的Spring bean 的作用域是Singleton.

**原型作用域的具体应用场景**

简单的说一个是有状态的，一个是无状态的。

比如对所有人进行一个身份验证，那么身份验证的bean就可以是一个singleton，人这个bean就是prototype的了。





### bean的生命周期

生命周期概括起来主要4个阶段，实例化、属性赋值、初始化、销毁。

1. 实例化阶段：包括调用BeanFactoryPostProcessor的postProcessorBeanFactory对工厂定义信息进行后处理，调用bean的构造函数进行实例化。
2. 属性赋值：
3. 初始化：
   + 前：检查Aware的相关接口（beanNameAware、beanClassLoaderAware、beanFactoryAware）并调用一些设置方法，BeanPostProcessor的前置处理方法；
   + 中：如果实现initializingBean接口，调用 afterPropertiesSet() 方法；通过init-method配置的初始化方法；
   + 后：beanPostProcessor的后置处理方法；
4. 注销：
   + 使用前：注册了Destruction销毁相关回调接口
   + 使用后：调用DisposableBean的destroy方法，通过destroy-method配置的销毁方法。



![preview](https://pic2.zhimg.com/v2-a2cb36aabe9b6b044ade2a4f5bcaa759_r.jpg)

![img](https://user-gold-cdn.xitu.io/2020/1/13/16f9e9c89adf58d3?imageslim)

#### 生命周期方法的重载

1. init-method和destroy-method，可以在xml文件中的bean定义里配置；

   也可以使用@PostConstruct @PreDestroy注解来配置方法

   springboot中也用@configuration配置类来写这两个方法，@bean（initmethod=？）

```java
@Configuration
public class LifeCycleConfig {
    @Bean(initMethod = "start", destroyMethod = "destroy")
    public SpringLifeCycle create(){
        SpringLifeCycle springLifeCycle = new SpringLifeCycle() ;
        return springLifeCycle ;
    }
}
public class SpringLifeCycle{
    private final static Logger LOGGER = LoggerFactory.getLogger(SpringLifeCycle.class);
    public void start(){
        LOGGER.info("SpringLifeCycle start");
    }
    public void destroy(){
        LOGGER.info("SpringLifeCycle destroy");
    }
}
```

2. 可以实现 `InitializingBean,DisposableBean` 这两个接口，也是在初始化以及销毁阶段调用。
3. 实现 BeanPostProcessor 接口，Spring 中所有 bean 在做初始化时都会调用该接口中的两个方法
4. 通过让bean 实现 Aware 接口，则能在 bean 中获得相应的 Spring 容器资源（beanname、classloader、beanfactory的引用）

### Autowired&Resource

1、共同点
两者都可以写在字段和setter方法上。两者如果都写在字段上，那么就不需要再写setter方法。
2、不同点
（1）@Autowired
@Autowired为Spring提供的注解，需要导入包org.springframework.beans.factory.annotation.Autowired;只按照byType注入。
@Autowired注解是按照类型（byType）装配依赖对象，默认情况下它要求依赖对象必须存在，如果允许null值，可以设置它的required属性为false。如果我们想使用按照名称（byName）来装配，可以结合@Qualifier注解一起使用。
（2）@Resource
@Resource默认按照ByName自动注入，由J2EE提供，需要导入包javax.annotation.Resource。@Resource有两个重要的属性：name和type，而Spring将@Resource注解的name属性解析为bean的名字，而type属性则解析为bean的类型。所以，如果使用name属性，则使用byName的自动注入策略，而使用type属性时则使用byType自动注入策略。如果既不制定name也不制定type属性，这时将通过反射机制使用byName自动注入策略。





### 静态代理和动态代理

代理模式是设计模式的一种，提供了对目标对象额外的访问方式，即通过代理对象访问目标对象，这样可以在不修改原目标对象的前提下，提供额外的功能操作，扩展目标对象的功能。

总之，代理模式就是设置一个中间代理类来控制访问原目标对象，以达到增强元对象的功能和简化访问的方式。

#### 静态代理

静态代理就是需要手动编写代码，继承同一个接口，并持有一个被代理人的引用。租房者和房屋中介的关系，房屋中介代理租房者行使租房的活动，并且比普通租房者完成的更好，增强了原目标对象的功能。

缺点：

1. 冗余：由于代理对象需要实现与目标对象一样的接口，会产生很多的代理类；
2. 不易维护：一旦接口增加了方法，就要同时修改目标对象和代理对象。



#### 动态代理

![img](https://user-gold-cdn.xitu.io/2019/10/25/16e032afb51eecf8?imageslim)



与静态代理相比，多了一个**InvocationHandler**角色和一个**Proxy**角色

+ InvocationHandler是java提供的一个接口，我们需要定义一个类实现InvocationHandler接口，这里就叫DynamicProxy角色；**这个接口的作用就是在invoke方法中执行真实对象的方法**。

+ Proxy是java提供用于动态生成代理对象【ProxySubject】，它需要ProxySubject继承。Proxy持有一个invocationHandler类型的引用，提供了**getProxyClass**方法和**newProxyInstance**方法。

  getProxyClass方法：在运行时根据.class的结构生成一个代理class二进制流，并通过传入的classLoader去把代理class二进制流加载成一个代理class对象，该代理class对象继承proxy并实现了传入的interfaces列表。

+ 他们都在reflect包下；

**使用动态代理的步骤：**

1. 定义代理对象和真实对象的公共接口【同静态代理】；
2. 真实对象实现公共接口中的方法【同静态代理】；
3. 定义一个实现了InvocationHandler接口的动态代理类；
4. 通过Proxy类的newProxyInstance方法创建代理对象，调用代理对象的方法。

```java
public class DynamicProxy implements InvocationHandler{
  private Object mObject; // 真实对象的引用，这里用Object就可以一个动态代理类代理很多类
  public DynamicProxy(Object obj){
    this.mObject = obj;
  }
  @Override
  public Object invoke(Object proxy, Method method. Object[] args) throws Throwable{
    // 通过反射调用真实对象的方法
    Object result = method.invoke(mObject, args);
    return result;
  }
}
public class Client{
  public static void main(String[] args){
    // 构造一个被代理的对象
    Iroom xiaoming = new XiaoMing();
    // 构造一个动态代理类
    InvocationHandler dynamicProxy = new DynamicProxy(xiaoming);
    // 获取被代理类xiaoming的classLoader
    ClassLoader classloader = xiaoming.getClass().getClassLoader();
    // 通过Proxy类的newProxyInstance方法动态构建一个代理人房产中介
    // new Class[]{Iroom.class}是传入了接口列表
    Iroom roomAgency = (Iroom) Proxy.newProxyInstance(classloader, new Class[]{Iroom.class}, dynamicProxy);
    // 通过代理对象调用方法
    roomAgency.watchRoom();
    // 当我们调用代理对象的方法时，这个方法的处理逻辑就会委托给InvocationHandler实例的invoke方法执行，通过反射调用真实对象的方法。
  }
}
```

动态代理可以通过一个代理类来代理N多个被代理类，它在更换接口时，不需要重新定义代理类，因为动态代理**不需要根据接口提前定义代理类，它把代理类的创建推迟到代码运行时来完成**。



#### cglib

code generation library 是一个第三方代码生成类库，运行时在内存中动态生成一个子类对象从而实现对目标对象功能的扩展。

+ JDK的动态代理需要实现一个或多个接口，但是cglib不需要实现接口就能代理；
+ cglib底层是通过使用一个小而快的字节码处理框架ASM，来转换字节码并生成新的类。
+ cglib会继承目标对象，需要重写方法，所以目标对象不能是final的。

```java
public class ProxyFactory implements MethodInterceptor{
  private Object target;
  public ProxyFactory(Object target){
    this.target = target;
  }
  public Object getProxyInstance(){
    Enhancer en = new Enhancer(); //获取工具类
    en.setSuperclass(target.getClass()); // 设置父类，提供superclass供代理类来继承
    en.setCallback(this); // 设置回调函数
    return en.create(); // 创建子类对象代理
  }
  // 所有方法都通过interceptor的invoke进行拦截代理
  @Override
  public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable{
    System.out.print("-----begin-----");
    Object returnvalue = method.invoke(target, args);
    System.out.print("-----end-----");
    return null;
  }
  
}
public static void main(){
  UserDao target = new UserDao();
  UserDao proxy = (UserDao) new ProxyFacory(target).getProxyInstance();
  proxy.save();
}
```



#### Spring AOP

AOP，一般称为【面向切面】编程，作为面向对象的一种补充，即找出对多个对象产生影响的公共行为，并将其封装为一个可重用的模块，这个模块被命名为“切面”（Aspect），切面将那些与业务无关，却被业务模块共同调用的逻辑提取并封装起来，减少了系统中的重复代码，降低了模块间的耦合度，同时提高了系统的可维护性。**可用于权限认证、日志、事务处理**。**AOP的核心功能的底层实现机制：如何用动态代理来实现切面拦截。**

 AOP实现的关键在于AOP框架自动创建的AOP代理，AOP代理主要分为静态代理和动态代理。**静态代理的代表为AspectJ；动态代理则以Spring AOP为代表。**

（1）AspectJ是静态代理的增强，所谓静态代理，就是AOP框架会在编译阶段生成AOP代理类，因此也称为编译时增强，他会在编译阶段将AspectJ织入到Java字节码中，运行的时候就是增强之后的AOP对象。

（2）Spring AOP使用的动态代理，所谓的动态代理就是说AOP框架不会去修改字节码，而是每次运行时在内存中临时为方法生成一个AOP对象，这个AOP对象包含了目标对象的全部方法，并且在特定的切点做了增强处理，并回调原对象的方法。

+ Aspect切面：使用@Aspect注解的类就是一个切面，是Advice通知和PointCut切点的结合，即切面在何时、何地做什么事；
+ Advice通知：定义了切面什么时候做什么，Spring有5种通知类型@Before @After @AfterThrowing @AfterReturning @Around
+ JoinPoint连接点：应用程序能够插入切面的一个时机，切点的集合；
+ PointCut切点：切面在哪做事；
+ Weaving织入：把切面应用到目标对象并创建新的代理对象的过程。切面在指定的链接点被织入到目标对象中。



Spring AOP中的动态代理主要有两种方式，JDK动态代理和CGLIB动态代理。JDK动态代理通过反射来接收被代理的类，并且要求被代理的类必须实现一个接口。JDK动态代理的核心是InvocationHandler接口和Proxy类。
如果目标类没有实现接口，那么Spring AOP会选择使用CGLIB来动态代理目标类。CGLIB（Code Generation Library），是一个代码生成的类库，可以在运行时动态的生成某个类的子类，注意，CGLIB是通过继承的方式做的动态代理，因此如果某个类被标记为final，那么它是无法使用CGLIB做动态代理的。

**Spring AOP方法内部调用不生效**

一个接口的两个方法都实现了动态代理，调用效果是什么：

1. 直接调用A方法，会进行增强；

2. 方法B中调用方法A，方法A增强逻辑会失效；**这里是this目标对象的直接调用，而不是代理对象进行调用**。AOP实现的基础是IOC，如果不是通过IOC管理的对象实例，而是this或者super调用，AOP无效。

3. 同样的道理，**@Transactional @Async等注解不起作用**也是因为，这些注解的功能实际上都是AOP实现的

4. 解决方法：

   + @EnableAspectJAutoProxy(exposeProxy = true,proxyTargetClass = true)切面开启注解；((X)AopContext.currentProxy()).B() 拿到当前类的代理对象，调用方法A；

     这种方法对Async注解还是没有用。

   + @Autowired private ApplicationContext applicationContext; 

     ((Son) applicationContext.getBean(Son.class)).A(); 先获取上下文，通过getbean来获取代理类

   + 将这个bean注入到自身，这时候注入的是代理对象，而不是目标对象，通过这个注入的bean来调用A方法，动态代理就生效了。



#### JDK动态代理和CGLib性能对比

在1.6和1.7的时候，JDK动态代理的速度要比CGLib动态代理的速度要慢。

低版本的时候，运行的性能可能不如CGLib，但是在1.8版本中运行多次，基本都可以得到一致的测试结果，那就是JDK动态代理已经比CGLib动态代理快了

+ JDK动态代理通过反射的方式来进行代理，比较消耗系统性能。而在类生成的时候比较高效
+ cglib通过生成类字节码来实现代理，比反射稍快。asm生成类【继承生成子类比较慢】之后的执行比较高效

**为什么1.8的动态代理变快了？**

可能是由于Proxy底层使用了concurrentHashMap用来缓存，而ConcurrentHashMap在1.8中做了比较大的优化。

#### AspectJ和Spring AOP

Spring AOP和AspectJ其实没有什么关系，只是它沿用了AspectJ中的概念，使用了AspectJ提供jar包的注解，但是没有依赖AspectJ框架来实现功能。因为AspectJ属于静态代理，实际运行之前就完成了织入，所以它生成类是没有额外运行时开销的。功能比Spring AOP更强大。

#### Springboot的AOP

1. pom文件下引入依赖，spring-boot-starter-aop。这时候不需要特殊配置，因为引入依赖之后，默认开启@EnableAspectJAutoProxy 。如果需要使用CGLIB来实现AOP，需要配置spring.aop.proxy-target-class=true。
2. 实现AOP切面，定义一个切面类，类上使用注解@Aspect 和 @Component；@Pointcut定义一个切入点，可以是规则表达式，也可以是一个注解；根据需要定义切入方法，例如@Before
3. 切入方法如果需要共享变量，可以使用ThreadLocal来保存；
4. @Order(i) 注解来表示切面的优先级，i的值越小，优先级越高。

### SpringMVC流程

MVC处理请求流程，适配器返回ModelAndView可以为NULL吗（不会），为什么可以直接返回JSON给前端，怎么处理的（不会）

#### 流程

1. 请求被Web容器获取，冰鞋根据ContextPath将请求发送给DispatcherServlet前端控制器；
2. DispatcherServlet接收到请求之后，会设置一些属性（LocaleResolver、ThemeResolver），在根据request在handlerMapping中查找对应的HandlerExecutionChain；然后根据HandlerExecutionChain中的handler来找到HandlerAdapter，然后通过反射来调用handler中的对应方法；
3. handler就是对应的controller，调用controller对应的方法来进行业务逻辑的处理，返回ModelAndView逻辑视图名称；
4. 前端控制器得到返回的ModelAndView，然后交给视图解析器来处理。ViewResolver视图解析器根据视图名称、视图前后缀来获取实际的物理视图；
5. 获取实际视图之后，就会使用model来渲染视图，得到用户实际看到的视图，然后返回给客户端。

#### 返回值

1. ModelAndView：通过SetViewName指定视图名称，addObject绑定数据模型；【如果ModelAndView的值是null，logger.debug会报错，指明是出现了null的ModelAndView】
2. void：返回值是void的时候不一定是没有返回值，可能返回值写在HttpServletResponse里；
3. String： 
   + String可能是逻辑视图名字，数据放在方法参数Model里面；
   + 重定向：**return** "redirect:/aa/index";
   + 转发：**return** "forward:/WEB-INF/jsp/order.jsp";
   + 真的是String，需要加上@responseBody注解



### Interceptor和Filter

拦截器：面向切面编程中，动态代理就是拦截器的简单实现。实现implements HandlerInterceptor，重写preHandle、postHandle、afterCompletion方法。注册拦截器的时候可以规定拦截规则，也就是哪些url可以应用这个拦截器。

过滤器：javaweb中可以在requst或者response提前过滤一些信息（比如非法url，没有登陆的用户请求）。实现filter接口，@WebFilter设置urlpattern和filtername

**区别：**

1. 拦截器基于反射机制；过滤器基于函数回调；
2. 拦截器不依赖servlet容器；过滤器依赖servlet容器；
3. 拦截器只能对action请求起作用；过滤器可以对几乎所有请求起作用；
4. 拦截器可以多次调用；但是过滤器只能一次，根据filtermapping的配置先后顺序来执行；

### Spring的事务

声明式事务：基于spring AOP实现。管理事务的接口：

+ PlatformTransactionManager平台事务管理：事务策略的核心；
+ TransactionDefinition事务定义：给定的事务规则；**事务隔离级别、事务传播行为、事务超时、事务的只读属性和事务的回滚规则**
+ TransactionStatus事务状态：代表事务本身；

#### 隔离级别

1. ISOLATION_Default：默认，大部分数据库都是READ_COMMITED
2. ISOLATION_READ_UNCOMMITTED：
3. ISOLATION_READ_COMMITTED：防止脏读；
4. ISOLATION_REPEATABLE_READ：防止脏读和可重复读；
5. ISOLATION_SERIALIZABLE：所有的义务依次执行，不可互相干扰。

#### 事务传播行为

1. **required（默认）**：
   + 已存在事务，加入当前事务；
   + 不存在事务，新开启事务；
   + 在ServiceA.methodA()或在ServiceB.methodB()内的任何地方出现异常，事务都会被回滚；即使B已经提交了。
2. **Requires_new**
   + 已存在事务A，挂起当前事务A，新开启事务B；
   + 不存在，新开启事务
   + A，B的异常互不影响，提交还是会滚看自己的情况。
3. supports
   + 支持当前事务；
   + 不存在，也没关系
4. Not_supported
   + 当前事务挂起，以非事务方式运行。
5. **nested**
   + 建立父子事务关系，B事务和A事务是相关联的，提交需要等待父事务提交；
   + 父事务会滚，子事务也回滚
   + 子事务回滚，父事务不影响。
6. mandatory：支持当前事务，且如果当前没有事务，抛出异常
7. Never：不支持事务，且如果存在事务，抛出异常

#### 回滚规则

通常情况下，如果在事务中抛出了未检查异常（继承自 RuntimeException 的异常），则默认将回滚事务。如果没有抛出任何异常，或者抛出了已检查异常，则仍然提交事务。



## Springboot

### Spring与Springboot的区别

springboot本质上是在Spring的基础上，默认配置了许多框架的使用方法，无需配置就能够快速整合第三方框架。

SpringMVC则是基于spring的一个MVC框架。

### 启动流程

1. SpringApplication初始化模块，配置一些基本的环境变量、资源、构造器、监听器；
2. 应用具体的启动，包括启动流程的监听模块、加载配置环境模块、核心是创建上下文环境模块；
3. 自动化配置模块；

#### SpringApplication类

1. 每个Springboot的启动入口main方法调用springApplication.run方法启动整个springboot程序，该方法所在的类使用了@SpringBootApplication注解，他包含了3个注解的意义：
   + EnableAutoConfiguration：自动配置；META-INF/spring.factories配置文件，并将其中对应的配置项通过反射实例化为对应的配置类，加入到IOC容器中。
   + SpringbootConfiguration：内部为configuration，可以在applicationContext.xml配置文件定义bean；
   + ComponentScan：组建扫描，可以自动发现和装配bean，【默认扫描SpringApplication的run方法里的启动类所在的包路径下文件，所以最好将该启动类放到根包路径下。】
   + @ImportResource("classpath:xxxx.xml")
   
2. 启动方法先是new 了一个SpringApplication对象，构造方法中initialize方法初始化模块。initialize方法主要为SpringApplication对象赋一些初始值。

3. run方法中：

   创建了应用的监听器开始监听**SpringApplicationRunListeners**，用于监听run方法的执行；

   加载了配置环境ConfigureableEnviroment。这个run方法返回的是一个上下文对象，**ConfigurableApplicationContext**应用配置上下文。prepareContext方法将之前得到的监听器、配置环境和这个上下文进行绑定，然后通过refreshContext方法来完成bean的实例化。

#### 自动化配置

springboot中用到的SpringFactories扩展机制，类似java中的SPI。SPI就是不同的服务开发商在各自提供的jar包中提供一个相同名字的配置文件，名字和java提供的通用接口同名。比如说数据库开发商的jar包下META_INF/services目录下放了Driver文件（Driver是java的一个接口）。

Springboot是在META_INF/spring.factories文件中配置接口的实现类名称，然后在程序中读取这些配置文件，并且实例化，注入到IOC容器中。这个过程主要用到的是spring工厂加载器，

主要用到了**SpringFactoriesLoader** 即**Spring工厂加载器**，根据**工厂类名称**和对应的**类加载器**，获取指定的spring.factories文件。传入的工厂类名称是接口名，而文件中对应的则是接口的实现类，所以spring.factories文件中存放的是一对多的类名集合。获取到这些类名集合，通过反射得到这些类的class对象、构造方法。最终生成工厂类实例返回。

xxx-starter 这些组件的meta-inf文件下均含有spring.factories文件。

配合@EnableAutoConfiguration使用的话，它更多是提供一种配置查找的功能支持，即根据@EnableAutoConfiguration的完整类名org.springframework.boot.autoconfigure.EnableAutoConfiguration作为查找的Key,获取对应的一组@Configuration类　　

从classpath中搜寻所有的META-INF/spring.factories配置文件，并将其中org.springframework.boot.autoconfigure.EnableAutoConfiguration对应的**配置项**通过**反射（Java Refletion）**实例化为对应的标注了**@Configuration**的JavaConfig形式的IoC容器配置类，然后汇总为一个并加载到IoC容器。











## SpringDataJPA

他是Spring给予ORM框架、JPA规范的基础上封装的一套JPA应用框架。底层使用了Hibernate的JPA技术实现。

**spring.jpa.properties.hibernate.hbm2ddl.auto**

+ create：每次都删除上一次生成的表和数据，然后重新生成表；（测试可用）
+ Create-drop：每次生成表，sessionFactory关闭的时候，删除生成的表；
+ update：第一次创建数据库，之后每次实体更新，就会新增字段，但是不会删除字段；
+ validate：每次验证数据表结构，智慧和已经存在的表进行比较，根据model修改表结构，不会创建新表。

### 实现原理

声明 `UserRepository`接口，继承`JpaRepository`。

可以发现`userRepository`被注入了一个动态代理，被代理的类是`JpaRepository`的一个实现`SimpleJpaRespositry`。

在进到`findByUserName`方法的时候，发现被上文提到的`JdkDynamicAopProxy`捕获，然后经过一系列的方法拦截，最终进到`QueryExecutorMethodInterceptor.doInvoke`中。这个拦截器主要做的事情就是判断方法类型，然后执行对应的操作.
我们的`findByUserName`属于自定义查询，于是就进入了查询策略对应的`execute`方法。在执行`execute`时，会先选取对应的`JpaQueryExecution`，调用`AbtractJpaQuery.getExecution()`

根据method变量实例化时的查询设置方式，实例化不同的JpaQueryExecution子类实例去运行。我们的`findByUserName`最终落入了`SingleEntityExecution` —— 返回单个实例的 `Execution`。继续跟踪`execute`方法，发现底层使用了 hibernate 的 `CriteriaQueryImpl` 完成了sql的拼装，这里就不做赘述了。



## 网络

### 体系7/5/4

四层：应用、运输、网际、网络接口层

五层：应用、运输、网络、链路、物理

七层：应用、表示、会话、运输、网络、链路、物理

### Ping

Ping命令是用来确定本地主机与网络中其他主机的网络通信情况，或者查看是否为有效IP。

#### 工作原理

向网络的另一台主机发送ICMP报文，通过返回信息判断网络连接状况。ping的返回信息里包括一个值为TTL，表示ping程序发送的ICMP数据包的生存周期，每经过一个网段，TTL减1，当其值被减到0的时候，该数据包将被丢弃，该数据包的源地址被告知这个情况，以重新发送数据包。

> 没有通过传输层的TCP或者UDP。通过ARP--ICMP--IP协议
>
> 创建通信套接字：将地址、端口信息绑定于套接字；构建IP包头和ICMP包头；发送构建数据包；接收对方主机回应；给出程序反馈信息。

1. 在同一网段：

   在主机 A 上运行“Ping 192.168.0.5”后，都发生了些什么呢? 首先，Ping 命令会构建一个 固定格式的 ICMP 请求数据包， 然后由 ICMP 协议将这个数据包连同地址“192.168.0.5”一起 交给IP 层协议（和 ICMP 一样，实际上是一组后台运行的进程），IP 层协议将以地址 “192.168.0.5”作为目的地址，本机 IP 地址作为源地址，加上一些其他的控制信息，构建一 个 IP 数据包，并想办法得到 192.168.0.5 的MAC 地址（物理地址，这是数据链路层协议构 建数据链路层的传输单元——帧所必需的），以便交给数据链路层构建一个数据帧。关键就 在这里，IP 层协议通过机器 B 的 IP 地址和自己的子网掩码，发现它跟自己属同一网络，就 直接在本网络内查找这台机器的 MAC,如果以前两机有过通信，在 A 机的 ARP 缓存表应该 有 B 机 IP 与其 MAC 的映射关系，如果没有，就发一个 ARP 请求广播，得到 B 机的 MAC, 一并交给数据链路层。后者构建一个数据帧，目的地址是 IP 层传过来的物理地址，源地址 则是本机的物理地址，还要附加上一些控制信息，依据以太网的介质访问规则，将它们传送 出去。 主机 B 收到这个数据帧后，先检查它的目的地址，并和本机的物理地址对比，如符合， 则接收；否则丢弃。接收后检查该数据帧，将 IP 数据包从帧中提取出来，交给本机的 IP 层 协议。同样，IP 层检查后，将有用的信息提取后交给 ICMP 协议，后者处理后，马上构建 一个 ICMP 应答包，发送给主机 A，其过程和主机 A 发送 ICMP 请求包到主机 B 一模一样。

2. 不同网段：

   在主机 A 上运行“Ping 192.168.1.4”后，开始跟上面一样，到了怎样得到 MAC 地址时，IP 协议通过计算发现 D 机与自己不在同一网段内，就直接将交由路由处理，也就是将路由的 MAC 取过来，至于怎样得到路由的 MAC，跟上面一样，先在 ARP 缓存表找，找不到就广 播吧。路由得到这个数据帧后，再跟主机 D 进行联系，如果找不到，就向主机 A 返回一个 超时的信息。

##### 返回信息分析

+ request time out：对方机器关机、或者地址错误；对方不在同一网段，通过路由也没办法找到或者超时；对方设置了防火墙这种ICMP数据包过滤；【可以用带参数 -a 的Ping命令探测对方，如果能得到对方的NETBIOS名称，则说明对方是存在的，是有防火墙设置】
+ Destination host unreachable：对方和自己不在同一网段，自己没有设置默认路由，或者所经过的路由表连到达目标的路由都没有。【timeout是路由表有目标路由，但是处于某种原因不可达】
+ bad ip address：没有连接到DNS，无法解析IP

##### ICMP协议

网际控制报文协议ICMP，Internet control message protocal。可以在网络中实现主机探测、路由维护、路由选择、流量控制。由于IP协议没有机制来获取错误信息、对错误进行处理，所以需要ICMP来解决。**ICMP是IP层的一部分**，ICMP报文封装在IP数据报内部，用于传输差错报文、控制报文。

![image-20200803095347865](/Users/chengleiyi/Library/Application Support/typora-user-images/image-20200803095347865.png)

ICMP数据包包括：类型、代码、检验和、标识符、序号、选项数据。

1. 查询报文类型：ping、子网掩码、时间戳查询。ping的时候发送的ICMP数据包类型8（回送请求），返回的类型0（回送应答）；
2. 差错报文类型：当数据传送发送错误的时候，目标不可达、超时、参数问题、重定向；
3. traceroute指令，利用ICMP的差错报文可以实现遍历到数据包传输路径上的所有路由器。

### ARP RARP

ARP地址解析协议：实现从IP到MAC地址的转换。IP是目标在网络中所被分配的节点，MAC是对应目标网卡所在的固定地址。

ping发送ICMP的时候，需要填写目标IP和目标MAC，此时我只知道目标IP，所以会使用ARP获取目标IP的MAC地址。存储到本地的【ARP缓存表】

```shell
arp -a
```

ARP协议是通过「广播」的形式发送的，地址是最大值，交换机收到广播，会把它发送给同一局域网的其他所有主机【MAC有12个f，IP有4个255】

目标主机收到之后，发现是自己的IP，不会丢弃包，而是返回ARP响应包。

功能上来看，ARP是链路层的协议，但是基于分层包装来看，他和IP协议同一级，属于网络层协议【都是基于ethernet】。

#### ARP攻击

①PC1需要跟PC2通信，通过ARP请求包询问PC2的MAC地址，由于采用广播形式，所以交换机将ARP请求包从接口P1广播到P2和PC3。（注：**交换机收到广播/组播/未知帧都会其他接口泛洪**）

②PC2根据询问信息，返回ARP单播回应包；此时PC3作为攻击者，没有返回ARP包，但是处于"**监听**"状态，为后续攻击做准备。

③PC1和PC2根据ARP问答，将各自的ARP映射信息（IP-MAC）存储在本地ARP缓存表。

④交换机根据其学习机制，记录MAC地址对应的接口信息，存储在**CAM缓存表**（也称为MAC地址表）。交换机收到数据包时，会解封装数据包，根据**目标MAC**字段进行转发。

⚠️正常情况下，若收到的ARP请求不是给自己的，则直接丢弃；而这里PC3（Hacker）在监听之后，发起了ARP回应包：**我就是PC2（IP2-MAC3）**。ARP和CAM表，就是遵循"后到优先"原则。只要持续不停发出ARP欺骗包，就一定能够覆盖掉正常的ARP回应包。**稳健的ARP嗅探/渗透工具，能在短时间内高并发做网络扫描（例如1秒钟成千上百的数据包），能够持续对外发送欺骗包。**就这样，PC1本来要发给PC2的数据包，落到了PC3（Hacker）手里，这就完成了一次完整的ARP攻击

+ 断网攻击；限速攻击
+ 窃取账号，只要是明文传输的都可能被窃取

**防御技术：**

+ **DAI（Dynamic ARP Inspection）- 动态ARP检测**：交换机记录每个port对应的IP地址和MAC，即**port<->mac<->ip**，生成DAI检测表；检测每个接口发送过来的ARP回应包，根据DAI表判断是否违规，若违规则丢弃此数据包并对接口进行惩罚。可以直接将接口"**软关闭**"，直接将攻击者断网；也可以"**静默处理**"，仅丢弃欺骗包，其他通信正常）
+ **通过安装ARP防火墙做安全防御**：绑定正确的的IP和MAC映射，收到攻击包时不被欺骗；ARP静态双向绑定，优先级高于动态学习到的。

#### RARP（Reverse ARP）

即反向ARP或者翻转ARP，顾名思义，它跟常规的ARP功能恰恰是相反的，**ARP是实现IP到MAC地址的映射，而RARP是实现MAC到IP地址的映射**。RARP是一种逝去的地址分配技术，是Bootp和DHCP的鼻祖，目前我们的电脑基本不会用到这个协议，只有部分无盘工作站等情况需要用到。

这就有了后面的DHCP，DHCP通过动态分配的方式解决了这个诟病，并且通过DHCP中继技术实现了跨网段地址分配，实现了全网IP地址的统一管理。

### TIME_WAIT & CLOSE_WAIT

![image-20200803133835627](/Users/chengleiyi/Documents/Lernen/笔记/image-20200803133835627.png)

ESTABLISHED 表示正在通信，TIME_WAIT表示主动关闭，CLOSE_WAIT表示被动关闭。

#### 如何产生

TIME_WAIT在TCP关闭连接的时候产生的，出现在主动方；CLOSE_WAIT出现在被动方：

+ 关闭连接的过程：
  + 主动关闭的一方，调用socket的close()；协议层发送FIN包；
  + 被动关闭的一方收到FIN包之后，回复ACK，进入CLOSE_WAIT状态；主动关闭的一方等待对方关闭，进入FIN_WAIT_2状态。这时候主动方在等待被动方应用程序调用close操作；
  + 被动方在完成一系列工作之后，调用close()操作，此时，协议层灰发送FIN包给主动方，进入LAST_ACK状态；
  + 主动关闭的一方收到FIN包之后回复ACK，进入TIME_WAIT状态；被动关闭的一方收到ACK之后，进入CLOSED状态；
  + 等待2MSL（Maximum Segment Lifetime）时间，主动方结束TIME_WAIT进入CLOSED
+ Linux中1个MSL是30s
+ **为什么要有TIME_WAIT？**
  + 网络情况不好时，如果主动方无TIME_WAIT等待，这个TCP连接直接关闭；马上，主动方和被动方之间又重新建立连接，这是被动方重传或延时过来的FIN包过来后会直接影响新的TCP连接；
  + **即使后面没有建立新的连接，当主动方接受到被动方重传或延迟的FIN包后，会给被动方回复一个RST包，结果server认为发生错误connet reset by peer异常，可能会影响被动方其他的服务连接？？**为了确保两端能完全关闭连接。
  + 网络拥塞，主动方最后一个ACK被动方没收到，这时被动方会对FIN开启TCP重传，发送多个FIN包，在这时尚未关闭的TIME_WAIT就会把这些尾巴问题处理掉，不至于对新连接及其它服务产生影响。
  + **可靠地实现TCP全双工连接的终止。**（确保最后的ACK能让被关闭方接收）
  + **允许老的重复分节在网络中消逝，**为了确保后续的连接不会收到“脏数据”。（TCP中是可靠的服务，当数据包丢失会重传，当有数据包迷路的情况下，如果不等待2MSL时，当客户端以同样地方式重新和服务建立连接后，上一次迷路的数据包这时可能会到达服务，这时会造成旧包被重新读取）
  + 为什么是2MSL？2MSL就是一个发送和一个回复所需的最大时间。如果直到2MSL，Client都没有再次收到FIN，那么Client推断ACK已经被成功接收，则结束TCP连接

#### 大量？

##### TIME_WAIT

服务器在执行完http的请求之后，短连接情况下，服务器主动发送FIN关闭连接，作为主动方，最后会进入TIME_WAIT状态。

Linux系统下，TCP/IP连接断开后，会以TIME_WAIT状态保留一定的时间，然后才会释放端口。当并发请求过多的时候，就会产生大量的 TIME_WAIT状态的连接，无法及时断开的话，会占用大量的端口资源和服务器资源。这个时候我们可以考虑优化TCP/IP 的内核参数，来及时将TIME_WAIT状态的端口清理掉。

服务器和数据库服务器之间的连接，如果是服务器主动关闭，则TIME_WAIT出现在服务器端。

**1. 怎么查有多少TIME_WAIT存在？**

+ netstat -a | grep TIME_WAIT | wc -l

**2. 数量多了会对机器的性能有什么影响？**

+ 占用内存：内核里有一个保存所有连接的hash table，当有新的数据到来的时候，从这个hash table里快速找到这条连接。还有一个hash table用来保存所有的bound ports，主要用于可以快速找到一个可用的端口或者随机端口。**tcp socket一个占用不到4k**
+ 占用CPU：每次找随机端口，需要遍历bound ports
+ 占用源端口数量：(net.ipv4.ip_local_port_range)
+ 占用TIME_WAIT bucket， 会产生 time wait bucket table overflow，原因是超过了linux系统tw数量的阀值；
+ 占用文件描述符数量 (max open file)：linux规定能打开的文件数量有限，一个socket占用一个文件描述符。

**3. 怎么处理TIME_WAIT过多的情况？**

优化相关的内核参数来解决这个问题：

+ 开启tw重用： net.ipv4.tcp_tw_reuse = 1。允许将TIME_WAIT状态的socket重新用于新的TCP连接。这个参数对客户端有意义，在主动发起连接的时候会在调用的inet_hash_connect()中会检查是否可以重用TIME_WAIT状态的套接字

+ 开启tw回收： net.ipv4.tcp_tw_recycle = 1。允许TIME_WAIT状态的socket快速回收，默认是关闭的0

  > 对于tw的reuse、recycle其实是违反TCP协议规定的，服务器资源允许、负载不大的条件下，尽量不要打开

+ 开启timestamps： net.ipv4.tcp_timestamps=1 （**reuse 和 recycle的依赖参数**）

+ net.ipv4.tcp_max_syn_backlog = 4096：表示SYN队列的长度，默认为1024，加大队列长度，可以容纳更多等待连接的网络连接数。

+ net.ipv4.tcp_max_tw_buckets = 30000 放大bucket

+ 修改linux内核减小MSL时间

+ **大量短连接导致建立新连接超时问题，最后是通过维护长连接解决的。**

##### CLOSE_WAIT

如果一直保持在CLOSE_WAIT状态，那么只有一种情况，就是在对方关闭连接之后服务器程序自己没有进一步发出FIN信号，一般原因都是TCP连接没有调用关闭方法。**换句话说，就是在对方连接关闭之后，程序里没有检测到，或者程序压根就忘记了这个时候需要关闭连接，于是这个资源就一直被程序占着。**

这种情况，通过服务器内核参数也没办法解决，服务器对于程序抢占的资源没有主动回收的权利，除非终止程序运行，一定程度上，可以使用TCP的KeepAlive功能。

### TCP长连接

使用TCP通信，大概率会涉及到Socket、Netty。TCP本身是不分长短的，取决于如何使用。

+ 长连接：每次通信完毕，不关闭，可以做到连接的服用。好处是省去了创建连接的耗时。
+ 短链接：每次通信，创建socket；一旦通信结束，就调用socket.close。

> socket：是对TCP/IP协议的封装和应用，提供了一组基本的接口比如listen、accept、send、write、close，以供代码层面使用，能更方便的使用TCP/IP协议栈。socket是一个五元组：源IP、源端口、目的IP、目的端口、类型TCP/UDP

#### 连接的保活

1. keepAlive机制：keepAlive其实并不是TCP协议中的一部分，而是操作系统实现的机制，通过参数 `tcp_keepalive_time` 来设置，默认是7200s。在链路上没有数据传送的情况下，TCP层发送响应的keepalive探针来确定连接的可用性，探测失败后重试10次，（`tcp_keepalive_probes`），时间间隔75s `tcp_keepalive_intvl`  ，所有的探测失败之后，认为连接不可用。

   ```java
   bootstrap.option(ChannelOption.SO_KEEPALIVE, true);// netty中开启keepalive
   // linux 操作系统设置keepalive参数，修改/etc/sysctl.conf文件
   // 当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是``2``小时，改为``300``秒 
   // net.ipv4.tcp_keepalive_time=1200
   ```

   Keepalive机制，网络层面保证来连接的可用性。但是在应用层面可能还不够：

2. 应用层面的心跳机制：客户端会开启一个定时任务，定时对已经建立连接的对端应用发送请求（这里的请求是特殊的心跳请求），服务端则需要特殊处理该请求，返回响应。如果心跳持续多次没有收到响应，客户端会认为连接不可用，主动断开连接。

   心跳定时任务在发现连接不可用时，会根据当前是客户端还是服务端走不同的分支，客户端发现不可用，是重连；服务端发现不可用，是直接 close。

#### 区别：HTTP的keepAlive

+ http协议的keepalive意图在于多个http请求复用同一个TCP连接，同一个连接上穿行方式传递请求、响应数据；
+ TCP的keepalive机制意图在于保活，心跳，检测连接错误；需要自己维护一套心跳策略；
+ 这是两个不同的概念

#### KeepAlive常见错误

1. Connection TIME OUT：在发送一个探测保护包经过 (tcp_keepalive_time + tcp_keepalive_intvl * tcp_keepalive_probes) 时间后仍然没有接收到 ACK 确认情况下触发的异常，套接字被关闭；
2. no route to host；host unreachable(主机不可达) 错误，这个应该是 ICMP 汇报给上层应用的。
3. Connection reset by peer：链接被重置，终端可能奔溃死机重启之后，接收到来自服务器的报文，但是这时候只能重置。



### TCP报文段

![image-20200803153017968](/Users/chengleiyi/Library/Application Support/typora-user-images/image-20200803153017968.png)

1. 源端口号+目的端口号
2. seq序列号+ack确认序列号
3. 首部长度+一些标志位（常见的有ACK，RST，SYN，FIN）+窗口大小
4. 检验和+紧急指针
5. 选项+数据

### 浏览器的地址栏里输入一个url



#### URI & URL

+ URI：uniform resource identifier 同一资源标识符；

+ URL：uniform resource locator 同一资源定位符，定位互联网上的资源，俗称网址；

  **scheme://host.domain:port/path/filename** 

  + http/https定义了因特网服务的协议；
  + host：例如www，主机
  + domain：例如baidu.com，域名
  + Port: 端口号，冒号后面的就是端口号，可以隐藏
  + path：网络服务器上资源的路径
  + filename资源名称

+ URN：uniform resource name 同一资源名称

  isbn：0412321345

#### 流程细节

##### 浏览器怎么得到url？  

+ 用户在地址栏输入URL，而地址栏会根据用户的输入，如果输入的是非URL结构的字符串，就会用浏览器默认的搜索引擎来搜索该字符串。如果是URL结构，浏览器进程会把完整的URL通过进程间通信，发送给网络进程。

##### DNS解析是发送整个url吗？  

+ 只是域名部分，因为DNS就是把域名转换为IP

##### DNS解析详细过程？

+ 浏览器搜索自身的DNS缓存；
+ 搜索操作系统的DNS缓存：读取本地的Host文件；
+ 路由器缓存；【这3步都属于客户端DNS缓存】
+ 向本地DNS服务器进行查询等。对于向本地DNS服务器进行查询，如果要查询的域名包含在本地配置区域资源中，则返回解析结果给客户机，完成域名解析(此解析具有权威性)；
+ 如果要查询的域名不由本地DNS服务器区域解析，但该服务器已缓存了此网址映射关系，则调用这个IP地址映射，完成域名解析（此解析不具有权威性）。
+ 如果本地域名服务器并未缓存该网址映射关系，就向上级域名服务器查找，根域名；
+ 主机向本地域名服务器的查询一般采用**递归查询**，如果主机询问的本地域名服务器不知道被查询的IP地址，本地服务器会以DNS客户的身份向其他域名服务器继续发出查询请求
+ 本地域名服务器向根域名服务器的查询一**般采用迭代方式**，根域名服务器要么告诉IP地址，要么告诉它该向谁进行查询。

##### DNS劫持/HTTP劫持 ？

+ DNS劫持（DNS Hijacking）：又被称为域名劫持，DNS重定向（DNS direaction）,是一种DNS攻击方式。即是DNS查询没有得到正确的解析，以致引导user访问到恶意的网站，从而窃取用户隐私，或者进行某些恶意的操作

##### TCP三次握手之前需要什么？  

+ 目标IP和MAC地址

##### SYN泛洪攻击

+ 服务端返回客户端SYN-ACK包之后一直没有收到客户端的回应，这时候需要等待一个超时间才能断开SYN连接【Linux下默认发送5次SYN_ACK包，重试时间从1s开始倍增，一共等待63s】这时候，攻击者可以在短时间内发送大量的syn包给服务器，可以耗尽服务端的SYN队列。服务端维护一个很大的半连接请求而消耗非常多的资源。

+ 如果使用netstat命令能看到大量的SYN_RCVD的半连接，可以判断出受到了SYN泛洪攻击。

+ 对于应对SYN过多的问题，可以**减少SYN-超时时间**；回收最早的半连接；SYN cookie

  ```shell
  // 减少SYN-ACK的重发次数，也就减少了超时时间。默认是5。
  sysctl -w net.ipv4.tcp_synack_retries=3
  sysctl -w net.ipv4.tcp_syn_retries=3
  // 增加backlog队列
  sysctl -w net.ipv4.tcp_max_syn_backlog=2048
  // 开启SYN cookie技术
  sysctl -w net.ipv4.tcp_syncookies=1
  ```

##### ip怎么寻址？  

+ 

##### TCP连接的初始化序列号能否固定？

+ 不能固定，如果一直从同一个值开始，前后两次TCP连接的包可能会造成紊乱。如果前一次连接的包被路由器缓存了，直到后一次TCP连接开始传数据，才到达目的地，这样两次连接的数据包无法区分。
+ 而顺序递增的初始化序列号，很容易让攻击者猜测到TCP连接的ISN。
+ 目前的实现：在一个基准值的基础上随机。

### TCP连接中的两个队列

![img](/Users/chengleiyi/Documents/Lernen/笔记/tcp-sync-queue-and-accept-queue-small-1024x747.png)

三次握手中，在第一步server收到client的syn后，把相关信息放到半连接队列中，同时回复syn+ack给client（第二步）；

第三步的时候server收到client的ack，如果这时全连接队列没满，那么从半连接队列拿出相关信息放入到全连接队列中，否则按tcp_abort_on_overflow指示的执行。

这时如果全连接队列满了并且tcp_abort_on_overflow是0的话，server过一段时间再次发送syn+ack给client（也就是重新走握手的第二步），如果client超时等待比较短【net.ipv4.tcp_synack_retries 超时等待重试次数】，就很容易异常了。

##### 连接检查

```bash
netstat -s | egrep "listen | LISTEN"
667399 times the listen queue of a socket overflowed #全连接队列溢出的次数
667399 SYNs to LISTEN sockets ignored #半连接队列溢出的次数

ss -lnt
Recv-Q Send-Q Local Address:Port  Peer Address:Port 
0        50               *:3306             *:* 
# Send-Q 表示第三列的listen端口上的全连接队列最大为50，第一列Recv-Q为全连接队列当前使用了多少
```

**全连接队列的大小取决于：min(backlog, somaxconn) . backlog是在socket创建的时候传入的，somaxconn是一个os级别的系统参数**。 可以修改backlog

**半连接队列的大小取决于：max(64, /proc/sys/net/ipv4/tcp_max_syn_backlog)。 不同版本的os会有些差异**

##### accept队列满？

【client fooling】

如果client走完第三步在client看来连接已经建立好了，但是server上的对应连接实际没有准备好，这个时候如果client发数据给server，server会怎么处理呢？

server忽略client传来的数据包，然后client重传，一定次数后client认为异常，然后断开连接。

⚠️另外，如果全连接队列满了会影响TCP连接第一步drop 半连接的发生。如果accept backlog队列已满，且未超时的request socket的数量大于1，则丢弃当前请求SYN半连接请求  。



### Token的Redis实现

#### 1、流程分析

（1）客户端登录，输入用户名和密码，后台进行验证，如果验证失败则返回登录失败的提示。如果验证成功，则生成 token 然后将 username 和 token 双向绑定 （可以根据 username 取出 token 也可以根据 token 取出username）存入redis，同时使用 token+username 作为key把当前时间戳也存入redis。并且给它们都设置过期时间。

（2）每次请求接口都会走拦截器，重写了拦截器的preHandler方法，在这个方法中如果该接口标注了@AuthToken注解，则要检查客户端传过来的Authorization字段，获取 token。由于 token 与 username 双向绑定，可以通过获取的 token 来尝试从 redis 中获取 username，如果可以获取则说明 token 正确，反之，说明错误，返回鉴权失败。

（3）token可以根据用户使用的情况来动态的调整自己过期时间。在生成 token 的同时也往 redis 里面存入了创建 token 时的时间戳，每次请求被拦截器拦截 token 验证成功之后，将当前时间与存在 redis 里面的 token 生成时刻的时间戳进行比较，当当前时间的距离创建时间快要到达设置的redis过期时间的话，就重新设置token过期时间，将过期时间延长。如果用户在设置的 redis 过期时间的时间长度内没有进行任何操作（没有发请求），则token会在redis中过期。

```java
@Target({ElementType.METHOD, ElementType.TYPE})
@Retention(RetentionPolicy.RUNTIME)
public @interface AuthToken {

}
```

```java
@Component
public class AuthorizationInterceptor implements HandlerInterceptor {

    //存放鉴权信息的Header名称，默认是Authorization
    private String httpHeaderName = "Authorization";
    //鉴权失败后返回的错误信息，默认为401 unauthorized
    private String unauthorizedErrorMessage = "401 unauthorized";
    //鉴权失败后返回的HTTP错误码，默认为401
    private int unauthorizedErrorCode = HttpServletResponse.SC_UNAUTHORIZED;
	//存放登录用户模型Key的Request Key
    public static final String REQUEST_CURRENT_KEY = "REQUEST_CURRENT_KEY";

    @Override
    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {
        if (!(handler instanceof HandlerMethod)) {
            return true;
        }
        HandlerMethod handlerMethod = (HandlerMethod) handler;
        Method method = handlerMethod.getMethod();

        // 如果打上了AuthToken注解则需要验证token
        if (method.getAnnotation(AuthToken.class) != null || handlerMethod.getBeanType().getAnnotation(AuthToken.class) != null) {

            String token = request.getParameter(httpHeaderName);
            log.info("Get token from request is {} ", token);
            String username = "";
            Jedis jedis = new Jedis("192.168.1.106", 6379);
            if (token != null && token.length() != 0) {
                username = jedis.get(token);
                log.info("Get username from Redis is {}", username);
            }
            if (username != null && !username.trim().equals("")) {
                Long tokeBirthTime = Long.valueOf(jedis.get(token + username));
                log.info("token Birth time is: {}", tokeBirthTime);
                Long diff = System.currentTimeMillis() - tokeBirthTime;
                log.info("token is exist : {} ms", diff);
                if (diff > ConstantKit.TOKEN_RESET_TIME) {
                    jedis.expire(username, ConstantKit.TOKEN_EXPIRE_TIME);
                    jedis.expire(token, ConstantKit.TOKEN_EXPIRE_TIME);
                    log.info("Reset expire time success!");
                    Long newBirthTime = System.currentTimeMillis();
                    jedis.set(token + username, newBirthTime.toString());
                }
                //用完关闭
                jedis.close();
                request.setAttribute(REQUEST_CURRENT_KEY, username);
                return true;
            } else {
                JSONObject jsonObject = new JSONObject();
                PrintWriter out = null;
                try {
                    response.setStatus(unauthorizedErrorCode);
                    response.setContentType(MediaType.APPLICATION_JSON_VALUE);
                    jsonObject.put("code", ((HttpServletResponse) response).getStatus());
                    jsonObject.put("message", HttpStatus.UNAUTHORIZED);
                    out = response.getWriter();
                    out.println(jsonObject);
                    return false;
                } catch (Exception e) {
                    e.printStackTrace();
                } finally {
                    if (null != out) {
                        out.flush();
                        out.close();
                    }
                }

            }

        }
        request.setAttribute(REQUEST_CURRENT_KEY, null);
        return true;
    }

    @Override
    public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception {

    }

    @Override
    public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception {

    }
}

```



### RESTful

REST，Representational State Transfer是一种软件架构风格。它将服务端的信息和功能等所有事物都统称为资源，客户端的请求就是对资源进行操作，每个资源对应一个独一无二的URL。而客户端通过请求方法不同，实现对资源的查询GET、创建POST、修改PUT、删除DELETE。

## 海量数据问题

海量数据：所以一次性无法导入内存。

+ 分而治之/ hash映射， hash统计 ， 堆/快排/归并排序。
+ 双层桶划分
+ 布隆过滤器/ bitmap
+ trie树即前缀树字典树/ 数据库 / 倒排索引
+ 外排序
+ 分布式处理hadoop/ mapreduce

### 1. 布隆过滤器

#### **原理**

使用k个hash函数，长度m的数组。当一个元素被加入集合时，通过K个Hash函数将这个元素映射成一个位阵列（Bit array）中的K个点，把它们置为1。检索时，我们只要看看这些点是不是都是1就（大约）知道集合中有没有它了：如果这些点有任何一个0，则被检索元素一定不在；如果都是1，则被检索元素很可能在。

很明显这 个过程并不保证查找的结果是100%正确的。同时也不支持删除一个已经插入的关键字，因为该关键字对应的位会牵动到其他的关键字。所以一个简单的改进就是 counting Bloom filter，用一个counter数组代替位数组，就可以支持删除了。

如何根据输入元素的个数n，确定数组长度m和hash函数个数k？有两个公式可以由错误率推导出来。

#### 应用

 实现数据字典，用来判断是否重复，求集合的交集。但是会有误差！

### 2. 分治+哈希+堆排序

> 海量日志数据，提取出某日访问百度次数最多的那个IP。

1. 首先是这一天，并且是访问百度的日志中的IP取出来，逐个写入到一个大文件中。
2. 把大文件化成(取模映射)小文件。IP的位数是一定的，把大文件中的IP的hashcode取模映射为1000个小文件，这样相同的IP只会在同一个文件中。
3. hashmap统计：针对每个小文件，hash_map(ip，value)来进行频率统计。key是IP
4. 堆/快速排序：统计完了之后，便进行排序(可采取堆排序)，依次找出各个文件中频率最大的那个IP。然后再在这1000个最大的IP中，找出那个频率最大的IP。

> 搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门），请你统计最热门的10个查询串，要求使用的内存不能超过1G。

1. 3百万，每个1/4Kb，约为0.75G，可以放进内存中；
2. 使用hashmap<String, count> 存储 统计这些数据出现的频率；
3. 借助小根堆，找出TOPK，时间复杂度NlogK；
4. 也可以使用前缀树

> 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？

1. 把a、b文件分别根据hashcode取模分到1000个小文件中，a、b中相同的url一定会被分到对应的小文件；
2. 使用hashset统计每对小文件的共同url；
3. 归并1000个小文件

### 3. 桶--分治

> 2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。

整数个数为2^32,也就是，我们可以将这2^32个数，划分为2^8个区域(比如用单个文件代表一个区域)，然后将数据分离到不同的区域，然后不同的区域在利用bitmap就可以直接解决了。

> 5亿个int找它们的中位数。

同样需要做两遍统计，如果数据存在硬盘上，就需要读取2次。
方法同基数排序有些像，开一个大小为65536的Int数组，第一遍读取，统计Int32的高16位的情况，也就是0-65535，都算作0,65536 - 131071都算作1。就相当于用该数除以65536。Int32 除以 65536的结果不会超过65536种情况，因此开一个长度为65536的数组计数就可以。每读取一个数，数组中对应的计数+1，考虑有负数的情况，需要将结果加32768后，记录在相应的数组内。
第一遍统计之后，遍历数组，逐个累加统计，看中位数处于哪个区间，比如处于区间k，那么0- k-1的区间里数字的数量sum应该<n/2（2.5亿）。而k+1 - 65535的计数和也<n/2，第二遍统计同上面的方法类似，但这次只统计处于区间k的情况，也就是说(x / 65536) + 32768 = k。统计只统计低16位的情况。并且利用刚才统计的sum，比如sum = 2.49亿，那么现在就是要在低16位里面找100万个数(2.5亿-2.49亿)。这次计数之后，再统计一下，看中位数所处的区间，最后将高位和低位组合一下就是结果了。

首先我们将int划分为2^16个区域，然后读取数据统计落到各个区域里的数的个数，之后我们根据统计结果就可以判断中位数落到那个区域，同时知道这个区域中的第几大数刚好是中位数。然后第二次扫描我们只统计落在这个区域中的那些数就可以了。
实际上，如果不是int是int64，我们可以经过3次这样的划分即可降低到可以接受的程度。即可以先将int64分成2^24个区域，然后确定区域的第几大数，在将该区域分成2^20个子区域，然后确定是子区域的第几大数，然后子区域里的数的个数只有2^20，就可以直接利用direct addr table进行统计了。

双堆：一个最大堆一个最小堆，维护中位数。

### 4. bit-map

**bit-map就是用一个bit位来标记某个元素对应的value。常用于排序，判重，统计，查找。**

> 排序：对0-7内的5个元素(4,7,2,5,3)排序（这里假设这些元素没有重复）

可扩展为海量不重复的元素排序，java中每个int从32bit将为1bit，减少占用的内存。

> 已知某个文件内包含一些电话号码，每个号码为8位数字，统计不同号码的个数。

​	8位最多99 999 999，0-99999999之间的10^8个数子，每个数字1bit位，大概需要99m个bit，大概十几MB的内存即可。

遍历电话号码，置对应的bit位为1。

> 2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。

int 整数有2^32种可能，如果内存限制严重（2^32bit~512M），就先用分治的思想，把整数区间划分为2^8个小区域，每个区域约2^24个值（2M）。

把每个整数用2bit位来表示，00表示不存在，01表示存在1个，10表示存在2个及以上（重复）。遍历这些数据，将其对应的bit位改变。最后遍历bitmap找出bit位01的数据。

> 给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？

用位图/Bitmap的方法，申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。

### 5. 倒排索引

问题实例：文档检索系统，查询那些文件包含了某单词，比如常见的学术论文的关键字搜索。
关于倒排索引的应用，更多请参见：
第二十三、四章：杨氏矩阵查找，倒排索引关键词Hash不重复编码实践，
第二十六章：基于给定的文档生成倒排索引的编码与实践。

### 6. 外排序

有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16个字节，内存限制大小是1M。返回频数最高的100个词。
这个数据具有很明显的特点，词的大小为16个字节，但是内存只有1M做hash明显不够，所以可以用来排序。内存可以当输入缓冲区使用。
关于多路归并算法及外排序的具体应用场景

### 7. 前缀树Trie

## 分布式事务

分布式事务这块实现方案比较多，互联网公司很少用2pc这样的方案，一般都是保证事务的最终一致性，常用事务消息实现（以及tcc等）。要知道如何利用事务消息去实现最终一致性，以及事务性消息是如何实现的。

分布式事务没有一个绝对的方案，都是因业务场景的不同而不同。举个例子，电商场景中如何保证订单落库和减库存、锁券的最终一致性的（不同公司有不同方案，只是列举个我知道的）。

1. 收到用户下单请求时，在订单库中创建一条不可见订单。
2. 同步调用减库存接口，如失败跳转到4
3. 同步调用锁券接口，成功跳转到5，失败跳转到4
4. 发送一条废单消息，收到消息后回库存、回券
5. 将订单改为可见

分布式事务的问题，我面试的公司基本上都问过。

### 如何保证系统高可用

这个一般会结合你的项目来问，比如上游系统请求量过大、依赖的下游非核心应用挂掉、系统出问题如何及时发现等等。主要是从限流、降级、监控、报警、主从备份、容灾等角度出发

### 分布式锁

实现分布式锁常见方案有：利用db的唯一键、redis、zk等。其中redis实现分布锁应该是用的最多的。关于如何用redis实现分布式锁可以看下这篇[文章](https://wudashan.cn/2017/10/23/Redis-Distributed-Lock-Implement/)，当然更严谨的实现还是看redis的分布式锁官方实现：[Redisson](https://redis.io/topics/distlock)

### 缓存与DB一致性

我们经常会在DB和应用之间加一层Redis缓存，以提高查询效率，但是当数据发生更新时，如何保证缓存与DB的数据一致性呢？可以看看这篇[文章](https://coolshell.cn/articles/17416.html)，之后我也写写阿里是如何保证缓存一致性的。

### 缓存击穿

Redis缓存穿透、缓存雪崩和缓存击穿几个问题，在网上都有比较成熟的解决方案了。比如加空value、设置随机超时时间、加互斥锁等方式

### 海量数据处理

如**海量日志数据，提取出某日访问百度次数最多的那个IP。**

一般是hash+归并、布隆过滤、Map Reduce的思路

这篇[文章](https://blog.csdn.net/v_JULY_v/article/details/6279498)说的很好了

### 限流

常见限流的方案，如令牌桶算法，可以看下我之前写的[来谈谈限流-从概念到实现](https://github.com/farmerjohngit/myblog/issues/18)、[来谈谈限流-RateLimiter源码分析](https://github.com/farmerjohngit/myblog/issues/19)

### 问题排查

一些常见线上问题比如系统的cpu占用过高、RT突然飙升、频繁发生full gc、OOM等如何定位并解决？

这主要来自于日常的积累了，排查问题主要依赖监控、日志、常用工具（top jstack jmap jstat vmstat等）。

 Dubbo

了解一个常用RPC框架如Dubbo的实现：服务发现、路由、异步调用、限流降级、失败重试

- Dubbo如何做负载均衡？
- Dubbo如何做限流降级？
- Dubbo如何优雅的下线服务？
- Dubbo如何实现异步调用的？

#### RocketMq

了解一个常用消息中间件如RocketMq的实现：如何保证高可用和高吞吐、消息顺序、重复消费、事务消息、延迟消息、死信队列

- RocketMq如何保证高可用的？
- RocketMq如何保证高吞吐的？
- RocketMq的消息是有序的吗？
- RocketMq的消息局部顺序是如何保证的?
- RocketMq事务消息的实现机制？
- RocketMq会有重复消费的问题吗？如何解决？
- RocketMq支持什么级别的延迟消息？如何实现的？
- RocketMq是推模型还是拉模型？
- Consumer的负载均衡是怎么样的？

## Redis

redis工作模型、redis持久化、redis过期淘汰机制、redis分布式集群的常见形式、分布式锁、缓存击穿、缓存雪崩、缓存一致性问题

推荐书籍：《*Redis* 设计与实现》

推荐文章：

[分布式Redis深度历险-复制](https://github.com/farmerjohngit/myblog/issues/1)

[分布式Redis深度历险-Sentinel](https://github.com/farmerjohngit/myblog/issues/2)

[分布式Redis深度历险-Clustor](https://github.com/farmerjohngit/myblog/issues/5)

- redis性能为什么高?
- 单线程的redis如何利用多核cpu机器？
- redis的缓存淘汰策略？
- redis如何持久化数据？
- redis有哪几种数据结构？
- redis集群有哪几种形式？
- 有海量key和value都比较小的数据，在redis中如何存储才更省内存？
- 如何保证redis和DB中的数据一致性？
- 如何解决缓存穿透和缓存雪崩？
- 如何用redis实现分布式锁？

- 



#### zk

zk大致原理（可以了解下原理相近的Raft算法）、zk实现分布式锁、zk做集群master选举

- 如何用zk实现分布式锁，与redis分布式锁有和优缺点

